{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "https://github.com/vanvalenlab/deepcell-tf/blob/master/scripts/feature_pyramids/RetinaNet%20-%20Movie.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import errno\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -\n",
      "X.shape: (192, 30, 154, 182, 1)\n",
      "y.shape: (192, 30, 154, 182, 1)\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.data_utils import get_data\n",
    "from deepcell.utils.tracking_utils import load_trks\n",
    "\n",
    "DATA_DIR = '/data/training_data/cells/3T3/NIH/movie'\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'nuclear_movie_3T3_0-2_same.trks')\n",
    "\n",
    "# Load Information for hardcoded image size training\n",
    "seed = 1\n",
    "test_size = .2\n",
    "train_dict, test_dict = get_data(DATA_FILE, mode='siamese_daughters', seed=seed, test_size=test_size)\n",
    "X_train, y_train = train_dict['X'], train_dict['y']\n",
    "X_test, y_test = test_dict['X'], test_dict['y']\n",
    "\n",
    "print(' -\\nX.shape: {}\\ny.shape: {}'.format(train_dict['X'].shape, train_dict['y'].shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Download four different sets of data (saves to ~/.keras/datasets)\n",
    "filename_3T3 = '3T3_NIH.trks'\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.tracked.nih_3t3.load_tracked_data(filename_3T3)\n",
    "print('3T3 -\\nX.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Contants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up other required filepaths\n",
    "PREFIX = os.path.relpath(os.path.dirname(DATA_FILE), DATA_DIR)\n",
    "ROOT_DIR = '/data' # mounted volume\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models', PREFIX))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs', PREFIX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each head of the model uses its own loss\n",
    "from deepcell.losses import RetinaNetLosses\n",
    "from deepcell.losses import discriminative_instance_loss\n",
    "\n",
    "\n",
    "sigma = 3.0\n",
    "alpha = 0.25\n",
    "gamma = 2.0\n",
    "iou_threshold = 0.5\n",
    "max_detections = 100\n",
    "mask_size = (28, 28)\n",
    "\n",
    "retinanet_losses = RetinaNetLosses(\n",
    "    sigma=sigma, alpha=alpha, gamma=gamma,\n",
    "    iou_threshold=iou_threshold,\n",
    "    mask_size=mask_size)\n",
    "\n",
    "loss = {\n",
    "    'regression': retinanet_losses.regress_loss,\n",
    "    'classification': retinanet_losses.classification_loss,\n",
    "    'association_features': discriminative_instance_loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RetinaMask Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "model_name = 'trackrcnn_model'\n",
    "backbone = 'resnet50'  # vgg16, vgg19, resnet50, densenet121, densenet169, densenet201\n",
    "\n",
    "n_epoch = 10  # Number of training epochs\n",
    "lr = 1e-5\n",
    "\n",
    "optimizer = Adam(lr=lr, clipnorm=0.001)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=lr, decay=0.99)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "num_classes = 1  # \"object\" is the only class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.retinanet_anchor_utils import get_anchor_parameters\n",
    "\n",
    "flat_shape = [y_train.shape[0] * y_train.shape[1]] + list(y_train.shape[2:])\n",
    "flat_y = np.reshape(y_train, tuple(flat_shape)).astype('int')\n",
    "\n",
    "# Generate backbone information from the data\n",
    "backbone_levels, pyramid_levels, anchor_params = get_anchor_parameters(flat_y)\n",
    "\n",
    "fpb = 3  # number of frames in each training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rois.shape (?, ?, ?, ?, ?, ?)\n"
     ]
    }
   ],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "# Pass frames_per_batch > 1 to enable 3D mode!\n",
    "model = model_zoo.RetinaMask(\n",
    "    backbone=backbone,\n",
    "    input_shape=X_train.shape[2:],\n",
    "    frames_per_batch=fpb,\n",
    "    class_specific_filter=False,\n",
    "    num_classes=num_classes,\n",
    "    backbone_levels=backbone_levels,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params\n",
    ")\n",
    "\n",
    "prediction_model = model\n",
    "\n",
    "# prediction_model = model_zoo.retinanet_bbox(\n",
    "#     model,\n",
    "#     panoptic=False,\n",
    "#     frames_per_batch=fpb,\n",
    "#     max_detections=100,\n",
    "#     anchor_params=anchor_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50_retinanet_mask\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_input (InputLayer)        [(None, 3, 154, 182, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_35 (TimeDistri (None, 3, 154, 182,  0           image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_36 (TimeDistri (None, 3, 154, 182,  6           time_distributed_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_38 (TimeDistri (None, 3, 39, 46, 25 229760      time_distributed_36[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_37 (TimeDistri (None, 3, 77, 91, 64 9728        time_distributed_36[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "C2_reduced (Conv3D)             (None, 3, 39, 46, 25 65792       time_distributed_38[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "C1_reduced (Conv3D)             (None, 3, 77, 91, 25 16640       time_distributed_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "P2_upsampled (UpsampleLike)     (None, None, None, N 0           C2_reduced[0][0]                 \n",
      "                                                                 time_distributed_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_39 (TimeDistri (None, 3, 20, 23, 51 1460096     time_distributed_36[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "P1_merged (Add)                 (None, 3, 77, 91, 25 0           C1_reduced[0][0]                 \n",
      "                                                                 P2_upsampled[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "C3_reduced (Conv3D)             (None, 3, 20, 23, 25 131328      time_distributed_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "P1 (Conv3D)                     (None, 3, 77, 91, 25 590080      P1_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P3_upsampled (UpsampleLike)     (None, None, None, N 0           C3_reduced[0][0]                 \n",
      "                                                                 time_distributed_38[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "P2_merged (Add)                 (None, 3, 39, 46, 25 0           C2_reduced[0][0]                 \n",
      "                                                                 P3_upsampled[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "boxes_input (InputLayer)        [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "upsamplelike (UpsampleLike)     (None, None, None, N 0           P1[0][0]                         \n",
      "                                                                 image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "P2 (Conv3D)                     (None, 3, 39, 46, 25 590080      P2_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P3 (Conv3D)                     (None, 3, 20, 23, 25 590080      C3_reduced[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "roialign (RoiAlign)             (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 upsamplelike[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "regression_submodel (Model)     (None, 3, None, 4)   7327780     P1[0][0]                         \n",
      "                                                                 P2[0][0]                         \n",
      "                                                                 P3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "classification_submodel (Model) (None, 3, None, 1)   7141129     P1[0][0]                         \n",
      "                                                                 P2[0][0]                         \n",
      "                                                                 P3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "mask_submodel (TimeDistributed) (None, None, None, 2 2950657     roialign[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_44 (TimeDistri (None, None, None, 1 2950657     roialign[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_49 (TimeDistri (None, None, None, N 1032960     roialign[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "regression (Concatenate)        (None, 3, None, 4)   0           regression_submodel[1][0]        \n",
      "                                                                 regression_submodel[2][0]        \n",
      "                                                                 regression_submodel[3][0]        \n",
      "__________________________________________________________________________________________________\n",
      "classification (Concatenate)    (None, 3, None, 1)   0           classification_submodel[1][0]    \n",
      "                                                                 classification_submodel[2][0]    \n",
      "                                                                 classification_submodel[3][0]    \n",
      "__________________________________________________________________________________________________\n",
      "masks (ConcatenateBoxes)        (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 mask_submodel[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_detection (ConcatenateBox (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 time_distributed_44[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "association_features (Concatena (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 time_distributed_49[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 24,847,285\n",
      "Trainable params: 24,837,173\n",
      "Non-trainable params: 10,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0121 17:03:12.146979 140071158327104 training_utils.py:1101] Output masks missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to masks.\n",
      "W0121 17:03:12.148856 140071158327104 training_utils.py:1101] Output final_detection missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to final_detection.\n",
      "W0121 17:03:12.150298 140071158327104 training_utils.py:1101] Output mask_submodel missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to mask_submodel.\n",
      "W0121 17:03:12.151503 140071158327104 training_utils.py:1101] Output time_distributed_44 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to time_distributed_44.\n",
      "W0121 17:03:12.152612 140071158327104 training_utils.py:1101] Output time_distributed_49 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to time_distributed_49.\n",
      "W0121 17:03:12.403715 140071158327104 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "reduction_indices is deprecated, use axis instead\n",
      "W0121 17:03:12.495692 140071158327104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/deepcell/losses.py:203: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\n",
      "W0121 17:03:12.498631 140071158327104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/deepcell/losses.py:204: The name tf.matrix_set_diag is deprecated. Please use tf.linalg.set_diag instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RetinaMask Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.image_generators import RetinaMovieDataGenerator\n",
    "\n",
    "datagen = RetinaMovieDataGenerator(\n",
    "    rotation_range=180,\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "datagen_val = RetinaMovieDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0121 17:14:19.914193 140071158327104 retinanet.py:633] Removing 2 of 192 images with fewer than 3 objects.\n",
      "W0121 17:14:21.400825 140071158327104 retinanet.py:633] Removing 1 of 48 images with fewer than 3 objects.\n"
     ]
    }
   ],
   "source": [
    "train_data = datagen.flow(\n",
    "    train_dict,\n",
    "    batch_size=1,\n",
    "    include_masks=True,\n",
    "    include_final_detection_layer=True,\n",
    "    assoc_head=True,\n",
    "    frames_per_batch=fpb,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params)\n",
    "\n",
    "val_data = datagen_val.flow(\n",
    "    test_dict,\n",
    "    batch_size=1,\n",
    "    include_masks=True,\n",
    "    include_final_detection_layer=True,\n",
    "    assoc_head=True,\n",
    "    frames_per_batch=fpb,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 4 arrays: [array([[[[  2.0580583 ,   1.6161165 ,  12.526019  ,   5.4549513 ,\n            0.        ],\n         [  2.1492307 ,   1.7984612 ,   9.426159  ,   3.8138487 ,\n            0.        ],\n         [  2.221...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-787d9f17aab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                      \u001b[0mframes_per_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                      weighted_average=True),\n\u001b[0;32m---> 29\u001b[0;31m             prediction_model)\n\u001b[0m\u001b[1;32m     30\u001b[0m     ])\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m   1152\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[0;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2671\u001b[0m           \u001b[0mshapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2673\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2675\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    344\u001b[0m                        \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                        \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m       raise ValueError('Error when checking model ' + exception_prefix +\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 4 arrays: [array([[[[  2.0580583 ,   1.6161165 ,  12.526019  ,   5.4549513 ,\n            0.        ],\n         [  2.1492307 ,   1.7984612 ,   9.426159  ,   3.8138487 ,\n            0.        ],\n         [  2.221..."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "from deepcell.callbacks import RedirectModel, Evaluate\n",
    "\n",
    "iou_threshold = 0.5\n",
    "score_threshold = 0.01\n",
    "max_detections = 100\n",
    "\n",
    "model.fit_generator(\n",
    "    train_data,\n",
    "    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "    epochs=n_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=X_test.shape[0] // batch_size,\n",
    "    callbacks=[\n",
    "        callbacks.LearningRateScheduler(lr_sched),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            os.path.join(MODEL_DIR, model_name + '.h5'),\n",
    "            monitor='val_loss',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False),\n",
    "        RedirectModel(\n",
    "            Evaluate(val_data,\n",
    "                     iou_threshold=iou_threshold,\n",
    "                     score_threshold=score_threshold,\n",
    "                     max_detections=max_detections,\n",
    "                     frames_per_batch=fpb,\n",
    "                     weighted_average=True),\n",
    "            prediction_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
