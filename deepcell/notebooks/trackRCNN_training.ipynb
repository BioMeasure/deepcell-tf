{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "https://github.com/vanvalenlab/deepcell-tf/blob/master/scripts/feature_pyramids/RetinaNet%20-%20Movie.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import errno\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -\n",
      "X.shape: (192, 30, 154, 182, 1)\n",
      "y.shape: (192, 30, 154, 182, 1)\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.data_utils import get_data\n",
    "from deepcell.utils.tracking_utils import load_trks\n",
    "\n",
    "DATA_DIR = '/data/training_data/cells/3T3/NIH/movie'\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'nuclear_movie_3T3_0-2_same.trks')\n",
    "\n",
    "# Load Information for hardcoded image size training\n",
    "seed = 1\n",
    "test_size = .2\n",
    "train_dict, test_dict = get_data(DATA_FILE, mode='siamese_daughters', seed=seed, test_size=test_size)\n",
    "X_train, y_train = train_dict['X'], train_dict['y']\n",
    "X_test, y_test = test_dict['X'], test_dict['y']\n",
    "\n",
    "print(' -\\nX.shape: {}\\ny.shape: {}'.format(train_dict['X'].shape, train_dict['y'].shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Download four different sets of data (saves to ~/.keras/datasets)\n",
    "filename_3T3 = '3T3_NIH.trks'\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.tracked.nih_3t3.load_tracked_data(filename_3T3)\n",
    "print('3T3 -\\nX.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Contants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up other required filepaths\n",
    "PREFIX = os.path.relpath(os.path.dirname(DATA_FILE), DATA_DIR)\n",
    "ROOT_DIR = '/data' # mounted volume\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models', PREFIX))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs', PREFIX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each head of the model uses its own loss\n",
    "from deepcell.losses import RetinaNetLosses\n",
    "from deepcell.losses import discriminative_instance_loss\n",
    "\n",
    "\n",
    "sigma = 3.0\n",
    "alpha = 0.25\n",
    "gamma = 2.0\n",
    "iou_threshold = 0.5\n",
    "max_detections = 100\n",
    "mask_size = (28, 28)\n",
    "\n",
    "retinanet_losses = RetinaNetLosses(\n",
    "    sigma=sigma, alpha=alpha, gamma=gamma,\n",
    "    iou_threshold=iou_threshold,\n",
    "    mask_size=mask_size)\n",
    "\n",
    "loss = {\n",
    "    'regression': retinanet_losses.regress_loss,\n",
    "    'classification': retinanet_losses.classification_loss,\n",
    "    'association_features': discriminative_instance_loss,\n",
    "    'masks': retinanet_losses.mask_loss,\n",
    "    'final_detection': retinanet_losses.final_detection_loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RetinaMask Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "model_name = 'trackrcnn_model'\n",
    "backbone = 'resnet50'  # vgg16, vgg19, resnet50, densenet121, densenet169, densenet201\n",
    "\n",
    "n_epoch = 10  # Number of training epochs\n",
    "lr = 1e-5\n",
    "\n",
    "optimizer = Adam(lr=lr, clipnorm=0.001)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=lr, decay=0.99)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "num_classes = 1  # \"object\" is the only class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.retinanet_anchor_utils import get_anchor_parameters\n",
    "\n",
    "flat_shape = [y_train.shape[0] * y_train.shape[1]] + list(y_train.shape[2:])\n",
    "flat_y = np.reshape(y_train, tuple(flat_shape)).astype('int')\n",
    "\n",
    "# Generate backbone information from the data\n",
    "backbone_levels, pyramid_levels, anchor_params = get_anchor_parameters(flat_y)\n",
    "\n",
    "fpb = 3  # number of frames in each training batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016-2019 The Van Valen Lab at the California Institute of\n",
    "# Technology (Caltech), with support from the Paul Allen Family Foundation,\n",
    "# Google, & National Institutes of Health (NIH) under Grant U24CA224309-01.\n",
    "# All rights reserved.\n",
    "#\n",
    "# Licensed under a modified Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.github.com/vanvalenlab/deepcell-tf/LICENSE\n",
    "#\n",
    "# The Work provided may be used for non-commercial academic purposes only.\n",
    "# For any other use of the Work, including commercial use, please contact:\n",
    "# vanvalenlab@gmail.com\n",
    "#\n",
    "# Neither the name of Caltech nor the names of its contributors may be used\n",
    "# to endorse or promote products derived from this software without specific\n",
    "# prior written permission.\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"TrackRCNN models adapted from MaskRCNN and https://github.com/fizyr/keras-maskrcnn\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.layers import Add, Activation, Flatten, Dense\n",
    "from tensorflow.python.keras.layers import Input, Concatenate\n",
    "from tensorflow.python.keras.layers import TimeDistributed, Conv2D, Conv3D\n",
    "from tensorflow.python.keras.layers import AveragePooling2D, AveragePooling3D \n",
    "from tensorflow.python.keras.layers import MaxPool2D, MaxPool3D, Lambda\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.initializers import normal\n",
    "\n",
    "from deepcell.layers import Cast, Shape, UpsampleLike\n",
    "from deepcell.layers import Upsample, RoiAlign, ConcatenateBoxes\n",
    "from deepcell.layers import ClipBoxes, RegressBoxes, FilterDetections\n",
    "from deepcell.layers import TensorProduct, ImageNormalization2D, Location2D\n",
    "from deepcell.layers import ImageNormalization3D, Location3D\n",
    "from deepcell.model_zoo.retinanet import retinanet, __build_anchors\n",
    "from deepcell.utils.retinanet_anchor_utils import AnchorParameters\n",
    "from deepcell.utils.backbone_utils import get_backbone\n",
    "\n",
    "\n",
    "def default_mask_model(num_classes,\n",
    "                       pyramid_feature_size=256,\n",
    "                       mask_feature_size=256,\n",
    "                       roi_size=(14, 14),\n",
    "                       mask_size=(28, 28),\n",
    "                       name='mask_submodel',\n",
    "                       mask_dtype=K.floatx(),\n",
    "                       retinanet_dtype=K.floatx()):\n",
    "    \"\"\"Creates the default mask submodel.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of classes to predict a score for at each\n",
    "            feature level.\n",
    "        pyramid_feature_size (int): The number of filters to expect from the\n",
    "            feature pyramid levels.\n",
    "        mask_feature_size (int): The number of filters to expect from the masks.\n",
    "        roi_size (tuple): The number of filters to use in the Roi Layers.\n",
    "        mask_size (tuple): The size of the masks.\n",
    "        mask_dtype (str): Dtype to use for mask tensors.\n",
    "        retinanet_dtype (str): Dtype retinanet models expect.\n",
    "        name (str): The name of the submodel.\n",
    "\n",
    "    Returns:\n",
    "        tensorflow.keras.Model: a Model that predicts classes for\n",
    "            each anchor.\n",
    "    \"\"\"\n",
    "    options = {\n",
    "        'kernel_size': 3,\n",
    "        'strides': 1,\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': normal(mean=0.0, stddev=0.01, seed=None),\n",
    "        'bias_initializer': 'zeros',\n",
    "        'activation': 'relu',\n",
    "    }\n",
    "\n",
    "    inputs = Input(shape=(None, roi_size[0], roi_size[1], pyramid_feature_size))\n",
    "    outputs = inputs\n",
    "\n",
    "    # casting to the desidered data type, which may be different than\n",
    "    # the one used for the underlying keras-retinanet model\n",
    "    if mask_dtype != retinanet_dtype:\n",
    "        outputs = TimeDistributed(\n",
    "            Cast(dtype=mask_dtype),\n",
    "            name='cast_masks')(outputs)\n",
    "\n",
    "    for i in range(4):\n",
    "        outputs = TimeDistributed(Conv2D(\n",
    "            filters=mask_feature_size,\n",
    "            **options\n",
    "        ), name='roi_mask_{}'.format(i))(outputs)\n",
    "\n",
    "    # perform upsampling + conv instead of deconv as in the paper\n",
    "    # https://distill.pub/2016/deconv-checkerboard/\n",
    "    outputs = TimeDistributed(\n",
    "        Upsample(mask_size),\n",
    "        name='roi_mask_upsample')(outputs)\n",
    "    outputs = TimeDistributed(Conv2D(\n",
    "        filters=mask_feature_size,\n",
    "        **options\n",
    "    ), name='roi_mask_features')(outputs)\n",
    "\n",
    "    outputs = TimeDistributed(Conv2D(\n",
    "        filters=num_classes,\n",
    "        kernel_size=1,\n",
    "        activation='sigmoid'\n",
    "    ), name='roi_mask')(outputs)\n",
    "\n",
    "    # casting back to the underlying keras-retinanet model data type\n",
    "    if mask_dtype != retinanet_dtype:\n",
    "        outputs = TimeDistributed(\n",
    "            Cast(dtype=retinanet_dtype),\n",
    "            name='recast_masks')(outputs)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "def default_final_detection_model(pyramid_feature_size=256,\n",
    "                                  final_detection_feature_size=256,\n",
    "                                  roi_size=(14, 14),\n",
    "                                  name='final_detection_submodel'):\n",
    "    options = {\n",
    "        'kernel_size': 3,\n",
    "        'strides': 1,\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': normal(mean=0.0, stddev=0.01, seed=None),\n",
    "        'bias_initializer': 'zeros',\n",
    "        'activation': 'relu'\n",
    "    }\n",
    "\n",
    "    inputs = Input(shape=(None, roi_size[0], roi_size[1], pyramid_feature_size))\n",
    "    outputs = inputs\n",
    "\n",
    "    for i in range(2):\n",
    "        outputs = TimeDistributed(Conv2D(\n",
    "            filters=final_detection_feature_size,\n",
    "            **options\n",
    "        ), name='final_detection_submodel_conv1_block{}'.format(i))(outputs)\n",
    "        outputs = TimeDistributed(Conv2D(\n",
    "            filters=final_detection_feature_size,\n",
    "            **options\n",
    "        ), name='final_detection_submodel_conv2_block{}'.format(i))(outputs)\n",
    "        outputs = TimeDistributed(MaxPool2D(\n",
    "        ), name='final_detection_submodel_pool1_block{}'.format(i))(outputs)\n",
    "\n",
    "    outputs = TimeDistributed(Conv2D(filters=final_detection_feature_size,\n",
    "                                     kernel_size=3,\n",
    "                                     padding='valid',\n",
    "                                     kernel_initializer=normal(mean=0.0, stddev=0.01, seed=None),\n",
    "                                     bias_initializer='zeros',\n",
    "                                     activation='relu'))(outputs)\n",
    "\n",
    "    outputs = TimeDistributed(Conv2D(filters=1,\n",
    "                                     kernel_size=1,\n",
    "                                     activation='sigmoid'))(outputs)\n",
    "\n",
    "    outputs = Lambda(lambda x: tf.squeeze(x, axis=[2, 3]))(outputs)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "def default_roi_submodels(num_classes,\n",
    "                          roi_size=(14, 14),\n",
    "                          mask_size=(28, 28),\n",
    "                          frames_per_batch=1,\n",
    "                          mask_dtype=K.floatx(),\n",
    "                          retinanet_dtype=K.floatx()):\n",
    "    \"\"\"Create a list of default roi submodels.\n",
    "\n",
    "    The default submodels contains a single mask model.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of classes to use.\n",
    "        roi_size (tuple): The number of filters to use in the Roi Layers.\n",
    "        mask_size (tuple): The size of the masks.\n",
    "        mask_dtype (str): Dtype to use for mask tensors.\n",
    "        retinanet_dtype (str): Dtype retinanet models expect.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuple, where the first element is the name of the\n",
    "            submodel and the second element is the submodel itself.\n",
    "    \"\"\"\n",
    "    if frames_per_batch > 1:\n",
    "        return [\n",
    "            ('masks', TimeDistributed(\n",
    "                default_mask_model(num_classes,\n",
    "                                   name='mask_submodel_0',\n",
    "                                   roi_size=roi_size,\n",
    "                                   mask_size=mask_size,\n",
    "                                   mask_dtype=mask_dtype,\n",
    "                                   retinanet_dtype=retinanet_dtype), name='mask_submodel')),\n",
    "            ('final_detection', TimeDistributed(\n",
    "                default_final_detection_model(roi_size=roi_size))),\n",
    "#             ('association_features', TimeDistributed(\n",
    "#                 association_vector_model(frames_per_batch=frames_per_batch)))\n",
    "        ]\n",
    "    return [\n",
    "        ('masks', default_mask_model(num_classes,\n",
    "                                     roi_size=roi_size,\n",
    "                                     mask_size=mask_size,\n",
    "                                     mask_dtype=mask_dtype,\n",
    "                                     retinanet_dtype=retinanet_dtype))\n",
    "        # ('final_detection', default_final_detection_model(roi_size=roi_size))\n",
    "        ]\n",
    "\n",
    "\n",
    "def association_vector_model(#association_vector,\n",
    "                             frames_per_batch=1,\n",
    "                             num_association_features=2,\n",
    "                             name='association_vector_model'):\n",
    "    options = {\n",
    "        'kernel_size': 3,\n",
    "        'strides': 1,\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': normal(mean=0.0, stddev=0.01, seed=None),\n",
    "        'bias_initializer': 'zeros',\n",
    "        'activation': 'relu'\n",
    "    }\n",
    "\n",
    "    # inputs = Input(shape=tuple(association_vector.shape))\n",
    "    inputs = Input(shape=(None, None, None, num_association_features))\n",
    "\n",
    "    conv1 = TimeDistributed(Conv2D(\n",
    "        filters=num_association_features,\n",
    "        **options\n",
    "    ), name='association_vector_submodel_conv1')(inputs)\n",
    "    conv2 = TimeDistributed(Conv2D(\n",
    "        filters=num_association_features,\n",
    "        **options\n",
    "    ), name='association_vector_submodel_conv2')(conv1)\n",
    "    x = TimeDistributed(MaxPool2D(\n",
    "    ), name='association_vector_submodel_pool1')(conv2)\n",
    "\n",
    "    # Residuals\n",
    "    for i in range(2):\n",
    "        x = TimeDistributed(Conv2D(filters=num_association_features,\n",
    "                                   kernel_size=3,\n",
    "                                   padding='valid',\n",
    "                                   kernel_initializer=normal(mean=0.0, stddev=0.01, seed=None),\n",
    "                                   bias_initializer='zeros',\n",
    "                                   activation='relu', \n",
    "                                   name='association_vector_residual_conv1_block{}'.format(i)))(x)\n",
    "        y = TimeDistributed(Conv2D(filters=num_association_features,\n",
    "                                   kernel_size=3,\n",
    "                                   padding='same',\n",
    "                                   kernel_initializer=normal(mean=0.0, stddev=0.01, seed=None),\n",
    "                                   bias_initializer='zeros',\n",
    "                                   activation='relu',\n",
    "                                   name='association_vector_residual_conv2_block{}'.format(i)))(x)\n",
    "        x = Add(name='association_vector_residual_add_block{}'.format(i))([x, y])\n",
    "        x = Activation('relu')(x)                          \n",
    "\n",
    "    y = AveragePooling3D(pool_size=3)(x)\n",
    "#     y = Flatten()(y)\n",
    "    outputs = Dense(num_association_features,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "    outputs = y\n",
    "    print(\"outputs.shape\", outputs.shape)\n",
    "    return Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "def retinanet_mask(inputs,\n",
    "                   backbone_dict,\n",
    "                   num_classes,\n",
    "                   frames_per_batch=1,\n",
    "                   backbone_levels=['C3', 'C4', 'C5'],\n",
    "                   pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "                   retinanet_model=None,\n",
    "                   anchor_params=None,\n",
    "                   nms=True,\n",
    "                   panoptic=False,\n",
    "                   use_assoc_head=False,\n",
    "                   class_specific_filter=True,\n",
    "                   crop_size=(14, 14),\n",
    "                   mask_size=(28, 28),\n",
    "                   name='retinanet-mask',\n",
    "                   roi_submodels=None,\n",
    "                   max_detections=100,\n",
    "                   score_threshold=0.05,\n",
    "                   nms_threshold=0.5,\n",
    "                   mask_dtype=K.floatx(),\n",
    "                   **kwargs):\n",
    "    \"\"\"Construct a RetinaNet mask model on top of a retinanet bbox model.\n",
    "    Uses the retinanet bbox model and appends layers to compute masks.\n",
    "\n",
    "    Args:\n",
    "        inputs (tensor): List of tensorflow.keras.layers.Input.\n",
    "            The first input is the image, the second input the blob of masks.\n",
    "        num_classes (int): Integer, number of classes to classify.\n",
    "        retinanet_model (tensorflow.keras.Model): RetinaNet model that predicts\n",
    "            regression and classification values.\n",
    "        anchor_params (AnchorParameters): Struct containing anchor parameters.\n",
    "        nms (bool): Whether to use NMS.\n",
    "        class_specific_filter (bool): Use class specific filtering.\n",
    "        roi_submodels (list): Submodels for processing ROIs.\n",
    "        name (str): Name of the model.\n",
    "        mask_dtype (str): Dtype to use for mask tensors.\n",
    "        kwargs (dict): Additional kwargs to pass to the retinanet bbox model.\n",
    "\n",
    "    Returns:\n",
    "        tensorflow.keras.Model: Model with inputs as input and as output\n",
    "            the output of each submodel for each pyramid level and the\n",
    "            detections. The order is as defined in submodels.\n",
    "\n",
    "            ```\n",
    "            [\n",
    "                regression, classification, other[0], ...,\n",
    "                boxes_masks, boxes, scores, labels, masks, other[0], ...\n",
    "            ]\n",
    "            ```\n",
    "\n",
    "    \"\"\"\n",
    "    if anchor_params is None:\n",
    "        anchor_params = AnchorParameters.default\n",
    "\n",
    "    if roi_submodels is None:\n",
    "        retinanet_dtype = K.floatx()\n",
    "        K.set_floatx(mask_dtype)\n",
    "        roi_submodels = default_roi_submodels(\n",
    "            num_classes, crop_size, mask_size,\n",
    "            frames_per_batch, mask_dtype, retinanet_dtype)\n",
    "        K.set_floatx(retinanet_dtype)\n",
    "\n",
    "    image = inputs\n",
    "    image_shape = Shape()(image)\n",
    "\n",
    "    if retinanet_model is None:\n",
    "        retinanet_model = retinanet(\n",
    "            inputs=image,\n",
    "            backbone_dict=backbone_dict,\n",
    "            num_classes=num_classes,\n",
    "            backbone_levels=backbone_levels,\n",
    "            pyramid_levels=pyramid_levels,\n",
    "            panoptic=panoptic,\n",
    "            num_anchors=anchor_params.num_anchors(),\n",
    "            frames_per_batch=frames_per_batch,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    # parse outputs\n",
    "    regression = retinanet_model.outputs[0]\n",
    "    classification = retinanet_model.outputs[1]\n",
    "\n",
    "    if panoptic:\n",
    "        # Determine the number of semantic heads\n",
    "        n_semantic_heads = len([1 for layer in retinanet_model.layers if 'semantic' in layer.name])\n",
    "\n",
    "        # The  panoptic output should not be sent to filter detections\n",
    "        other = retinanet_model.outputs[2:-n_semantic_heads]\n",
    "        semantic = retinanet_model.outputs[-n_semantic_heads:]\n",
    "    else:\n",
    "        other = retinanet_model.outputs[2:]\n",
    "\n",
    "\n",
    "    features = [retinanet_model.get_layer(name).output\n",
    "                for name in pyramid_levels]\n",
    "\n",
    "    # build boxes\n",
    "    anchors = __build_anchors(anchor_params, features,\n",
    "                              frames_per_batch=frames_per_batch)\n",
    "    boxes = RegressBoxes(name='boxes')([anchors, regression])\n",
    "    boxes = ClipBoxes(name='clipped_boxes')([image, boxes])\n",
    "\n",
    "    # filter detections (apply NMS / score threshold / select top-k)\n",
    "    # use ground truth boxes\n",
    "    if frames_per_batch == 1:\n",
    "        boxes = Input(shape=(None, 4), name='boxes_input')\n",
    "    else:\n",
    "        boxes = Input(shape=(None, None, 4), name='boxes_input')\n",
    "    \n",
    "    inputs = [image, boxes]\n",
    "    \n",
    "    if use_assoc_head:\n",
    "        num_association_features=2\n",
    "        if frames_per_batch == 1:\n",
    "            assoc_head_input = Input(shape=(None, None, None, num_association_features), name='assoc_head_input')\n",
    "        else:\n",
    "            assoc_head_input = Input(shape=(None, None, None, None, num_association_features), name='assoc_head_input')\n",
    "        inputs.append(assoc_head_input)\n",
    "\n",
    "    fpn = features[0]\n",
    "    fpn = UpsampleLike(name='upsamplelike')([fpn, image])\n",
    "    rois = RoiAlign(crop_size=crop_size, name='roialign')([boxes, fpn])\n",
    "    print(\"rois.shape\", rois.shape)\n",
    "\n",
    "    # execute trackrcnn submodels\n",
    "    trackrcnn_outputs = [submodel(rois) for _, submodel in roi_submodels]\n",
    "\n",
    "    # concatenate boxes for loss computation\n",
    "    trainable_outputs = [ConcatenateBoxes(name=name)([boxes, output])\n",
    "                         for (name, _), output in zip(\n",
    "                             roi_submodels, trackrcnn_outputs)]\n",
    "    \n",
    "    if use_assoc_head:\n",
    "        print(\"retinanet_model.outputs\", retinanet_model.outputs)\n",
    "\n",
    "    if use_assoc_head:\n",
    "        print(\"assoc_head_input.shape\", assoc_head_input.shape)\n",
    "        association_head_submodel = TimeDistributed(\n",
    "                                association_vector_model(num_association_features=num_association_features,\n",
    "                                                        frames_per_batch=frames_per_batch))\n",
    "        trackrcnn_outputs.append(association_head_submodel(assoc_head_input))\n",
    "        trainable_outputs.append(ConcatenateBoxes(name='association_features')([boxes, trackrcnn_outputs[-1]]))\n",
    "\n",
    "    print(\"roi_submodels names in roi_submodels:\")\n",
    "    for (name, _) in roi_submodels:\n",
    "        print(name)\n",
    "    \n",
    "    print(\"trainable_outputs:\")\n",
    "    for x in trainable_outputs:\n",
    "        print(x)\n",
    "\n",
    "    # reconstruct the new output\n",
    "    detections = []\n",
    "\n",
    "    outputs = [regression, classification] + other + trainable_outputs + \\\n",
    "        detections + trackrcnn_outputs\n",
    "\n",
    "    if panoptic:\n",
    "        outputs += list(semantic)\n",
    "\n",
    "    print(\"outputs:\", outputs)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=name)\n",
    "    model.backbone_levels = backbone_levels\n",
    "    model.pyramid_levels = pyramid_levels\n",
    "\n",
    "    return model\n",
    "\n",
    "def RetinaMask(backbone,\n",
    "               num_classes,\n",
    "               input_shape,\n",
    "               inputs=None,\n",
    "               backbone_levels=['C3', 'C4', 'C5'],\n",
    "               pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "               norm_method='whole_image',\n",
    "               location=False,\n",
    "               use_imagenet=False,\n",
    "               crop_size=(14, 14),\n",
    "               pooling=None,\n",
    "               mask_dtype=K.floatx(),\n",
    "               required_channels=3,\n",
    "               frames_per_batch=1,\n",
    "               use_assoc_head=False,\n",
    "               **kwargs):\n",
    "    \"\"\"Constructs a mrcnn model using a backbone from keras-applications.\n",
    "\n",
    "    Args:\n",
    "        backbone (str): Name of backbone to use.\n",
    "        num_classes (int): Number of classes to classify.\n",
    "        input_shape (tuple): The shape of the input data.\n",
    "        weights (str): one of None (random initialization),\n",
    "            'imagenet' (pre-training on ImageNet),\n",
    "            or the path to the weights file to be loaded.\n",
    "        pooling (str): optional pooling mode for feature extraction\n",
    "            when include_top is False.\n",
    "            - None means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - 'avg' means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - 'max' means that global max pooling will\n",
    "                be applied.\n",
    "        required_channels (int): The required number of channels of the\n",
    "            backbone.  3 is the default for all current backbones.\n",
    "\n",
    "    Returns:\n",
    "        tensorflow.keras.Model: RetinaNet model with a backbone.\n",
    "    \"\"\"\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    if inputs is None:\n",
    "        if frames_per_batch > 1:\n",
    "            if channel_axis == 1:\n",
    "                input_shape_with_time = tuple(\n",
    "                    [input_shape[0], frames_per_batch] + list(input_shape)[1:])\n",
    "            else:\n",
    "                input_shape_with_time = tuple(\n",
    "                    [frames_per_batch] + list(input_shape))\n",
    "            inputs = Input(shape=input_shape_with_time, name='image_input')\n",
    "        else:\n",
    "            inputs = Input(shape=input_shape, name='image_input')\n",
    "\n",
    "    if location:\n",
    "        if frames_per_batch > 1:\n",
    "            # TODO: TimeDistributed is incompatible with channels_first\n",
    "            loc = TimeDistributed(Location2D(in_shape=input_shape))(inputs)\n",
    "        else:\n",
    "            loc = Location2D(in_shape=input_shape)(inputs)\n",
    "        concat = Concatenate(axis=channel_axis)([inputs, loc])\n",
    "    else:\n",
    "        concat = inputs\n",
    "\n",
    "    # force the channel size for backbone input to be `required_channels`\n",
    "    if frames_per_batch > 1:\n",
    "        norm = TimeDistributed(ImageNormalization2D(norm_method=norm_method))(concat)\n",
    "        fixed_inputs = TimeDistributed(TensorProduct(required_channels))(norm)\n",
    "    else:\n",
    "        norm = ImageNormalization2D(norm_method=norm_method)(concat)\n",
    "        fixed_inputs = TensorProduct(required_channels)(norm)\n",
    "\n",
    "    # force the input shape\n",
    "    axis = 0 if K.image_data_format() == 'channels_first' else -1\n",
    "    fixed_input_shape = list(input_shape)\n",
    "    fixed_input_shape[axis] = required_channels\n",
    "    fixed_input_shape = tuple(fixed_input_shape)\n",
    "\n",
    "    model_kwargs = {\n",
    "        'include_top': False,\n",
    "        'weights': None,\n",
    "        'input_shape': fixed_input_shape,\n",
    "        'pooling': pooling\n",
    "    }\n",
    "\n",
    "    _, backbone_dict = get_backbone(backbone, fixed_inputs,\n",
    "                                    use_imagenet=use_imagenet,\n",
    "                                    frames_per_batch=frames_per_batch,\n",
    "                                    return_dict=True, **model_kwargs)\n",
    "\n",
    "    # create the full model\n",
    "    return retinanet_mask(\n",
    "        inputs=inputs,\n",
    "        num_classes=num_classes,\n",
    "        backbone_dict=backbone_dict,\n",
    "        crop_size=crop_size,\n",
    "        backbone_levels=backbone_levels,\n",
    "        pyramid_levels=pyramid_levels,\n",
    "        name='{}_retinanet_mask'.format(backbone),\n",
    "        mask_dtype=mask_dtype,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        use_assoc_head=use_assoc_head,\n",
    "        **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rois.shape (?, ?, ?, ?, ?, ?)\n",
      "retinanet_model.outputs [<tf.Tensor 'regression_23/concat:0' shape=(?, 3, ?, 4) dtype=float32>, <tf.Tensor 'classification_23/concat:0' shape=(?, 3, ?, 1) dtype=float32>]\n",
      "assoc_head_input.shape (?, ?, ?, ?, ?, 2)\n",
      "outputs.shape (?, ?, ?, ?, 2)\n",
      "roi_submodels names in roi_submodels:\n",
      "masks\n",
      "final_detection\n",
      "trainable_outputs:\n",
      "Tensor(\"masks_14/concat:0\", shape=(?, ?, ?, ?), dtype=float32)\n",
      "Tensor(\"final_detection_14/concat:0\", shape=(?, ?, ?, ?), dtype=float32)\n",
      "Tensor(\"association_features/concat:0\", shape=(?, ?, ?, ?), dtype=float32)\n",
      "outputs: [<tf.Tensor 'regression_23/concat:0' shape=(?, 3, ?, 4) dtype=float32>, <tf.Tensor 'classification_23/concat:0' shape=(?, 3, ?, 1) dtype=float32>, <tf.Tensor 'masks_14/concat:0' shape=(?, ?, ?, ?) dtype=float32>, <tf.Tensor 'final_detection_14/concat:0' shape=(?, ?, ?, ?) dtype=float32>, <tf.Tensor 'association_features/concat:0' shape=(?, ?, ?, ?) dtype=float32>, <tf.Tensor 'mask_submodel_14/Reshape_1:0' shape=(?, ?, ?, 28, 28, 1) dtype=float32>, <tf.Tensor 'time_distributed_345/Reshape_1:0' shape=(?, ?, ?, 1) dtype=float32>, <tf.Tensor 'time_distributed_353/Reshape_1:0' shape=(?, ?, ?, ?, ?, 2) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "# Pass frames_per_batch > 1 to enable 3D mode!\n",
    "model = RetinaMask(\n",
    "    backbone=backbone,\n",
    "    input_shape=X_train.shape[2:],\n",
    "    frames_per_batch=fpb,\n",
    "    class_specific_filter=False,\n",
    "    num_classes=num_classes,\n",
    "    backbone_levels=backbone_levels,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params,\n",
    "    use_assoc_head=True,\n",
    ")\n",
    "\n",
    "prediction_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"final_detection\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_input (InputLayer)        [(None, 3, 154, 182, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_336 (TimeDistr (None, 3, 154, 182,  0           image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_337 (TimeDistr (None, 3, 154, 182,  6           time_distributed_336[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_339 (TimeDistr (None, 3, 39, 46, 25 229760      time_distributed_337[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_338 (TimeDistr (None, 3, 77, 91, 64 9728        time_distributed_337[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "C2_reduced (Conv3D)             (None, 3, 39, 46, 25 65792       time_distributed_339[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "C1_reduced (Conv3D)             (None, 3, 77, 91, 25 16640       time_distributed_338[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "P2_upsampled (UpsampleLike)     (None, None, None, N 0           C2_reduced[0][0]                 \n",
      "                                                                 time_distributed_338[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_340 (TimeDistr (None, 3, 20, 23, 51 1460096     time_distributed_337[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "P1_merged (Add)                 (None, 3, 77, 91, 25 0           C1_reduced[0][0]                 \n",
      "                                                                 P2_upsampled[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "C3_reduced (Conv3D)             (None, 3, 20, 23, 25 131328      time_distributed_340[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "P1 (Conv3D)                     (None, 3, 77, 91, 25 590080      P1_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P3_upsampled (UpsampleLike)     (None, None, None, N 0           C3_reduced[0][0]                 \n",
      "                                                                 time_distributed_339[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "P2_merged (Add)                 (None, 3, 39, 46, 25 0           C2_reduced[0][0]                 \n",
      "                                                                 P3_upsampled[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "boxes_input (InputLayer)        [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "upsamplelike (UpsampleLike)     (None, None, None, N 0           P1[0][0]                         \n",
      "                                                                 image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "P2 (Conv3D)                     (None, 3, 39, 46, 25 590080      P2_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P3 (Conv3D)                     (None, 3, 20, 23, 25 590080      C3_reduced[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "roialign (RoiAlign)             (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 upsamplelike[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "assoc_head_input (InputLayer)   [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "regression_submodel (Model)     (None, 3, None, 4)   7327780     P1[0][0]                         \n",
      "                                                                 P2[0][0]                         \n",
      "                                                                 P3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "classification_submodel (Model) (None, 3, None, 1)   7141129     P1[0][0]                         \n",
      "                                                                 P2[0][0]                         \n",
      "                                                                 P3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "mask_submodel (TimeDistributed) (None, None, None, 2 2950657     roialign[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_345 (TimeDistr (None, None, None, 1 2950657     roialign[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_353 (TimeDistr (None, None, None, N 228         assoc_head_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "regression (Concatenate)        (None, 3, None, 4)   0           regression_submodel[1][0]        \n",
      "                                                                 regression_submodel[2][0]        \n",
      "                                                                 regression_submodel[3][0]        \n",
      "__________________________________________________________________________________________________\n",
      "classification (Concatenate)    (None, 3, None, 1)   0           classification_submodel[1][0]    \n",
      "                                                                 classification_submodel[2][0]    \n",
      "                                                                 classification_submodel[3][0]    \n",
      "__________________________________________________________________________________________________\n",
      "masks (ConcatenateBoxes)        (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 mask_submodel[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_detection (ConcatenateBox (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 time_distributed_345[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "association_features (Concatena (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 time_distributed_353[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 23,814,553\n",
      "Trainable params: 23,804,441\n",
      "Non-trainable params: 10,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0218 20:41:38.424769 140303936599872 training_utils.py:1101] Output mask_submodel missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to mask_submodel.\n",
      "W0218 20:41:38.426364 140303936599872 training_utils.py:1101] Output time_distributed_345 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to time_distributed_345.\n",
      "W0218 20:41:38.427592 140303936599872 training_utils.py:1101] Output time_distributed_353 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to time_distributed_353.\n",
      "W0218 20:41:38.543709 140303936599872 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/deepcell/losses.py:332: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0218 20:41:39.277669 140303936599872 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "reduction_indices is deprecated, use axis instead\n",
      "W0218 20:41:39.365728 140303936599872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/deepcell/losses.py:203: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\n",
      "W0218 20:41:39.368892 140303936599872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/deepcell/losses.py:204: The name tf.matrix_set_diag is deprecated. Please use tf.linalg.set_diag instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RetinaMask Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016-2019 The Van Valen Lab at the California Institute of\n",
    "# Technology (Caltech), with support from the Paul Allen Family Foundation,\n",
    "# Google, & National Institutes of Health (NIH) under Grant U24CA224309-01.\n",
    "# All rights reserved.\n",
    "#\n",
    "# Licensed under a modified Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.github.com/vanvalenlab/deepcell-tf/LICENSE\n",
    "#\n",
    "# The Work provided may be used for non-commercial academic purposes only.\n",
    "# For any other use of the Work, including commercial use, please contact:\n",
    "# vanvalenlab@gmail.com\n",
    "#\n",
    "# Neither the name of Caltech nor the names of its contributors may be used\n",
    "# to endorse or promote products derived from this software without specific\n",
    "# prior written permission.\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Image generators for training convolutional neural networks.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from skimage.measure import regionprops\n",
    "from skimage.segmentation import clear_border\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.python.keras.preprocessing.image import Iterator\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "from deepcell.utils.retinanet_anchor_utils import anchor_targets_bbox\n",
    "from deepcell.utils.retinanet_anchor_utils import anchors_for_shape\n",
    "from deepcell.utils.retinanet_anchor_utils import guess_shapes\n",
    "\n",
    "from deepcell.image_generators import _transform_masks\n",
    "from deepcell.image_generators import ImageFullyConvDataGenerator\n",
    "from deepcell.image_generators import MovieDataGenerator\n",
    "\n",
    "\n",
    "class RetinaNetGenerator(ImageFullyConvDataGenerator):\n",
    "    \"\"\"Generates batches of tensor image data with real-time data augmentation.\n",
    "    The data will be looped over (in batches).\n",
    "\n",
    "    Args:\n",
    "        featurewise_center: boolean, set input mean to 0 over the dataset,\n",
    "            feature-wise.\n",
    "        samplewise_center: boolean, set each sample mean to 0.\n",
    "        featurewise_std_normalization: boolean, divide inputs by std\n",
    "            of the dataset, feature-wise.\n",
    "        samplewise_std_normalization: boolean, divide each input by its std.\n",
    "        zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
    "        zca_whitening: boolean, apply ZCA whitening.\n",
    "        rotation_range: int, degree range for random rotations.\n",
    "        width_shift_range: float, 1-D array-like or int\n",
    "            float: fraction of total width, if < 1, or pixels if >= 1.\n",
    "            1-D array-like: random elements from the array.\n",
    "            int: integer number of pixels from interval\n",
    "                (-width_shift_range, +width_shift_range)\n",
    "            With width_shift_range=2 possible values are ints [-1, 0, +1],\n",
    "            same as with width_shift_range=[-1, 0, +1],\n",
    "            while with width_shift_range=1.0 possible values are floats in\n",
    "            the interval [-1.0, +1.0).\n",
    "        shear_range: float, shear Intensity\n",
    "            (Shear angle in counter-clockwise direction in degrees)\n",
    "        zoom_range: float or [lower, upper], Range for random zoom.\n",
    "            If a float, [lower, upper] = [1-zoom_range, 1+zoom_range].\n",
    "        channel_shift_range: float, range for random channel shifts.\n",
    "        fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n",
    "            Default is 'nearest'. Points outside the boundaries of the input\n",
    "            are filled according to the given mode:\n",
    "                'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
    "                'nearest':  aaaaaaaa|abcd|dddddddd\n",
    "                'reflect':  abcddcba|abcd|dcbaabcd\n",
    "                'wrap':  abcdabcd|abcd|abcdabcd\n",
    "        cval: float or int, value used for points outside the boundaries\n",
    "            when fill_mode = \"constant\".\n",
    "        horizontal_flip: boolean, randomly flip inputs horizontally.\n",
    "        vertical_flip: boolean, randomly flip inputs vertically.\n",
    "        rescale: rescaling factor. Defaults to None. If None or 0, no rescaling\n",
    "            is applied, otherwise we multiply the data by the value provided\n",
    "            (before applying any other transformation).\n",
    "        preprocessing_function: function that will be implied on each input.\n",
    "            The function will run after the image is resized and augmented.\n",
    "            The function should take one argument:\n",
    "            one image (Numpy tensor with rank 3),\n",
    "            and should output a Numpy tensor with the same shape.\n",
    "        data_format: One of {\"channels_first\", \"channels_last\"}.\n",
    "            \"channels_last\" mode means that the images should have shape\n",
    "                (samples, height, width, channels),\n",
    "            \"channels_first\" mode means that the images should have shape\n",
    "                (samples, channels, height, width).\n",
    "            It defaults to the image_data_format value found in your\n",
    "                Keras config file at \"~/.keras/keras.json\".\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "        validation_split: float, fraction of images reserved for validation\n",
    "            (strictly between 0 and 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def flow(self,\n",
    "             train_dict,\n",
    "             compute_shapes=guess_shapes,\n",
    "             min_objects=3,\n",
    "             num_classes=1,\n",
    "             clear_borders=False,\n",
    "             include_masks=False,\n",
    "             include_final_detection_layer=False,\n",
    "             panoptic=False,\n",
    "             transforms=['watershed'],\n",
    "             transforms_kwargs={},\n",
    "             assoc_head=False,\n",
    "             anchor_params=None,\n",
    "             pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "             batch_size=32,\n",
    "             shuffle=False,\n",
    "             seed=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png'):\n",
    "        \"\"\"Generates batches of augmented/normalized data with given arrays.\n",
    "\n",
    "        Args:\n",
    "            train_dict: dictionary of X and y tensors. Both should be rank 4.\n",
    "            compute_shapes: function to determine the shapes of the anchors\n",
    "            min_classes: images with fewer than 'min_objects' are ignored\n",
    "            num_classes: number of classes to predict\n",
    "            clear_borders: boolean, whether to use clear_border on y.\n",
    "            include_masks: boolean, train on mask data (MaskRCNN).\n",
    "            batch_size: int (default: 1).\n",
    "            shuffle: boolean (default: True).\n",
    "            seed: int (default: None).\n",
    "            save_to_dir: None or str (default: None).\n",
    "                This allows you to optionally specify a directory\n",
    "                to which to save the augmented pictures being generated\n",
    "                (useful for visualizing what you are doing).\n",
    "            save_prefix: str (default: \"\"). Prefix to use for filenames of\n",
    "                saved pictures (only relevant if save_to_dir is set).\n",
    "            save_format: one of \"png\", \"jpeg\". Default: \"png\".\n",
    "                (only relevant if save_to_dir is set)\n",
    "\n",
    "        Returns:\n",
    "            An Iterator yielding tuples of (x, y) where x is a numpy array\n",
    "            of image data and y is a numpy array of labels of the same shape.\n",
    "        \"\"\"\n",
    "        return RetinaNetIterator(\n",
    "            train_dict,\n",
    "            self,\n",
    "            compute_shapes=compute_shapes,\n",
    "            min_objects=min_objects,\n",
    "            num_classes=num_classes,\n",
    "            clear_borders=clear_borders,\n",
    "            include_masks=include_masks,\n",
    "            include_final_detection_layer=include_final_detection_layer,\n",
    "            panoptic=panoptic,\n",
    "            transforms=transforms,\n",
    "            transforms_kwargs=transforms_kwargs,\n",
    "            assoc_head=assoc_head,\n",
    "            anchor_params=anchor_params,\n",
    "            pyramid_levels=pyramid_levels,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            data_format=self.data_format,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format)\n",
    "\n",
    "\n",
    "class RetinaNetIterator(Iterator):\n",
    "    \"\"\"Iterator yielding data from Numpy arrayss (X and y).\n",
    "\n",
    "    Adapted from https://github.com/fizyr/keras-retinanet.\n",
    "\n",
    "    Args:\n",
    "        train_dict: dictionary consisting of numpy arrays for X and y.\n",
    "        image_data_generator: Instance of ImageDataGenerator\n",
    "            to use for random transformations and normalization.\n",
    "        compute_shapes: functor for generating shapes, based on the model.\n",
    "        min_objects: Integer, image with fewer than min_objects are ignored.\n",
    "        num_classes: Integer, number of classes for classification.\n",
    "        clear_borders: Boolean, whether to call clear_border on y.\n",
    "        include_masks: Boolean, whether to yield mask data.\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        data_format: String, one of 'channels_first', 'channels_last'.\n",
    "        save_to_dir: Optional directory where to save the pictures\n",
    "            being yielded, in a viewable format. This is useful\n",
    "            for visualizing the random transformations being\n",
    "            applied, for debugging purposes.\n",
    "        save_prefix: String prefix to use for saving sample\n",
    "            images (if save_to_dir is set).\n",
    "        save_format: Format to use for saving sample images\n",
    "            (if save_to_dir is set).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_dict,\n",
    "                 image_data_generator,\n",
    "                 compute_shapes=guess_shapes,\n",
    "                 anchor_params=None,\n",
    "                 pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "                 min_objects=3,\n",
    "                 num_classes=1,\n",
    "                 clear_borders=False,\n",
    "                 include_masks=False,\n",
    "                 panoptic=False,\n",
    "                 include_final_detection_layer=False,\n",
    "                 transforms=['watershed'],\n",
    "                 transforms_kwargs={},\n",
    "                 assoc_head=False,\n",
    "                 batch_size=32,\n",
    "                 shuffle=False,\n",
    "                 seed=None,\n",
    "                 data_format='channels_last',\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png'):\n",
    "        X, y = train_dict['X'], train_dict['y']\n",
    "\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError('Training batches and labels should have the same'\n",
    "                             'length. Found X.shape: {} y.shape: {}'.format(\n",
    "                                 X.shape, y.shape))\n",
    "\n",
    "        if X.ndim != 4:\n",
    "            raise ValueError('Input data in `RetinaNetIterator` '\n",
    "                             'should have rank 4. You passed an array '\n",
    "                             'with shape', X.shape)\n",
    "\n",
    "        self.x = np.asarray(X, dtype=K.floatx())\n",
    "        self.y = np.asarray(y, dtype='int32')\n",
    "\n",
    "        # `compute_shapes` changes based on the model backbone.\n",
    "        self.compute_shapes = compute_shapes\n",
    "        self.anchor_params = anchor_params\n",
    "        self.pyramid_levels = [int(l[1:]) for l in pyramid_levels]\n",
    "        self.min_objects = min_objects\n",
    "        self.num_classes = num_classes\n",
    "        self.include_masks = include_masks\n",
    "        self.include_final_detection_layer = include_final_detection_layer\n",
    "        self.panoptic = panoptic\n",
    "        self.transforms = transforms\n",
    "        self.transforms_kwargs = transforms_kwargs\n",
    "        self.assoc_head = assoc_head\n",
    "        self.channel_axis = 3 if data_format == 'channels_last' else 1\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.data_format = data_format\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "\n",
    "        self.y_semantic_list = []  # optional semantic segmentation targets\n",
    "\n",
    "        # Add semantic segmentation targets if panoptic segmentation\n",
    "        # flag is True\n",
    "        if panoptic:\n",
    "            # Create a list of all the semantic targets. We need to be able\n",
    "            # to have multiple semantic heads\n",
    "            # Add all the keys that contain y_semantic\n",
    "            for key in train_dict:\n",
    "                if 'y_semantic' in key:\n",
    "                    self.y_semantic_list.append(train_dict[key])\n",
    "\n",
    "            # Add transformed masks\n",
    "            for transform in transforms:\n",
    "                transform_kwargs = transforms_kwargs.get(transform, dict())\n",
    "                y_transform = _transform_masks(y, transform,\n",
    "                                               data_format=data_format,\n",
    "                                               **transform_kwargs)\n",
    "                y_transform = np.asarray(y_transform, dtype='int32')\n",
    "                self.y_semantic_list.append(y_transform)\n",
    "\n",
    "        invalid_batches = []\n",
    "        # Remove images with small numbers of cells\n",
    "        for b in range(self.x.shape[0]):\n",
    "            y_batch = np.squeeze(self.y[b], axis=self.channel_axis - 1)\n",
    "            y_batch = clear_border(y_batch) if clear_borders else y_batch\n",
    "            y_batch = np.expand_dims(y_batch, axis=self.channel_axis - 1)\n",
    "\n",
    "            self.y[b] = y_batch\n",
    "\n",
    "            if len(np.unique(self.y[b])) - 1 < self.min_objects:\n",
    "                invalid_batches.append(b)\n",
    "\n",
    "        invalid_batches = np.array(invalid_batches, dtype='int')\n",
    "\n",
    "        if invalid_batches.size > 0:\n",
    "            logging.warning('Removing %s of %s images with fewer than %s '\n",
    "                            'objects.', invalid_batches.size, self.x.shape[0],\n",
    "                            self.min_objects)\n",
    "\n",
    "        self.y = np.delete(self.y, invalid_batches, axis=0)\n",
    "        self.x = np.delete(self.x, invalid_batches, axis=0)\n",
    "\n",
    "        self.y_semantic_list = [np.delete(y, invalid_batches, axis=0)\n",
    "                                for y in self.y_semantic_list]\n",
    "\n",
    "        super(RetinaNetIterator, self).__init__(\n",
    "            self.x.shape[0], batch_size, shuffle, seed)\n",
    "\n",
    "    def filter_annotations(self, image, annotations):\n",
    "        \"\"\"Filter annotations by removing those that are outside of the\n",
    "        image bounds or whose width/height < 0.\n",
    "\n",
    "        Args:\n",
    "            image: ndarray, the raw image data.\n",
    "            annotations: dict of annotations including labels and bboxes\n",
    "        \"\"\"\n",
    "        row_axis = 1 if self.data_format == 'channels_first' else 0\n",
    "        invalid_indices = np.where(\n",
    "            (annotations['bboxes'][:, 2] <= annotations['bboxes'][:, 0]) |\n",
    "            (annotations['bboxes'][:, 3] <= annotations['bboxes'][:, 1]) |\n",
    "            (annotations['bboxes'][:, 0] < 0) |\n",
    "            (annotations['bboxes'][:, 1] < 0) |\n",
    "            (annotations['bboxes'][:, 2] > image.shape[row_axis + 1]) |\n",
    "            (annotations['bboxes'][:, 3] > image.shape[row_axis])\n",
    "        )[0]\n",
    "\n",
    "        # delete invalid indices\n",
    "        if invalid_indices.size > 0:\n",
    "            logging.warn('Image with shape {} contains the following invalid '\n",
    "                         'boxes: {}.'.format(\n",
    "                             image.shape,\n",
    "                             annotations['bboxes'][invalid_indices, :]))\n",
    "\n",
    "            for k in annotations.keys():\n",
    "                filtered = np.delete(annotations[k], invalid_indices, axis=0)\n",
    "                annotations[k] = filtered\n",
    "        return annotations\n",
    "\n",
    "    def load_annotations(self, y):\n",
    "        \"\"\"Generate bounding box and label annotations for a tensor\n",
    "\n",
    "        Args:\n",
    "            y: tensor to annotate\n",
    "\n",
    "        Returns:\n",
    "            dict: annotations of bboxes and labels\n",
    "        \"\"\"\n",
    "        labels, bboxes, masks = [], [], []\n",
    "        for prop in regionprops(np.squeeze(y.astype('int'))):\n",
    "            y1, x1, y2, x2 = prop.bbox\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            labels.append(0)  # boolean object detection\n",
    "            masks.append(np.where(y == prop.label, 1, 0))\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        bboxes = np.array(bboxes)\n",
    "        masks = np.array(masks).astype('uint8')\n",
    "\n",
    "        # reshape bboxes in case it is empty.\n",
    "        bboxes = np.reshape(bboxes, (bboxes.shape[0], 4))\n",
    "\n",
    "        annotations = {'labels': labels, 'bboxes': bboxes}\n",
    "        if self.include_masks:\n",
    "            annotations['masks'] = masks\n",
    "\n",
    "        annotations = self.filter_annotations(y, annotations)\n",
    "        return annotations\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros(tuple([len(index_array)] + list(self.x.shape)[1:]))\n",
    "\n",
    "        batch_y_semantic_list = []\n",
    "        for y_sem in self.y_semantic_list:\n",
    "            shape = tuple([len(index_array)] + list(y_sem.shape[1:]))\n",
    "            batch_y_semantic_list.append(np.zeros(shape, dtype=y_sem.dtype))\n",
    "\n",
    "        annotations_list = []\n",
    "\n",
    "        max_shape = []\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            x = self.x[j]\n",
    "            y = self.y[j]\n",
    "\n",
    "            y_semantic_list = [y_sem[j] for y_sem in self.y_semantic_list]\n",
    "\n",
    "            # Apply transformation\n",
    "            x, y_list = self.image_data_generator.random_transform(\n",
    "                x, [y] + y_semantic_list)\n",
    "\n",
    "            y = y_list[0]\n",
    "            y_semantic_list = y_list[1:]\n",
    "\n",
    "            # Find max shape of image data.  Used for masking.\n",
    "            if not max_shape:\n",
    "                max_shape = list(x.shape)\n",
    "            else:\n",
    "                for k in range(len(x.shape)):\n",
    "                    if x.shape[k] > max_shape[k]:\n",
    "                        max_shape[k] = x.shape[k]\n",
    "\n",
    "            # Get the bounding boxes from the transformed masks!\n",
    "            annotations = self.load_annotations(y)\n",
    "            annotations_list.append(annotations)\n",
    "\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "\n",
    "            batch_x[i] = x\n",
    "\n",
    "            for k, y_sem in enumerate(y_semantic_list):\n",
    "                batch_y_semantic_list[k][i] = y_sem\n",
    "\n",
    "        anchors = anchors_for_shape(\n",
    "            batch_x.shape[1:],\n",
    "            pyramid_levels=self.pyramid_levels,\n",
    "            anchor_params=self.anchor_params,\n",
    "            shapes_callback=self.compute_shapes)\n",
    "\n",
    "        regressions, labels = anchor_targets_bbox(\n",
    "            anchors,\n",
    "            batch_x,\n",
    "            annotations_list,\n",
    "            self.num_classes)\n",
    "\n",
    "        max_shape = tuple(max_shape)  # was a list for max shape indexing\n",
    "\n",
    "        print(\"annotations_list: \", annotations_list)          \n",
    "\n",
    "        if self.include_masks:\n",
    "            # masks_batch has shape: (batch size, max_annotations,\n",
    "            #     bbox_x1 + bbox_y1 + bbox_x2 + bbox_y2 + label +\n",
    "            #     width + height + max_image_dimension)\n",
    "            max_annotations = max(len(a['masks']) for a in annotations_list)\n",
    "            masks_batch_shape = (len(index_array), max_annotations,\n",
    "                                 5 + 2 + max_shape[0] * max_shape[1])\n",
    "            masks_batch = np.zeros(masks_batch_shape, dtype=K.floatx())\n",
    "\n",
    "            for i, ann in enumerate(annotations_list):\n",
    "                masks_batch[i, :ann['bboxes'].shape[0], :4] = ann['bboxes']\n",
    "                masks_batch[i, :ann['labels'].shape[0], 4] = ann['labels']\n",
    "                masks_batch[i, :, 5] = max_shape[1]  # width\n",
    "                masks_batch[i, :, 6] = max_shape[0]  # height\n",
    "\n",
    "                # add flattened mask\n",
    "                for j, mask in enumerate(ann['masks']):\n",
    "                    masks_batch[i, j, 7:] = mask.flatten()\n",
    "\n",
    "        if self.save_to_dir:\n",
    "            for i, j in enumerate(index_array):\n",
    "                if self.data_format == 'channels_first':\n",
    "                    img_x = np.expand_dims(batch_x[i, 0, ...], 0)\n",
    "                else:\n",
    "                    img_x = np.expand_dims(batch_x[i, ..., 0], -1)\n",
    "                img = array_to_img(img_x, self.data_format, scale=True)\n",
    "                fname = '{prefix}_{index}_{hash}.{format}'.format(\n",
    "                    prefix=self.save_prefix,\n",
    "                    index=j,\n",
    "                    hash=np.random.randint(1e4),\n",
    "                    format=self.save_format)\n",
    "                img.save(os.path.join(self.save_to_dir, fname))\n",
    "\n",
    "        batch_inputs = batch_x\n",
    "        batch_outputs = [regressions, labels]\n",
    "        \n",
    "        if self.assoc_head:\n",
    "            # batch_inputs = [batch_x, batch_x_bbox]\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.include_masks:\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.include_final_detection_layer:\n",
    "            batch_outputs.append(masks_batch)\n",
    "\n",
    "        batch_outputs.extend(batch_y_semantic_list)\n",
    "\n",
    "        print(\"batch_inputs: \", batch_inputs)\n",
    "        print(\"batch_outputs: \", batch_outputs)\n",
    "\n",
    "        return batch_inputs, batch_outputs\n",
    "\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x. Returns the next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)\n",
    "\n",
    "\n",
    "class RetinaMovieIterator(Iterator):\n",
    "    \"\"\"Iterator yielding data from Numpy arrayss (`X and `y`).\n",
    "\n",
    "    Adapted from https://github.com/fizyr/keras-retinanet.\n",
    "\n",
    "    Args:\n",
    "        train_dict: dictionary consisting of numpy arrays for `X` and `y`.\n",
    "        image_data_generator: Instance of `ImageDataGenerator`\n",
    "            to use for random transformations and normalization.\n",
    "        compute_shapes: functor for generating shapes, based on the model.\n",
    "        min_objects: Integer, image with fewer than `min_objects` are ignored.\n",
    "        num_classes: Integer, number of classes for classification.\n",
    "        clear_borders: Boolean, whether to call `clear_border` on `y`.\n",
    "        include_masks: Boolean, whether to yield mask data.\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        data_format: String, one of `channels_first`, `channels_last`.\n",
    "        save_to_dir: Optional directory where to save the pictures\n",
    "            being yielded, in a viewable format. This is useful\n",
    "            for visualizing the random transformations being\n",
    "            applied, for debugging purposes.\n",
    "        save_prefix: String prefix to use for saving sample\n",
    "            images (if `save_to_dir` is set).\n",
    "        save_format: Format to use for saving sample images\n",
    "            (if `save_to_dir` is set).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_dict,\n",
    "                 movie_data_generator,\n",
    "                 compute_shapes=guess_shapes,\n",
    "                 anchor_params=None,\n",
    "                 pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "                 min_objects=3,\n",
    "                 num_classes=1,\n",
    "                 frames_per_batch=2,\n",
    "                 clear_borders=False,\n",
    "                 include_masks=False,\n",
    "                 include_final_detection_layer=False,\n",
    "                 assoc_head=False,\n",
    "                 panoptic=False,\n",
    "                 transforms=['watershed'],\n",
    "                 transforms_kwargs={},\n",
    "                 batch_size=32,\n",
    "                 shuffle=False,\n",
    "                 seed=None,\n",
    "                 data_format='channels_last',\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png'):\n",
    "        X, y = train_dict['X'], train_dict['y']\n",
    "\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError('Training batches and labels should have the same'\n",
    "                             'length. Found X.shape: {} y.shape: {}'.format(\n",
    "                                 X.shape, y.shape))\n",
    "\n",
    "        if X.ndim != 5:\n",
    "            raise ValueError('Input data in `RetinaNetIterator` '\n",
    "                             'should have rank 5. You passed an array '\n",
    "                             'with shape', X.shape)\n",
    "\n",
    "        self.x = np.asarray(X, dtype=K.floatx())\n",
    "        self.y = np.asarray(y, dtype='int32')\n",
    "\n",
    "        # `compute_shapes` changes based on the model backbone.\n",
    "        self.compute_shapes = compute_shapes\n",
    "        self.anchor_params = anchor_params\n",
    "        self.pyramid_levels = [int(l[1:]) for l in pyramid_levels]\n",
    "        self.min_objects = min_objects\n",
    "        self.num_classes = num_classes\n",
    "        self.frames_per_batch = frames_per_batch\n",
    "        self.include_masks = include_masks\n",
    "        self.include_final_detection_layer = include_final_detection_layer\n",
    "        self.assoc_head = assoc_head\n",
    "        self.panoptic = panoptic\n",
    "        self.transforms = transforms\n",
    "        self.transforms_kwargs = transforms_kwargs\n",
    "        self.channel_axis = 4 if data_format == 'channels_last' else 1\n",
    "        self.time_axis = 1 if data_format == 'channels_last' else 2\n",
    "        self.row_axis = 2 if data_format == 'channels_last' else 3\n",
    "        self.col_axis = 3 if data_format == 'channels_last' else 4\n",
    "        self.movie_data_generator = movie_data_generator\n",
    "        self.data_format = data_format\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "\n",
    "        self.y_semantic_list = []  # optional semantic segmentation targets\n",
    "\n",
    "        if X.shape[self.time_axis] - frames_per_batch < 0:\n",
    "            raise ValueError(\n",
    "                'The number of frames used in each training batch should '\n",
    "                'be less than the number of frames in the training data!')\n",
    "\n",
    "        # Add semantic segmentation targets if panoptic segmentation\n",
    "        # flag is True\n",
    "        print(\"train_dict keys :\")\n",
    "        for key in train_dict:\n",
    "            print(key)\n",
    "        if panoptic:\n",
    "            # Create a list of all the semantic targets. We need to be able\n",
    "            # to have multiple semantic heads\n",
    "            # Add all the keys that contain y_semantic\n",
    "            for key in train_dict:\n",
    "                if 'y_semantic' in key:\n",
    "                    self.y_semantic_list.append(train_dict[key])\n",
    "\n",
    "            # Add transformed masks\n",
    "            for transform in transforms:\n",
    "                transform_kwargs = transforms_kwargs.get(transform, dict())\n",
    "                y_transforms = []\n",
    "                for time in range(y.shape[self.time_axis]):\n",
    "                    if data_format == 'channels_first':\n",
    "                        y_temp = y[:, :, time, ...]\n",
    "                    else:\n",
    "                        y_temp = y[:, time, ...]\n",
    "                    y_temp_transform = _transform_masks(\n",
    "                        y_temp, transform,\n",
    "                        data_format=data_format,\n",
    "                        **transform_kwargs)\n",
    "                    y_temp_transform = np.asarray(y_temp_transform, dtype='int32')\n",
    "                    y_transforms.append(y_temp_transform)\n",
    "\n",
    "                y_transform = np.stack(y_transforms, axis=self.time_axis)\n",
    "                self.y_semantic_list.append(y_transform)\n",
    "\n",
    "        invalid_batches = []\n",
    "        # Remove images with small numbers of cells\n",
    "        for b in range(self.x.shape[0]):\n",
    "            y_batch = np.squeeze(self.y[b], axis=self.channel_axis - 1)\n",
    "            y_batch = clear_border(y_batch) if clear_borders else y_batch\n",
    "            y_batch = np.expand_dims(y_batch, axis=self.channel_axis - 1)\n",
    "\n",
    "            self.y[b] = y_batch\n",
    "\n",
    "            if len(np.unique(self.y[b])) - 1 < self.min_objects:\n",
    "                invalid_batches.append(b)\n",
    "\n",
    "        invalid_batches = np.array(invalid_batches, dtype='int')\n",
    "\n",
    "        if invalid_batches.size > 0:\n",
    "            logging.warning('Removing %s of %s images with fewer than %s '\n",
    "                            'objects.', invalid_batches.size, self.x.shape[0],\n",
    "                            self.min_objects)\n",
    "\n",
    "        self.y = np.delete(self.y, invalid_batches, axis=0)\n",
    "        self.x = np.delete(self.x, invalid_batches, axis=0)\n",
    "\n",
    "        self.y_semantic_list = [np.delete(y, invalid_batches, axis=0)\n",
    "                                for y in self.y_semantic_list]\n",
    "\n",
    "        super(RetinaMovieIterator, self).__init__(\n",
    "            self.x.shape[0], batch_size, shuffle, seed)\n",
    "\n",
    "    def filter_annotations(self, image, annotations):\n",
    "        \"\"\"Filter annotations by removing those that are outside of the\n",
    "        image bounds or whose width/height < 0.\n",
    "\n",
    "        Args:\n",
    "            image: ndarray, the raw image data.\n",
    "            annotations: dict of annotations including `labels` and `bboxes`\n",
    "        \"\"\"\n",
    "        row_axis = 1 if self.data_format == 'channels_first' else 0\n",
    "        invalid_indices = np.where(\n",
    "            (annotations['bboxes'][:, 2] <= annotations['bboxes'][:, 0]) |\n",
    "            (annotations['bboxes'][:, 3] <= annotations['bboxes'][:, 1]) |\n",
    "            (annotations['bboxes'][:, 0] < 0) |\n",
    "            (annotations['bboxes'][:, 1] < 0) |\n",
    "            (annotations['bboxes'][:, 2] > image.shape[row_axis + 1]) |\n",
    "            (annotations['bboxes'][:, 3] > image.shape[row_axis])\n",
    "        )[0]\n",
    "\n",
    "        # delete invalid indices\n",
    "        if invalid_indices.size > 0:\n",
    "            logging.warn('Image with shape {} contains the following invalid '\n",
    "                         'boxes: {}.'.format(\n",
    "                             image.shape,\n",
    "                             annotations['bboxes'][invalid_indices, :]))\n",
    "\n",
    "            for k in annotations.keys():\n",
    "                filtered = np.delete(annotations[k], invalid_indices, axis=0)\n",
    "                annotations[k] = filtered\n",
    "        return annotations\n",
    "\n",
    "    def load_annotations(self, y):\n",
    "        \"\"\"Generate bounding box and label annotations for a tensor\n",
    "\n",
    "        Args:\n",
    "            y: tensor to annotate\n",
    "\n",
    "        Returns:\n",
    "            annotations: dict of `bboxes` and `labels`\n",
    "        \"\"\"\n",
    "        labels, bboxes, masks = [], [], []\n",
    "        channel_axis = 1 if self.data_format == 'channels_first' else -1\n",
    "\n",
    "        y_cropped = []\n",
    "        max_width = 0\n",
    "        max_height = 0\n",
    "        \n",
    "        for prop in regionprops(np.squeeze(y.astype('int'))):\n",
    "            y1, x1, y2, x2 = prop.bbox\n",
    "            if x2-x1 > max_width:\n",
    "                max_width = x2-x1\n",
    "            if y2-y1 > max_height:\n",
    "                max_height = y2-y1\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            labels.append(0)  # boolean object detection\n",
    "            masks.append(np.where(y == prop.label, 1, 0))\n",
    "            \n",
    "        labels = np.array(labels)\n",
    "        bboxes = np.array(bboxes)\n",
    "        masks = np.array(masks).astype('uint8')\n",
    "\n",
    "        # reshape bboxes in case it is empty.\n",
    "        bboxes = np.reshape(bboxes, (bboxes.shape[0], 4))\n",
    "\n",
    "        annotations = {'labels': labels, 'bboxes': bboxes}\n",
    "\n",
    "        if self.include_masks:\n",
    "            annotations['masks'] = masks\n",
    "\n",
    "\n",
    "        if self.assoc_head:\n",
    "            y_transform = to_categorical(y.squeeze(channel_axis))\n",
    "            if self.data_format == 'channels_first':\n",
    "                y_transform = np.rollaxis(y_transform, y.ndim - 1, 1)\n",
    "            N = y_transform.shape[-1]\n",
    "            print(\"y_transform.shape\", y_transform.shape)\n",
    "            print(\"bboxes.shape[0], max_width, max_height, N\", bboxes.shape[0], max_width, max_height, N)\n",
    "            \n",
    "            assoc_head = np.zeros((bboxes.shape[0], max_height, max_width, N))\n",
    "            for i in range(bboxes.shape[0]):\n",
    "                x1, y1, x2, y2 = bboxes[i]\n",
    "                assoc_head[i, :y2-y1, :x2-x1, ...] = y_transform[y1:y2, x1:x2, ...]\n",
    "            annotations['assoc_head'] = assoc_head\n",
    "\n",
    "        annotations = self.filter_annotations(y, annotations)\n",
    "        return annotations\n",
    "\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        if self.data_format == 'channels_first':\n",
    "            batch_x = np.zeros((len(index_array),\n",
    "                                self.x.shape[1],\n",
    "                                self.frames_per_batch,\n",
    "                                self.x.shape[3],\n",
    "                                self.x.shape[4]))\n",
    "        else:\n",
    "            batch_x = np.zeros(tuple([len(index_array), self.frames_per_batch] +\n",
    "                                     list(self.x.shape)[2:]))\n",
    "\n",
    "        if self.panoptic:\n",
    "            if self.data_format == 'channels_first':\n",
    "                batch_y_semantic_list = [np.zeros(tuple([len(index_array),\n",
    "                                                         y_semantic.shape[1],\n",
    "                                                         self.frames_per_batch,\n",
    "                                                         y_semantic.shape[3],\n",
    "                                                         y_semantic.shape[4]]))\n",
    "                                         for y_semantic in self.y_semantic_list]\n",
    "            else:\n",
    "                batch_y_semantic_list = [\n",
    "                    np.zeros(tuple([len(index_array), self.frames_per_batch] +\n",
    "                                   list(y_semantic.shape[2:])))\n",
    "                    for y_semantic in self.y_semantic_list\n",
    "                ]\n",
    "\n",
    "        annotations_list = [[] for _ in range(self.frames_per_batch)]\n",
    "\n",
    "        max_shape = []\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            last_frame = self.x.shape[self.time_axis] - self.frames_per_batch\n",
    "            time_start = np.random.randint(0, high=last_frame)\n",
    "            time_end = time_start + self.frames_per_batch\n",
    "            times = list(np.arange(time_start, time_end))\n",
    "\n",
    "            if self.time_axis == 1:\n",
    "                x = self.x[j, time_start:time_end, ...]\n",
    "                y = self.y[j, time_start:time_end, ...]\n",
    "            elif self.time_axis == 2:\n",
    "                x = self.x[j, :, time_start:time_end, ...]\n",
    "                y = self.y[j, :, time_start:time_end, ...]\n",
    "\n",
    "            if self.panoptic:\n",
    "                if self.time_axis == 1:\n",
    "                    y_semantic_list = [y_semantic[j, time_start:time_end, ...]\n",
    "                                       for y_semantic in self.y_semantic_list]\n",
    "                elif self.time_axis == 2:\n",
    "                    y_semantic_list = [y_semantic[j, :, time_start:time_end, ...]\n",
    "                                       for y_semantic in self.y_semantic_list]\n",
    "\n",
    "            # Apply transformation\n",
    "            if self.panoptic:\n",
    "                x, y_list = self.movie_data_generator.random_transform(x, [y] + y_semantic_list)\n",
    "                y = y_list[0]\n",
    "                y_semantic_list = y_list[1:]\n",
    "            else:\n",
    "                x, y = self.movie_data_generator.random_transform(x, y)\n",
    "\n",
    "            x = self.movie_data_generator.standardize(x)\n",
    "\n",
    "            # Find max shape of image data.  Used for masking.\n",
    "            if not max_shape:\n",
    "                max_shape = list(x.shape)\n",
    "            else:\n",
    "                for k in range(len(x.shape)):\n",
    "                    if x.shape[k] > max_shape[k]:\n",
    "                        max_shape[k] = x.shape[k]\n",
    "\n",
    "            # Get the bounding boxes from the transformed masks!\n",
    "            for idx_time, time in enumerate(times):\n",
    "                if self.time_axis == 1:\n",
    "                    annotations = self.load_annotations(y[idx_time])\n",
    "                elif self.time_axis == 2:\n",
    "                    annotations = self.load_annotations(y[:, idx_time, ...])\n",
    "                annotations_list[idx_time].append(annotations)\n",
    "\n",
    "            batch_x[i] = x\n",
    "\n",
    "            if self.panoptic:\n",
    "                for k in range(len(y_semantic_list)):\n",
    "                    batch_y_semantic_list[k][i] = y_semantic_list[k]\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            batch_x_shape = [batch_x.shape[1], batch_x.shape[3], batch_x.shape[4]]\n",
    "        else:\n",
    "            batch_x_shape = batch_x.shape[2:]\n",
    "\n",
    "        anchors = anchors_for_shape(\n",
    "            batch_x_shape,\n",
    "            pyramid_levels=self.pyramid_levels,\n",
    "            anchor_params=self.anchor_params,\n",
    "            shapes_callback=self.compute_shapes)\n",
    "\n",
    "        regressions_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            batch_x_frame = batch_x[:, :, 0, ...]\n",
    "        else:\n",
    "            batch_x_frame = batch_x[:, 0, ...]\n",
    "        for idx, time in enumerate(times):\n",
    "            regressions, labels = anchor_targets_bbox(\n",
    "                anchors,\n",
    "                batch_x_frame,\n",
    "                annotations_list[idx],\n",
    "                self.num_classes)\n",
    "            regressions_list.append(regressions)\n",
    "            labels_list.append(labels)\n",
    "\n",
    "        regressions = np.stack(regressions_list, axis=self.time_axis)\n",
    "        # print(\"regressions.shape: \", regressions.shape)\n",
    "        labels = np.stack(labels_list, axis=self.time_axis)\n",
    "        # print(\"labels.shape: \", labels.shape)\n",
    "\n",
    "        # was a list for max shape indexing\n",
    "        max_shape = tuple([max_shape[self.row_axis - 1],\n",
    "                           max_shape[self.col_axis - 1]])\n",
    "\n",
    "\n",
    "        # print(\"annotations_list: \", annotations_list)\n",
    "\n",
    "        if self.assoc_head:\n",
    "            print(\"annotations['assoc_head'].shape\", annotations['assoc_head'].shape)\n",
    "            N = annotations['assoc_head'].shape[-1]\n",
    "            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "            annotations_list_flatten = flatten(annotations_list)\n",
    "#             print(\"len(annotations_list_flatten)\", len(annotations_list_flatten))\n",
    "            max_annotations = max(len(a['assoc_head']) for a in annotations_list_flatten)\n",
    "            max_width = max(a['assoc_head'].shape[1] for a in annotations_list_flatten)\n",
    "            max_height = max(a['assoc_head'].shape[2] for a in annotations_list_flatten)\n",
    "            max_N = max(a['assoc_head'].shape[-1] for a in annotations_list_flatten)\n",
    "            print(\"max_width, max_height: \", max_width, max_height)\n",
    "            assoc_heads_batch_shape = (len(index_array), self.frames_per_batch, max_annotations,\n",
    "                                       max_width, max_height, max_N)\n",
    "                                       #5 + 2 + 1 + max_shape[0] * max_shape[1] * N)\n",
    "            assoc_heads_batch = np.zeros(assoc_heads_batch_shape, dtype=K.floatx())\n",
    "            print(\"assoc_heads_batch_shape: \", assoc_heads_batch_shape)\n",
    "            for idx_time, time in enumerate(times):\n",
    "                annotations_frame = annotations_list[idx_time]\n",
    "                for idx_batch, ann in enumerate(annotations_frame):\n",
    "#                     assoc_heads_batch[idx_batch, idx_time, :ann['bboxes'].shape[0], :4] = ann['bboxes']\n",
    "#                     assoc_heads_batch[idx_batch, idx_time, :ann['labels'].shape[0], 4] = ann['labels']\n",
    "#                     assoc_heads_batch[idx_batch, idx_time, :, 5] = max_shape[1]  # width\n",
    "#                     assoc_heads_batch[idx_batch, idx_time, :, 6] = max_shape[0]  # height\n",
    "#                     assoc_heads_batch[idx_batch, idx_time, :, 7] = ann['assoc_head'].shape[-1] # num classes\n",
    "                    \n",
    "                    # add flattened association head\n",
    "                    for idx_mask, assoc_head in enumerate(ann['assoc_head']):\n",
    "                        print(\"assoc_head.shape\", assoc_head.shape)\n",
    "                        dim_x, dim_y, dim_z = assoc_head.shape\n",
    "                        assoc_heads_batch[idx_batch, idx_time, idx_mask,:dim_x, :dim_y, :dim_z] = assoc_head\n",
    "#                         assoc_heads_batch[idx_batch, idx_time, \n",
    "#                                           idx_mask, 8:8+assoc_head.flatten().shape[0]] = assoc_head.flatten()\n",
    "\n",
    "\n",
    "        if self.include_masks:\n",
    "            # masks_batch has shape: (batch size, max_annotations,\n",
    "            #     bbox_x1 + bbox_y1 + bbox_x2 + bbox_y2 + label +\n",
    "            #     width + height + max_image_dimension)\n",
    "\n",
    "            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "            annotations_list_flatten = flatten(annotations_list)\n",
    "            max_annotations = max(len(a['masks']) for a in annotations_list_flatten)\n",
    "            masks_batch_shape = (len(index_array), self.frames_per_batch, max_annotations,\n",
    "                                 5 + 2 + max_shape[0] * max_shape[1])\n",
    "            # print(\"masks_batch_shape: \", masks_batch_shape)\n",
    "            masks_batch = np.zeros(masks_batch_shape, dtype=K.floatx())\n",
    "            \n",
    "            batch_x_bbox_shape = (len(index_array), self.frames_per_batch, max_annotations, 4)\n",
    "            batch_x_bbox = np.zeros(batch_x_bbox_shape, dtype=K.floatx())\n",
    "\n",
    "            for idx_time, time in enumerate(times):\n",
    "                annotations_frame = annotations_list[idx_time]\n",
    "                for idx_batch, ann in enumerate(annotations_frame):\n",
    "                    batch_x_bbox[idx_batch, idx_time, :ann['bboxes'].shape[0], :4] = ann['bboxes']\n",
    "                    \n",
    "                    masks_batch[idx_batch, idx_time, :ann['bboxes'].shape[0], :4] = ann['bboxes']\n",
    "                    masks_batch[idx_batch, idx_time, :ann['labels'].shape[0], 4] = ann['labels']\n",
    "                    masks_batch[idx_batch, idx_time, :, 5] = max_shape[1]  # width\n",
    "                    masks_batch[idx_batch, idx_time, :, 6] = max_shape[0]  # height\n",
    "\n",
    "                    # add flattened mask\n",
    "                    for idx_mask, mask in enumerate(ann['masks']):\n",
    "                        masks_batch[idx_batch, idx_time, idx_mask, 7:] = mask.flatten()\n",
    "\n",
    "        if self.save_to_dir:\n",
    "            for i, j in enumerate(index_array):\n",
    "                for frame in range(batch_x.shape[self.time_axis]):\n",
    "                    if self.time_axis == 2:\n",
    "                        img = array_to_img(batch_x[i, :, frame], self.data_format, scale=True)\n",
    "                    else:\n",
    "                        img = array_to_img(batch_x[i, frame], self.data_format, scale=True)\n",
    "                    fname = '{prefix}_{index}_{hash}.{format}'.format(\n",
    "                        prefix=self.save_prefix,\n",
    "                        index=j,\n",
    "                        hash=np.random.randint(1e4),\n",
    "                        format=self.save_format)\n",
    "                    img.save(os.path.join(self.save_to_dir, fname))\n",
    "\n",
    "        batch_inputs = batch_x\n",
    "        batch_outputs = [regressions, labels]\n",
    "        print(\"annotations['bboxes'].shape\", annotations['bboxes'].shape)\n",
    "    \n",
    "        if self.include_masks:\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.include_final_detection_layer:\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.panoptic:\n",
    "            batch_outputs += batch_y_semantic_list\n",
    "        if self.assoc_head:\n",
    "            batch_inputs = [batch_x, batch_x_bbox, assoc_heads_batch]\n",
    "            # batch_outputs.append(assoc_heads_batch)\n",
    "        return batch_inputs, batch_outputs\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x. Returns the next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)\n",
    "\n",
    "\n",
    "class RetinaMovieDataGenerator(MovieDataGenerator):\n",
    "    \"\"\"Generates batches of tensor image data with real-time data augmentation.\n",
    "    The data will be looped over (in batches).\n",
    "\n",
    "    Args:\n",
    "        featurewise_center: boolean, set input mean to 0 over the dataset,\n",
    "            feature-wise.\n",
    "        samplewise_center: boolean, set each sample mean to 0.\n",
    "        featurewise_std_normalization: boolean, divide inputs by std\n",
    "            of the dataset, feature-wise.\n",
    "        samplewise_std_normalization: boolean, divide each input by its std.\n",
    "        zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
    "        zca_whitening: boolean, apply ZCA whitening.\n",
    "        rotation_range: int, degree range for random rotations.\n",
    "        width_shift_range: float, 1-D array-like or int\n",
    "            float: fraction of total width, if < 1, or pixels if >= 1.\n",
    "            1-D array-like: random elements from the array.\n",
    "            int: integer number of pixels from interval\n",
    "                `(-width_shift_range, +width_shift_range)`\n",
    "            With `width_shift_range=2` possible values are ints [-1, 0, +1],\n",
    "            same as with `width_shift_range=[-1, 0, +1]`,\n",
    "            while with `width_shift_range=1.0` possible values are floats in\n",
    "            the interval [-1.0, +1.0).\n",
    "        shear_range: float, shear Intensity\n",
    "            (Shear angle in counter-clockwise direction in degrees)\n",
    "        zoom_range: float or [lower, upper], Range for random zoom.\n",
    "            If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
    "        channel_shift_range: float, range for random channel shifts.\n",
    "        fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n",
    "            Default is 'nearest'. Points outside the boundaries of the input\n",
    "            are filled according to the given mode:\n",
    "                'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
    "                'nearest':  aaaaaaaa|abcd|dddddddd\n",
    "                'reflect':  abcddcba|abcd|dcbaabcd\n",
    "                'wrap':  abcdabcd|abcd|abcdabcd\n",
    "        cval: float or int, value used for points outside the boundaries\n",
    "            when `fill_mode = \"constant\"`.\n",
    "        horizontal_flip: boolean, randomly flip inputs horizontally.\n",
    "        vertical_flip: boolean, randomly flip inputs vertically.\n",
    "        rescale: rescaling factor. Defaults to None. If None or 0, no rescaling\n",
    "            is applied, otherwise we multiply the data by the value provided\n",
    "            (before applying any other transformation).\n",
    "        preprocessing_function: function that will be implied on each input.\n",
    "            The function will run after the image is resized and augmented.\n",
    "            The function should take one argument:\n",
    "            one image (Numpy tensor with rank 3),\n",
    "            and should output a Numpy tensor with the same shape.\n",
    "        data_format: One of {\"channels_first\", \"channels_last\"}.\n",
    "            \"channels_last\" mode means that the images should have shape\n",
    "                `(samples, height, width, channels)`,\n",
    "            \"channels_first\" mode means that the images should have shape\n",
    "                `(samples, channels, height, width)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "                Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "        validation_split: float, fraction of images reserved for validation\n",
    "            (strictly between 0 and 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def flow(self,\n",
    "             train_dict,\n",
    "             batch_size=1,\n",
    "             frames_per_batch=5,\n",
    "             compute_shapes=guess_shapes,\n",
    "             num_classes=1,\n",
    "             clear_borders=False,\n",
    "             include_masks=False,\n",
    "             include_final_detection_layer=False,\n",
    "             panoptic=False,\n",
    "             assoc_head=False,\n",
    "             transforms=['watershed'],\n",
    "             transforms_kwargs={},\n",
    "             anchor_params=None,\n",
    "             pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "             shuffle=False,\n",
    "             seed=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png'):\n",
    "        \"\"\"Generates batches of augmented/normalized data with given arrays.\n",
    "\n",
    "        Args:\n",
    "            train_dict: dictionary of X and y tensors. Both should be rank 5.\n",
    "            frames_per_batch: int (default: 10).\n",
    "                size of z axis in generated batches\n",
    "            batch_size: int (default: 1).\n",
    "            shuffle: boolean (default: True).\n",
    "            seed: int (default: None).\n",
    "            save_to_dir: None or str (default: None).\n",
    "                This allows you to optionally specify a directory\n",
    "                to which to save the augmented pictures being generated\n",
    "                (useful for visualizing what you are doing).\n",
    "            save_prefix: str (default: `''`). Prefix to use for filenames of\n",
    "                saved pictures (only relevant if `save_to_dir` is set).\n",
    "            save_format: one of \"png\", \"jpeg\". Default: \"png\".\n",
    "                (only relevant if `save_to_dir` is set)\n",
    "\n",
    "        Returns:\n",
    "            An Iterator yielding tuples of `(x, y)` where `x` is a numpy array\n",
    "            of image data and `y` is a numpy array of labels of the same shape.\n",
    "        \"\"\"\n",
    "        return RetinaMovieIterator(\n",
    "            train_dict,\n",
    "            self,\n",
    "            compute_shapes=compute_shapes,\n",
    "            num_classes=num_classes,\n",
    "            clear_borders=clear_borders,\n",
    "            include_masks=include_masks,\n",
    "            include_final_detection_layer=include_final_detection_layer,\n",
    "            assoc_head=assoc_head,\n",
    "            panoptic=panoptic,\n",
    "            transforms=transforms,\n",
    "            transforms_kwargs=transforms_kwargs,\n",
    "            anchor_params=anchor_params,\n",
    "            pyramid_levels=pyramid_levels,\n",
    "            batch_size=batch_size,\n",
    "            frames_per_batch=frames_per_batch,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            data_format=self.data_format,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from deepcell.image_generators import RetinaMovieDataGenerator\n",
    "\n",
    "datagen = RetinaMovieDataGenerator(\n",
    "    rotation_range=180,\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "datagen_val = RetinaMovieDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dict keys :\n",
      "X\n",
      "y\n",
      "daughters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0218 22:03:27.976842 140303936599872 <ipython-input-196-651f3736db62>:644] Removing 2 of 192 images with fewer than 3 objects.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dict keys :\n",
      "X\n",
      "y\n",
      "daughters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0218 22:03:29.392986 140303936599872 <ipython-input-196-651f3736db62>:644] Removing 1 of 48 images with fewer than 3 objects.\n"
     ]
    }
   ],
   "source": [
    "train_data = datagen.flow(\n",
    "    train_dict,\n",
    "    batch_size=1,\n",
    "    include_masks=True,\n",
    "    include_final_detection_layer=True,\n",
    "    assoc_head=True,\n",
    "    # panoptic=True,\n",
    "    frames_per_batch=fpb,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params)\n",
    "\n",
    "val_data = datagen_val.flow(\n",
    "    test_dict,\n",
    "    batch_size=1,\n",
    "    include_masks=True,\n",
    "    include_final_detection_layer=True,\n",
    "    assoc_head=True,\n",
    "    # panoptic=True,\n",
    "    frames_per_batch=fpb,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_transform.shape (154, 182, 16)\n",
      "bboxes.shape[0], max_width, max_height, N 10 55 49 16\n",
      "Epoch 1/10\n",
      "y_transform.shape (154, 182, 18)\n",
      "bboxes.shape[0], max_width, max_height, N 11 54 54 18\n",
      "y_transform.shape (154, 182, 18)\n",
      "bboxes.shape[0], max_width, max_height, N 12 43 73 18\n",
      "annotations['assoc_head'].shape (12, 73, 43, 18)\n",
      "max_width, max_height:  73 55\n",
      "assoc_heads_batch_shape:  (1, 3, 12, 73, 55, 18)\n",
      "assoc_head.shape (49, 55, 16)\n",
      "assoc_head.shape (49, 55, 16)\n",
      "assoc_head.shape (49, 55, 16)\n",
      "assoc_head.shape (49, 55, 16)\n",
      "assoc_head.shape (49, 55, 16)\n",
      "assoc_head.shape (49, 55, 16)\n",
      "assoc_head.shape (49, 55, 16)\n",
      "assoc_head.shape (49, 55, 16)\n",
      "assoc_head.shape (49, 55, 16)\n",
      "assoc_head.shape (49, 55, 16)\n",
      "assoc_head.shape (54, 54, 18)\n",
      "assoc_head.shape (54, 54, 18)\n",
      "assoc_head.shape (54, 54, 18)\n",
      "assoc_head.shape (54, 54, 18)\n",
      "assoc_head.shape (54, 54, 18)\n",
      "assoc_head.shape (54, 54, 18)\n",
      "assoc_head.shape (54, 54, 18)\n",
      "assoc_head.shape (54, 54, 18)\n",
      "assoc_head.shape (54, 54, 18)\n",
      "assoc_head.shape (54, 54, 18)\n",
      "assoc_head.shape (54, 54, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "assoc_head.shape (73, 43, 18)\n",
      "annotations['bboxes'].shape (12, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected assoc_head_input to have shape (None, None, None, None, 2) but got array with shape (3, 12, 73, 55, 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-b7b73d750888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                      \u001b[0mframes_per_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                      weighted_average=True),\n\u001b[0;32m---> 29\u001b[0;31m             prediction_model)]\n\u001b[0m\u001b[1;32m     30\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m   1152\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[0;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    383\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    386\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected assoc_head_input to have shape (None, None, None, None, 2) but got array with shape (3, 12, 73, 55, 18)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_transform.shape (154, 182, 10)\n",
      "bboxes.shape[0], max_width, max_height, N 8 48 55 10\n",
      "y_transform.shape (154, 182, 10)\n",
      "bboxes.shape[0], max_width, max_height, N 9 66 39 10\n",
      "y_transform.shape (154, 182, 10)\n",
      "bboxes.shape[0], max_width, max_height, N 9 58 38 10\n",
      "annotations['assoc_head'].shape (9, 38, 58, 10)\n",
      "max_width, max_height:  55 66\n",
      "assoc_heads_batch_shape:  (1, 3, 9, 55, 66, 10)\n",
      "assoc_head.shape (55, 48, 10)\n",
      "assoc_head.shape (55, 48, 10)\n",
      "assoc_head.shape (55, 48, 10)\n",
      "assoc_head.shape (55, 48, 10)\n",
      "assoc_head.shape (55, 48, 10)\n",
      "assoc_head.shape (55, 48, 10)\n",
      "assoc_head.shape (55, 48, 10)\n",
      "assoc_head.shape (55, 48, 10)\n",
      "assoc_head.shape (39, 66, 10)\n",
      "assoc_head.shape (39, 66, 10)\n",
      "assoc_head.shape (39, 66, 10)\n",
      "assoc_head.shape (39, 66, 10)\n",
      "assoc_head.shape (39, 66, 10)\n",
      "assoc_head.shape (39, 66, 10)\n",
      "assoc_head.shape (39, 66, 10)\n",
      "assoc_head.shape (39, 66, 10)\n",
      "assoc_head.shape (39, 66, 10)\n",
      "assoc_head.shape (38, 58, 10)\n",
      "assoc_head.shape (38, 58, 10)\n",
      "assoc_head.shape (38, 58, 10)\n",
      "assoc_head.shape (38, 58, 10)\n",
      "assoc_head.shape (38, 58, 10)\n",
      "assoc_head.shape (38, 58, 10)\n",
      "assoc_head.shape (38, 58, 10)\n",
      "assoc_head.shape (38, 58, 10)\n",
      "assoc_head.shape (38, 58, 10)\n",
      "annotations['bboxes'].shape (9, 4)\n",
      "y_transform.shape (154, 182, 8)\n",
      "bboxes.shape[0], max_width, max_height, N 6 34 46 8\n",
      "y_transform.shape (154, 182, 9)\n",
      "bboxes.shape[0], max_width, max_height, N 8 41 29 9\n",
      "y_transform.shape (154, 182, 9)\n",
      "bboxes.shape[0], max_width, max_height, N 7 36 49 9\n",
      "annotations['assoc_head'].shape (7, 49, 36, 9)\n",
      "max_width, max_height:  49 41\n",
      "assoc_heads_batch_shape:  (1, 3, 8, 49, 41, 9)\n",
      "assoc_head.shape (46, 34, 8)\n",
      "assoc_head.shape (46, 34, 8)\n",
      "assoc_head.shape (46, 34, 8)\n",
      "assoc_head.shape (46, 34, 8)\n",
      "assoc_head.shape (46, 34, 8)\n",
      "assoc_head.shape (46, 34, 8)\n",
      "assoc_head.shape (29, 41, 9)\n",
      "assoc_head.shape (29, 41, 9)\n",
      "assoc_head.shape (29, 41, 9)\n",
      "assoc_head.shape (29, 41, 9)\n",
      "assoc_head.shape (29, 41, 9)\n",
      "assoc_head.shape (29, 41, 9)\n",
      "assoc_head.shape (29, 41, 9)\n",
      "assoc_head.shape (29, 41, 9)\n",
      "assoc_head.shape (49, 36, 9)\n",
      "assoc_head.shape (49, 36, 9)\n",
      "assoc_head.shape (49, 36, 9)\n",
      "assoc_head.shape (49, 36, 9)\n",
      "assoc_head.shape (49, 36, 9)\n",
      "assoc_head.shape (49, 36, 9)\n",
      "assoc_head.shape (49, 36, 9)\n",
      "annotations['bboxes'].shape (7, 4)\n",
      "y_transform.shape (154, 182, 19)\n",
      "bboxes.shape[0], max_width, max_height, N 12 52 44 19\n",
      "y_transform.shape (154, 182, 19)\n",
      "bboxes.shape[0], max_width, max_height, N 12 41 44 19\n",
      "y_transform.shape (154, 182, 19)\n",
      "bboxes.shape[0], max_width, max_height, N 11 45 58 19\n",
      "annotations['assoc_head'].shape (11, 58, 45, 19)\n",
      "max_width, max_height:  58 52\n",
      "assoc_heads_batch_shape:  (1, 3, 12, 58, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 52, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (44, 41, 19)\n",
      "assoc_head.shape (58, 45, 19)\n",
      "assoc_head.shape (58, 45, 19)\n",
      "assoc_head.shape (58, 45, 19)\n",
      "assoc_head.shape (58, 45, 19)\n",
      "assoc_head.shape (58, 45, 19)\n",
      "assoc_head.shape (58, 45, 19)\n",
      "assoc_head.shape (58, 45, 19)\n",
      "assoc_head.shape (58, 45, 19)\n",
      "assoc_head.shape (58, 45, 19)\n",
      "assoc_head.shape (58, 45, 19)\n",
      "assoc_head.shape (58, 45, 19)\n",
      "annotations['bboxes'].shape (11, 4)\n",
      "y_transform.shape (154, 182, 9)\n",
      "bboxes.shape[0], max_width, max_height, N 8 47 34 9\n",
      "y_transform.shape (154, 182, 10)\n",
      "bboxes.shape[0], max_width, max_height, N 9 35 37 10\n",
      "y_transform.shape (154, 182, 9)\n",
      "bboxes.shape[0], max_width, max_height, N 8 42 65 9\n",
      "annotations['assoc_head'].shape (8, 65, 42, 9)\n",
      "max_width, max_height:  65 47\n",
      "assoc_heads_batch_shape:  (1, 3, 9, 65, 47, 10)\n",
      "assoc_head.shape (34, 47, 9)\n",
      "assoc_head.shape (34, 47, 9)\n",
      "assoc_head.shape (34, 47, 9)\n",
      "assoc_head.shape (34, 47, 9)\n",
      "assoc_head.shape (34, 47, 9)\n",
      "assoc_head.shape (34, 47, 9)\n",
      "assoc_head.shape (34, 47, 9)\n",
      "assoc_head.shape (34, 47, 9)\n",
      "assoc_head.shape (37, 35, 10)\n",
      "assoc_head.shape (37, 35, 10)\n",
      "assoc_head.shape (37, 35, 10)\n",
      "assoc_head.shape (37, 35, 10)\n",
      "assoc_head.shape (37, 35, 10)\n",
      "assoc_head.shape (37, 35, 10)\n",
      "assoc_head.shape (37, 35, 10)\n",
      "assoc_head.shape (37, 35, 10)\n",
      "assoc_head.shape (37, 35, 10)\n",
      "assoc_head.shape (65, 42, 9)\n",
      "assoc_head.shape (65, 42, 9)\n",
      "assoc_head.shape (65, 42, 9)\n",
      "assoc_head.shape (65, 42, 9)\n",
      "assoc_head.shape (65, 42, 9)\n",
      "assoc_head.shape (65, 42, 9)\n",
      "assoc_head.shape (65, 42, 9)\n",
      "assoc_head.shape (65, 42, 9)\n",
      "annotations['bboxes'].shape (8, 4)\n",
      "y_transform.shape (154, 182, 13)\n",
      "bboxes.shape[0], max_width, max_height, N 7 50 47 13\n",
      "y_transform.shape (154, 182, 13)\n",
      "bboxes.shape[0], max_width, max_height, N 8 48 51 13\n",
      "y_transform.shape (154, 182, 13)\n",
      "bboxes.shape[0], max_width, max_height, N 11 85 154 13\n",
      "annotations['assoc_head'].shape (11, 154, 85, 13)\n",
      "max_width, max_height:  154 85\n",
      "assoc_heads_batch_shape:  (1, 3, 11, 154, 85, 13)\n",
      "assoc_head.shape (47, 50, 13)\n",
      "assoc_head.shape (47, 50, 13)\n",
      "assoc_head.shape (47, 50, 13)\n",
      "assoc_head.shape (47, 50, 13)\n",
      "assoc_head.shape (47, 50, 13)\n",
      "assoc_head.shape (47, 50, 13)\n",
      "assoc_head.shape (47, 50, 13)\n",
      "assoc_head.shape (51, 48, 13)\n",
      "assoc_head.shape (51, 48, 13)\n",
      "assoc_head.shape (51, 48, 13)\n",
      "assoc_head.shape (51, 48, 13)\n",
      "assoc_head.shape (51, 48, 13)\n",
      "assoc_head.shape (51, 48, 13)\n",
      "assoc_head.shape (51, 48, 13)\n",
      "assoc_head.shape (51, 48, 13)\n",
      "assoc_head.shape (154, 85, 13)\n",
      "assoc_head.shape (154, 85, 13)\n",
      "assoc_head.shape (154, 85, 13)\n",
      "assoc_head.shape (154, 85, 13)\n",
      "assoc_head.shape (154, 85, 13)\n",
      "assoc_head.shape (154, 85, 13)\n",
      "assoc_head.shape (154, 85, 13)\n",
      "assoc_head.shape (154, 85, 13)\n",
      "assoc_head.shape (154, 85, 13)\n",
      "assoc_head.shape (154, 85, 13)\n",
      "assoc_head.shape (154, 85, 13)\n",
      "annotations['bboxes'].shape (11, 4)\n",
      "y_transform.shape (154, 182, 9)\n",
      "bboxes.shape[0], max_width, max_height, N 8 61 34 9\n",
      "y_transform.shape (154, 182, 9)\n",
      "bboxes.shape[0], max_width, max_height, N 8 52 56 9\n",
      "y_transform.shape (154, 182, 8)\n",
      "bboxes.shape[0], max_width, max_height, N 6 59 51 8\n",
      "annotations['assoc_head'].shape (6, 51, 59, 8)\n",
      "max_width, max_height:  56 61\n",
      "assoc_heads_batch_shape:  (1, 3, 8, 56, 61, 9)\n",
      "assoc_head.shape (34, 61, 9)\n",
      "assoc_head.shape (34, 61, 9)\n",
      "assoc_head.shape (34, 61, 9)\n",
      "assoc_head.shape (34, 61, 9)\n",
      "assoc_head.shape (34, 61, 9)\n",
      "assoc_head.shape (34, 61, 9)\n",
      "assoc_head.shape (34, 61, 9)\n",
      "assoc_head.shape (34, 61, 9)\n",
      "assoc_head.shape (56, 52, 9)\n",
      "assoc_head.shape (56, 52, 9)\n",
      "assoc_head.shape (56, 52, 9)\n",
      "assoc_head.shape (56, 52, 9)\n",
      "assoc_head.shape (56, 52, 9)\n",
      "assoc_head.shape (56, 52, 9)\n",
      "assoc_head.shape (56, 52, 9)\n",
      "assoc_head.shape (56, 52, 9)\n",
      "assoc_head.shape (51, 59, 8)\n",
      "assoc_head.shape (51, 59, 8)\n",
      "assoc_head.shape (51, 59, 8)\n",
      "assoc_head.shape (51, 59, 8)\n",
      "assoc_head.shape (51, 59, 8)\n",
      "assoc_head.shape (51, 59, 8)\n",
      "annotations['bboxes'].shape (6, 4)\n",
      "y_transform.shape (154, 182, 14)\n",
      "bboxes.shape[0], max_width, max_height, N 9 59 46 14\n",
      "y_transform.shape (154, 182, 14)\n",
      "bboxes.shape[0], max_width, max_height, N 10 52 47 14\n",
      "y_transform.shape (154, 182, 14)\n",
      "bboxes.shape[0], max_width, max_height, N 11 60 45 14\n",
      "annotations['assoc_head'].shape (11, 45, 60, 14)\n",
      "max_width, max_height:  47 60\n",
      "assoc_heads_batch_shape:  (1, 3, 11, 47, 60, 14)\n",
      "assoc_head.shape (46, 59, 14)\n",
      "assoc_head.shape (46, 59, 14)\n",
      "assoc_head.shape (46, 59, 14)\n",
      "assoc_head.shape (46, 59, 14)\n",
      "assoc_head.shape (46, 59, 14)\n",
      "assoc_head.shape (46, 59, 14)\n",
      "assoc_head.shape (46, 59, 14)\n",
      "assoc_head.shape (46, 59, 14)\n",
      "assoc_head.shape (46, 59, 14)\n",
      "assoc_head.shape (47, 52, 14)\n",
      "assoc_head.shape (47, 52, 14)\n",
      "assoc_head.shape (47, 52, 14)\n",
      "assoc_head.shape (47, 52, 14)\n",
      "assoc_head.shape (47, 52, 14)\n",
      "assoc_head.shape (47, 52, 14)\n",
      "assoc_head.shape (47, 52, 14)\n",
      "assoc_head.shape (47, 52, 14)\n",
      "assoc_head.shape (47, 52, 14)\n",
      "assoc_head.shape (47, 52, 14)\n",
      "assoc_head.shape (45, 60, 14)\n",
      "assoc_head.shape (45, 60, 14)\n",
      "assoc_head.shape (45, 60, 14)\n",
      "assoc_head.shape (45, 60, 14)\n",
      "assoc_head.shape (45, 60, 14)\n",
      "assoc_head.shape (45, 60, 14)\n",
      "assoc_head.shape (45, 60, 14)\n",
      "assoc_head.shape (45, 60, 14)\n",
      "assoc_head.shape (45, 60, 14)\n",
      "assoc_head.shape (45, 60, 14)\n",
      "assoc_head.shape (45, 60, 14)\n",
      "annotations['bboxes'].shape (11, 4)\n",
      "y_transform.shape (154, 182, 8)\n",
      "bboxes.shape[0], max_width, max_height, N 5 57 54 8\n",
      "y_transform.shape (154, 182, 8)\n",
      "bboxes.shape[0], max_width, max_height, N 6 46 52 8\n",
      "y_transform.shape (154, 182, 6)\n",
      "bboxes.shape[0], max_width, max_height, N 4 45 47 6\n",
      "annotations['assoc_head'].shape (4, 47, 45, 6)\n",
      "max_width, max_height:  54 57\n",
      "assoc_heads_batch_shape:  (1, 3, 6, 54, 57, 8)\n",
      "assoc_head.shape (54, 57, 8)\n",
      "assoc_head.shape (54, 57, 8)\n",
      "assoc_head.shape (54, 57, 8)\n",
      "assoc_head.shape (54, 57, 8)\n",
      "assoc_head.shape (54, 57, 8)\n",
      "assoc_head.shape (52, 46, 8)\n",
      "assoc_head.shape (52, 46, 8)\n",
      "assoc_head.shape (52, 46, 8)\n",
      "assoc_head.shape (52, 46, 8)\n",
      "assoc_head.shape (52, 46, 8)\n",
      "assoc_head.shape (52, 46, 8)\n",
      "assoc_head.shape (47, 45, 6)\n",
      "assoc_head.shape (47, 45, 6)\n",
      "assoc_head.shape (47, 45, 6)\n",
      "assoc_head.shape (47, 45, 6)\n",
      "annotations['bboxes'].shape (4, 4)\n",
      "y_transform.shape (154, 182, 6)\n",
      "bboxes.shape[0], max_width, max_height, N 4 48 53 6\n",
      "y_transform.shape (154, 182, 7)\n",
      "bboxes.shape[0], max_width, max_height, N 5 58 37 7\n",
      "y_transform.shape (154, 182, 6)\n",
      "bboxes.shape[0], max_width, max_height, N 4 46 40 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations['assoc_head'].shape (4, 40, 46, 6)\n",
      "max_width, max_height:  53 58\n",
      "assoc_heads_batch_shape:  (1, 3, 5, 53, 58, 7)\n",
      "assoc_head.shape (53, 48, 6)\n",
      "assoc_head.shape (53, 48, 6)\n",
      "assoc_head.shape (53, 48, 6)\n",
      "assoc_head.shape (53, 48, 6)\n",
      "assoc_head.shape (37, 58, 7)\n",
      "assoc_head.shape (37, 58, 7)\n",
      "assoc_head.shape (37, 58, 7)\n",
      "assoc_head.shape (37, 58, 7)\n",
      "assoc_head.shape (37, 58, 7)\n",
      "assoc_head.shape (40, 46, 6)\n",
      "assoc_head.shape (40, 46, 6)\n",
      "assoc_head.shape (40, 46, 6)\n",
      "assoc_head.shape (40, 46, 6)\n",
      "annotations['bboxes'].shape (4, 4)\n",
      "y_transform.shape (154, 182, 11)\n",
      "bboxes.shape[0], max_width, max_height, N 10 56 37 11\n",
      "y_transform.shape (154, 182, 10)\n",
      "bboxes.shape[0], max_width, max_height, N 9 42 50 10\n",
      "y_transform.shape (154, 182, 11)\n",
      "bboxes.shape[0], max_width, max_height, N 10 35 54 11\n",
      "annotations['assoc_head'].shape (10, 54, 35, 11)\n",
      "max_width, max_height:  54 56\n",
      "assoc_heads_batch_shape:  (1, 3, 10, 54, 56, 11)\n",
      "assoc_head.shape (37, 56, 11)\n",
      "assoc_head.shape (37, 56, 11)\n",
      "assoc_head.shape (37, 56, 11)\n",
      "assoc_head.shape (37, 56, 11)\n",
      "assoc_head.shape (37, 56, 11)\n",
      "assoc_head.shape (37, 56, 11)\n",
      "assoc_head.shape (37, 56, 11)\n",
      "assoc_head.shape (37, 56, 11)\n",
      "assoc_head.shape (37, 56, 11)\n",
      "assoc_head.shape (37, 56, 11)\n",
      "assoc_head.shape (50, 42, 10)\n",
      "assoc_head.shape (50, 42, 10)\n",
      "assoc_head.shape (50, 42, 10)\n",
      "assoc_head.shape (50, 42, 10)\n",
      "assoc_head.shape (50, 42, 10)\n",
      "assoc_head.shape (50, 42, 10)\n",
      "assoc_head.shape (50, 42, 10)\n",
      "assoc_head.shape (50, 42, 10)\n",
      "assoc_head.shape (50, 42, 10)\n",
      "assoc_head.shape (54, 35, 11)\n",
      "assoc_head.shape (54, 35, 11)\n",
      "assoc_head.shape (54, 35, 11)\n",
      "assoc_head.shape (54, 35, 11)\n",
      "assoc_head.shape (54, 35, 11)\n",
      "assoc_head.shape (54, 35, 11)\n",
      "assoc_head.shape (54, 35, 11)\n",
      "assoc_head.shape (54, 35, 11)\n",
      "assoc_head.shape (54, 35, 11)\n",
      "assoc_head.shape (54, 35, 11)\n",
      "annotations['bboxes'].shape (10, 4)\n",
      "y_transform.shape (154, 182, 12)\n",
      "bboxes.shape[0], max_width, max_height, N 8 44 44 12\n",
      "y_transform.shape (154, 182, 12)\n",
      "bboxes.shape[0], max_width, max_height, N 8 40 36 12\n",
      "y_transform.shape (154, 182, 12)\n",
      "bboxes.shape[0], max_width, max_height, N 7 51 39 12\n",
      "annotations['assoc_head'].shape (7, 39, 51, 12)\n",
      "max_width, max_height:  44 51\n",
      "assoc_heads_batch_shape:  (1, 3, 8, 44, 51, 12)\n",
      "assoc_head.shape (44, 44, 12)\n",
      "assoc_head.shape (44, 44, 12)\n",
      "assoc_head.shape (44, 44, 12)\n",
      "assoc_head.shape (44, 44, 12)\n",
      "assoc_head.shape (44, 44, 12)\n",
      "assoc_head.shape (44, 44, 12)\n",
      "assoc_head.shape (44, 44, 12)\n",
      "assoc_head.shape (44, 44, 12)\n",
      "assoc_head.shape (36, 40, 12)\n",
      "assoc_head.shape (36, 40, 12)\n",
      "assoc_head.shape (36, 40, 12)\n",
      "assoc_head.shape (36, 40, 12)\n",
      "assoc_head.shape (36, 40, 12)\n",
      "assoc_head.shape (36, 40, 12)\n",
      "assoc_head.shape (36, 40, 12)\n",
      "assoc_head.shape (36, 40, 12)\n",
      "assoc_head.shape (39, 51, 12)\n",
      "assoc_head.shape (39, 51, 12)\n",
      "assoc_head.shape (39, 51, 12)\n",
      "assoc_head.shape (39, 51, 12)\n",
      "assoc_head.shape (39, 51, 12)\n",
      "assoc_head.shape (39, 51, 12)\n",
      "assoc_head.shape (39, 51, 12)\n",
      "annotations['bboxes'].shape (7, 4)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "from deepcell.callbacks import RedirectModel, Evaluate\n",
    "\n",
    "iou_threshold = 0.5\n",
    "score_threshold = 0.01\n",
    "max_detections = 100\n",
    "\n",
    "model.fit_generator(\n",
    "    train_data,\n",
    "    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "    epochs=n_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=X_test.shape[0] // batch_size,\n",
    "    callbacks=[\n",
    "        callbacks.LearningRateScheduler(lr_sched),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            os.path.join(MODEL_DIR, model_name + '.h5'),\n",
    "            monitor='val_loss',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False),\n",
    "        RedirectModel(\n",
    "            Evaluate(val_data,\n",
    "                     iou_threshold=iou_threshold,\n",
    "                     score_threshold=score_threshold,\n",
    "                     max_detections=max_detections,\n",
    "                     frames_per_batch=fpb,\n",
    "                     weighted_average=True),\n",
    "            prediction_model)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in batch_outputs:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
