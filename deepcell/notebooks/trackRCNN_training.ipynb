{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "https://github.com/vanvalenlab/deepcell-tf/blob/master/scripts/feature_pyramids/RetinaNet%20-%20Movie.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import errno\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -\n",
      "X.shape: (192, 30, 154, 182, 1)\n",
      "y.shape: (192, 30, 154, 182, 1)\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.data_utils import get_data\n",
    "from deepcell.utils.tracking_utils import load_trks\n",
    "\n",
    "DATA_DIR = '/data/training_data/cells/3T3/NIH/movie'\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'nuclear_movie_3T3_0-2_same.trks')\n",
    "\n",
    "# Load Information for hardcoded image size training\n",
    "seed = 1\n",
    "test_size = .2\n",
    "train_dict, test_dict = get_data(DATA_FILE, mode='siamese_daughters', seed=seed, test_size=test_size)\n",
    "X_train, y_train = train_dict['X'], train_dict['y']\n",
    "X_test, y_test = test_dict['X'], test_dict['y']\n",
    "\n",
    "print(' -\\nX.shape: {}\\ny.shape: {}'.format(train_dict['X'].shape, train_dict['y'].shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Download four different sets of data (saves to ~/.keras/datasets)\n",
    "filename_3T3 = '3T3_NIH.trks'\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.tracked.nih_3t3.load_tracked_data(filename_3T3)\n",
    "print('3T3 -\\nX.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Contants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up other required filepaths\n",
    "PREFIX = os.path.relpath(os.path.dirname(DATA_FILE), DATA_DIR)\n",
    "ROOT_DIR = '/data' # mounted volume\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models', PREFIX))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs', PREFIX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each head of the model uses its own loss\n",
    "from deepcell.losses import RetinaNetLosses\n",
    "from deepcell.losses import discriminative_instance_loss\n",
    "\n",
    "\n",
    "sigma = 3.0\n",
    "alpha = 0.25\n",
    "gamma = 2.0\n",
    "iou_threshold = 0.5\n",
    "max_detections = 100\n",
    "mask_size = (28, 28)\n",
    "\n",
    "retinanet_losses = RetinaNetLosses(\n",
    "    sigma=sigma, alpha=alpha, gamma=gamma,\n",
    "    iou_threshold=iou_threshold,\n",
    "    mask_size=mask_size)\n",
    "\n",
    "loss = {\n",
    "    'regression': retinanet_losses.regress_loss,\n",
    "    'classification': retinanet_losses.classification_loss,\n",
    "    'association_features': discriminative_instance_loss,\n",
    "    'masks': retinanet_losses.mask_loss,\n",
    "    'final_detection': retinanet_losses.final_detection_loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RetinaMask Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "model_name = 'trackrcnn_model'\n",
    "backbone = 'resnet50'  # vgg16, vgg19, resnet50, densenet121, densenet169, densenet201\n",
    "\n",
    "n_epoch = 10  # Number of training epochs\n",
    "lr = 1e-5\n",
    "\n",
    "optimizer = Adam(lr=lr, clipnorm=0.001)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=lr, decay=0.99)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "num_classes = 1  # \"object\" is the only class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.retinanet_anchor_utils import get_anchor_parameters\n",
    "\n",
    "flat_shape = [y_train.shape[0] * y_train.shape[1]] + list(y_train.shape[2:])\n",
    "flat_y = np.reshape(y_train, tuple(flat_shape)).astype('int')\n",
    "\n",
    "# Generate backbone information from the data\n",
    "backbone_levels, pyramid_levels, anchor_params = get_anchor_parameters(flat_y)\n",
    "\n",
    "fpb = 3  # number of frames in each training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0207 16:33:32.061117 140181711468352 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0207 16:33:48.614712 140181711468352 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0207 16:34:00.203530 140181711468352 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py:255: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rois.shape (?, ?, ?, ?, ?, ?)\n"
     ]
    }
   ],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "# Pass frames_per_batch > 1 to enable 3D mode!\n",
    "model = model_zoo.RetinaMask(\n",
    "    backbone=backbone,\n",
    "    input_shape=X_train.shape[2:],\n",
    "    frames_per_batch=fpb,\n",
    "    class_specific_filter=False,\n",
    "    num_classes=num_classes,\n",
    "    backbone_levels=backbone_levels,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params\n",
    ")\n",
    "\n",
    "prediction_model = model\n",
    "\n",
    "# prediction_model = model_zoo.retinanet_bbox(\n",
    "#     model,\n",
    "#     panoptic=False,\n",
    "#     frames_per_batch=fpb,\n",
    "#     max_detections=100,\n",
    "#     anchor_params=anchor_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50_retinanet_mask\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_input (InputLayer)        [(None, 3, 154, 182, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 3, 154, 182,  0           image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 3, 154, 182,  6           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 3, 39, 46, 25 229760      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 3, 77, 91, 64 9728        time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "C2_reduced (Conv3D)             (None, 3, 39, 46, 25 65792       time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "C1_reduced (Conv3D)             (None, 3, 77, 91, 25 16640       time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "P2_upsampled (UpsampleLike)     (None, None, None, N 0           C2_reduced[0][0]                 \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 3, 20, 23, 51 1460096     time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "P1_merged (Add)                 (None, 3, 77, 91, 25 0           C1_reduced[0][0]                 \n",
      "                                                                 P2_upsampled[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "C3_reduced (Conv3D)             (None, 3, 20, 23, 25 131328      time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "P1 (Conv3D)                     (None, 3, 77, 91, 25 590080      P1_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P3_upsampled (UpsampleLike)     (None, None, None, N 0           C3_reduced[0][0]                 \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "P2_merged (Add)                 (None, 3, 39, 46, 25 0           C2_reduced[0][0]                 \n",
      "                                                                 P3_upsampled[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "boxes_input (InputLayer)        [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "upsamplelike (UpsampleLike)     (None, None, None, N 0           P1[0][0]                         \n",
      "                                                                 image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "P2 (Conv3D)                     (None, 3, 39, 46, 25 590080      P2_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P3 (Conv3D)                     (None, 3, 20, 23, 25 590080      C3_reduced[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "roialign (RoiAlign)             (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 upsamplelike[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "regression_submodel (Model)     (None, 3, None, 4)   7327780     P1[0][0]                         \n",
      "                                                                 P2[0][0]                         \n",
      "                                                                 P3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "classification_submodel (Model) (None, 3, None, 1)   7141129     P1[0][0]                         \n",
      "                                                                 P2[0][0]                         \n",
      "                                                                 P3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "mask_submodel (TimeDistributed) (None, None, None, 2 2950657     roialign[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, None, None, 1 2950657     roialign[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, None, None, N 1032960     roialign[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "regression (Concatenate)        (None, 3, None, 4)   0           regression_submodel[1][0]        \n",
      "                                                                 regression_submodel[2][0]        \n",
      "                                                                 regression_submodel[3][0]        \n",
      "__________________________________________________________________________________________________\n",
      "classification (Concatenate)    (None, 3, None, 1)   0           classification_submodel[1][0]    \n",
      "                                                                 classification_submodel[2][0]    \n",
      "                                                                 classification_submodel[3][0]    \n",
      "__________________________________________________________________________________________________\n",
      "masks (ConcatenateBoxes)        (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 mask_submodel[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_detection (ConcatenateBox (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "association_features (Concatena (None, None, None, N 0           boxes_input[0][0]                \n",
      "                                                                 time_distributed_14[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 24,847,285\n",
      "Trainable params: 24,837,173\n",
      "Non-trainable params: 10,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0207 16:34:01.839611 140181711468352 training_utils.py:1101] Output mask_submodel missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to mask_submodel.\n",
      "W0207 16:34:01.840869 140181711468352 training_utils.py:1101] Output time_distributed_9 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to time_distributed_9.\n",
      "W0207 16:34:01.841755 140181711468352 training_utils.py:1101] Output time_distributed_14 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to time_distributed_14.\n",
      "W0207 16:34:01.948335 140181711468352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/deepcell/losses.py:332: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0207 16:34:02.945173 140181711468352 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "reduction_indices is deprecated, use axis instead\n",
      "W0207 16:34:03.036840 140181711468352 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/deepcell/losses.py:203: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\n",
      "W0207 16:34:03.039930 140181711468352 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/deepcell/losses.py:204: The name tf.matrix_set_diag is deprecated. Please use tf.linalg.set_diag instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RetinaMask Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016-2019 The Van Valen Lab at the California Institute of\n",
    "# Technology (Caltech), with support from the Paul Allen Family Foundation,\n",
    "# Google, & National Institutes of Health (NIH) under Grant U24CA224309-01.\n",
    "# All rights reserved.\n",
    "#\n",
    "# Licensed under a modified Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.github.com/vanvalenlab/deepcell-tf/LICENSE\n",
    "#\n",
    "# The Work provided may be used for non-commercial academic purposes only.\n",
    "# For any other use of the Work, including commercial use, please contact:\n",
    "# vanvalenlab@gmail.com\n",
    "#\n",
    "# Neither the name of Caltech nor the names of its contributors may be used\n",
    "# to endorse or promote products derived from this software without specific\n",
    "# prior written permission.\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Image generators for training convolutional neural networks.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from skimage.measure import regionprops\n",
    "from skimage.segmentation import clear_border\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.python.keras.preprocessing.image import Iterator\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "from deepcell.utils.retinanet_anchor_utils import anchor_targets_bbox\n",
    "from deepcell.utils.retinanet_anchor_utils import anchors_for_shape\n",
    "from deepcell.utils.retinanet_anchor_utils import guess_shapes\n",
    "\n",
    "from deepcell.image_generators import _transform_masks\n",
    "from deepcell.image_generators import ImageFullyConvDataGenerator\n",
    "from deepcell.image_generators import MovieDataGenerator\n",
    "\n",
    "\n",
    "class RetinaNetGenerator(ImageFullyConvDataGenerator):\n",
    "    \"\"\"Generates batches of tensor image data with real-time data augmentation.\n",
    "    The data will be looped over (in batches).\n",
    "\n",
    "    Args:\n",
    "        featurewise_center: boolean, set input mean to 0 over the dataset,\n",
    "            feature-wise.\n",
    "        samplewise_center: boolean, set each sample mean to 0.\n",
    "        featurewise_std_normalization: boolean, divide inputs by std\n",
    "            of the dataset, feature-wise.\n",
    "        samplewise_std_normalization: boolean, divide each input by its std.\n",
    "        zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
    "        zca_whitening: boolean, apply ZCA whitening.\n",
    "        rotation_range: int, degree range for random rotations.\n",
    "        width_shift_range: float, 1-D array-like or int\n",
    "            float: fraction of total width, if < 1, or pixels if >= 1.\n",
    "            1-D array-like: random elements from the array.\n",
    "            int: integer number of pixels from interval\n",
    "                (-width_shift_range, +width_shift_range)\n",
    "            With width_shift_range=2 possible values are ints [-1, 0, +1],\n",
    "            same as with width_shift_range=[-1, 0, +1],\n",
    "            while with width_shift_range=1.0 possible values are floats in\n",
    "            the interval [-1.0, +1.0).\n",
    "        shear_range: float, shear Intensity\n",
    "            (Shear angle in counter-clockwise direction in degrees)\n",
    "        zoom_range: float or [lower, upper], Range for random zoom.\n",
    "            If a float, [lower, upper] = [1-zoom_range, 1+zoom_range].\n",
    "        channel_shift_range: float, range for random channel shifts.\n",
    "        fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n",
    "            Default is 'nearest'. Points outside the boundaries of the input\n",
    "            are filled according to the given mode:\n",
    "                'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
    "                'nearest':  aaaaaaaa|abcd|dddddddd\n",
    "                'reflect':  abcddcba|abcd|dcbaabcd\n",
    "                'wrap':  abcdabcd|abcd|abcdabcd\n",
    "        cval: float or int, value used for points outside the boundaries\n",
    "            when fill_mode = \"constant\".\n",
    "        horizontal_flip: boolean, randomly flip inputs horizontally.\n",
    "        vertical_flip: boolean, randomly flip inputs vertically.\n",
    "        rescale: rescaling factor. Defaults to None. If None or 0, no rescaling\n",
    "            is applied, otherwise we multiply the data by the value provided\n",
    "            (before applying any other transformation).\n",
    "        preprocessing_function: function that will be implied on each input.\n",
    "            The function will run after the image is resized and augmented.\n",
    "            The function should take one argument:\n",
    "            one image (Numpy tensor with rank 3),\n",
    "            and should output a Numpy tensor with the same shape.\n",
    "        data_format: One of {\"channels_first\", \"channels_last\"}.\n",
    "            \"channels_last\" mode means that the images should have shape\n",
    "                (samples, height, width, channels),\n",
    "            \"channels_first\" mode means that the images should have shape\n",
    "                (samples, channels, height, width).\n",
    "            It defaults to the image_data_format value found in your\n",
    "                Keras config file at \"~/.keras/keras.json\".\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "        validation_split: float, fraction of images reserved for validation\n",
    "            (strictly between 0 and 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def flow(self,\n",
    "             train_dict,\n",
    "             compute_shapes=guess_shapes,\n",
    "             min_objects=3,\n",
    "             num_classes=1,\n",
    "             clear_borders=False,\n",
    "             include_masks=False,\n",
    "             include_final_detection_layer=False,\n",
    "             panoptic=False,\n",
    "             transforms=['watershed'],\n",
    "             transforms_kwargs={},\n",
    "             assoc_head=False,\n",
    "             anchor_params=None,\n",
    "             pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "             batch_size=32,\n",
    "             shuffle=False,\n",
    "             seed=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png'):\n",
    "        \"\"\"Generates batches of augmented/normalized data with given arrays.\n",
    "\n",
    "        Args:\n",
    "            train_dict: dictionary of X and y tensors. Both should be rank 4.\n",
    "            compute_shapes: function to determine the shapes of the anchors\n",
    "            min_classes: images with fewer than 'min_objects' are ignored\n",
    "            num_classes: number of classes to predict\n",
    "            clear_borders: boolean, whether to use clear_border on y.\n",
    "            include_masks: boolean, train on mask data (MaskRCNN).\n",
    "            batch_size: int (default: 1).\n",
    "            shuffle: boolean (default: True).\n",
    "            seed: int (default: None).\n",
    "            save_to_dir: None or str (default: None).\n",
    "                This allows you to optionally specify a directory\n",
    "                to which to save the augmented pictures being generated\n",
    "                (useful for visualizing what you are doing).\n",
    "            save_prefix: str (default: \"\"). Prefix to use for filenames of\n",
    "                saved pictures (only relevant if save_to_dir is set).\n",
    "            save_format: one of \"png\", \"jpeg\". Default: \"png\".\n",
    "                (only relevant if save_to_dir is set)\n",
    "\n",
    "        Returns:\n",
    "            An Iterator yielding tuples of (x, y) where x is a numpy array\n",
    "            of image data and y is a numpy array of labels of the same shape.\n",
    "        \"\"\"\n",
    "        return RetinaNetIterator(\n",
    "            train_dict,\n",
    "            self,\n",
    "            compute_shapes=compute_shapes,\n",
    "            min_objects=min_objects,\n",
    "            num_classes=num_classes,\n",
    "            clear_borders=clear_borders,\n",
    "            include_masks=include_masks,\n",
    "            include_final_detection_layer=include_final_detection_layer,\n",
    "            panoptic=panoptic,\n",
    "            transforms=transforms,\n",
    "            transforms_kwargs=transforms_kwargs,\n",
    "            assoc_head=assoc_head,\n",
    "            anchor_params=anchor_params,\n",
    "            pyramid_levels=pyramid_levels,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            data_format=self.data_format,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format)\n",
    "\n",
    "\n",
    "class RetinaNetIterator(Iterator):\n",
    "    \"\"\"Iterator yielding data from Numpy arrayss (X and y).\n",
    "\n",
    "    Adapted from https://github.com/fizyr/keras-retinanet.\n",
    "\n",
    "    Args:\n",
    "        train_dict: dictionary consisting of numpy arrays for X and y.\n",
    "        image_data_generator: Instance of ImageDataGenerator\n",
    "            to use for random transformations and normalization.\n",
    "        compute_shapes: functor for generating shapes, based on the model.\n",
    "        min_objects: Integer, image with fewer than min_objects are ignored.\n",
    "        num_classes: Integer, number of classes for classification.\n",
    "        clear_borders: Boolean, whether to call clear_border on y.\n",
    "        include_masks: Boolean, whether to yield mask data.\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        data_format: String, one of 'channels_first', 'channels_last'.\n",
    "        save_to_dir: Optional directory where to save the pictures\n",
    "            being yielded, in a viewable format. This is useful\n",
    "            for visualizing the random transformations being\n",
    "            applied, for debugging purposes.\n",
    "        save_prefix: String prefix to use for saving sample\n",
    "            images (if save_to_dir is set).\n",
    "        save_format: Format to use for saving sample images\n",
    "            (if save_to_dir is set).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_dict,\n",
    "                 image_data_generator,\n",
    "                 compute_shapes=guess_shapes,\n",
    "                 anchor_params=None,\n",
    "                 pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "                 min_objects=3,\n",
    "                 num_classes=1,\n",
    "                 clear_borders=False,\n",
    "                 include_masks=False,\n",
    "                 panoptic=False,\n",
    "                 include_final_detection_layer=False,\n",
    "                 transforms=['watershed'],\n",
    "                 transforms_kwargs={},\n",
    "                 assoc_head=False,\n",
    "                 batch_size=32,\n",
    "                 shuffle=False,\n",
    "                 seed=None,\n",
    "                 data_format='channels_last',\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png'):\n",
    "        X, y = train_dict['X'], train_dict['y']\n",
    "\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError('Training batches and labels should have the same'\n",
    "                             'length. Found X.shape: {} y.shape: {}'.format(\n",
    "                                 X.shape, y.shape))\n",
    "\n",
    "        if X.ndim != 4:\n",
    "            raise ValueError('Input data in `RetinaNetIterator` '\n",
    "                             'should have rank 4. You passed an array '\n",
    "                             'with shape', X.shape)\n",
    "\n",
    "        self.x = np.asarray(X, dtype=K.floatx())\n",
    "        self.y = np.asarray(y, dtype='int32')\n",
    "\n",
    "        # `compute_shapes` changes based on the model backbone.\n",
    "        self.compute_shapes = compute_shapes\n",
    "        self.anchor_params = anchor_params\n",
    "        self.pyramid_levels = [int(l[1:]) for l in pyramid_levels]\n",
    "        self.min_objects = min_objects\n",
    "        self.num_classes = num_classes\n",
    "        self.include_masks = include_masks\n",
    "        self.include_final_detection_layer = include_final_detection_layer\n",
    "        self.panoptic = panoptic\n",
    "        self.transforms = transforms\n",
    "        self.transforms_kwargs = transforms_kwargs\n",
    "        self.assoc_head = assoc_head\n",
    "        self.channel_axis = 3 if data_format == 'channels_last' else 1\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.data_format = data_format\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "\n",
    "        self.y_semantic_list = []  # optional semantic segmentation targets\n",
    "\n",
    "        # Add semantic segmentation targets if panoptic segmentation\n",
    "        # flag is True\n",
    "        if panoptic:\n",
    "            # Create a list of all the semantic targets. We need to be able\n",
    "            # to have multiple semantic heads\n",
    "            # Add all the keys that contain y_semantic\n",
    "            for key in train_dict:\n",
    "                if 'y_semantic' in key:\n",
    "                    self.y_semantic_list.append(train_dict[key])\n",
    "\n",
    "            # Add transformed masks\n",
    "            for transform in transforms:\n",
    "                transform_kwargs = transforms_kwargs.get(transform, dict())\n",
    "                y_transform = _transform_masks(y, transform,\n",
    "                                               data_format=data_format,\n",
    "                                               **transform_kwargs)\n",
    "                y_transform = np.asarray(y_transform, dtype='int32')\n",
    "                self.y_semantic_list.append(y_transform)\n",
    "\n",
    "        invalid_batches = []\n",
    "        # Remove images with small numbers of cells\n",
    "        for b in range(self.x.shape[0]):\n",
    "            y_batch = np.squeeze(self.y[b], axis=self.channel_axis - 1)\n",
    "            y_batch = clear_border(y_batch) if clear_borders else y_batch\n",
    "            y_batch = np.expand_dims(y_batch, axis=self.channel_axis - 1)\n",
    "\n",
    "            self.y[b] = y_batch\n",
    "\n",
    "            if len(np.unique(self.y[b])) - 1 < self.min_objects:\n",
    "                invalid_batches.append(b)\n",
    "\n",
    "        invalid_batches = np.array(invalid_batches, dtype='int')\n",
    "\n",
    "        if invalid_batches.size > 0:\n",
    "            logging.warning('Removing %s of %s images with fewer than %s '\n",
    "                            'objects.', invalid_batches.size, self.x.shape[0],\n",
    "                            self.min_objects)\n",
    "\n",
    "        self.y = np.delete(self.y, invalid_batches, axis=0)\n",
    "        self.x = np.delete(self.x, invalid_batches, axis=0)\n",
    "\n",
    "        self.y_semantic_list = [np.delete(y, invalid_batches, axis=0)\n",
    "                                for y in self.y_semantic_list]\n",
    "\n",
    "        super(RetinaNetIterator, self).__init__(\n",
    "            self.x.shape[0], batch_size, shuffle, seed)\n",
    "\n",
    "    def filter_annotations(self, image, annotations):\n",
    "        \"\"\"Filter annotations by removing those that are outside of the\n",
    "        image bounds or whose width/height < 0.\n",
    "\n",
    "        Args:\n",
    "            image: ndarray, the raw image data.\n",
    "            annotations: dict of annotations including labels and bboxes\n",
    "        \"\"\"\n",
    "        row_axis = 1 if self.data_format == 'channels_first' else 0\n",
    "        invalid_indices = np.where(\n",
    "            (annotations['bboxes'][:, 2] <= annotations['bboxes'][:, 0]) |\n",
    "            (annotations['bboxes'][:, 3] <= annotations['bboxes'][:, 1]) |\n",
    "            (annotations['bboxes'][:, 0] < 0) |\n",
    "            (annotations['bboxes'][:, 1] < 0) |\n",
    "            (annotations['bboxes'][:, 2] > image.shape[row_axis + 1]) |\n",
    "            (annotations['bboxes'][:, 3] > image.shape[row_axis])\n",
    "        )[0]\n",
    "\n",
    "        # delete invalid indices\n",
    "        if invalid_indices.size > 0:\n",
    "            logging.warn('Image with shape {} contains the following invalid '\n",
    "                         'boxes: {}.'.format(\n",
    "                             image.shape,\n",
    "                             annotations['bboxes'][invalid_indices, :]))\n",
    "\n",
    "            for k in annotations.keys():\n",
    "                filtered = np.delete(annotations[k], invalid_indices, axis=0)\n",
    "                annotations[k] = filtered\n",
    "        return annotations\n",
    "\n",
    "    def load_annotations(self, y):\n",
    "        \"\"\"Generate bounding box and label annotations for a tensor\n",
    "\n",
    "        Args:\n",
    "            y: tensor to annotate\n",
    "\n",
    "        Returns:\n",
    "            dict: annotations of bboxes and labels\n",
    "        \"\"\"\n",
    "        labels, bboxes, masks = [], [], []\n",
    "        for prop in regionprops(np.squeeze(y.astype('int'))):\n",
    "            y1, x1, y2, x2 = prop.bbox\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            labels.append(0)  # boolean object detection\n",
    "            masks.append(np.where(y == prop.label, 1, 0))\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        bboxes = np.array(bboxes)\n",
    "        masks = np.array(masks).astype('uint8')\n",
    "\n",
    "        # reshape bboxes in case it is empty.\n",
    "        bboxes = np.reshape(bboxes, (bboxes.shape[0], 4))\n",
    "\n",
    "        annotations = {'labels': labels, 'bboxes': bboxes}\n",
    "        if self.include_masks:\n",
    "            annotations['masks'] = masks\n",
    "\n",
    "        annotations = self.filter_annotations(y, annotations)\n",
    "        return annotations\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros(tuple([len(index_array)] + list(self.x.shape)[1:]))\n",
    "\n",
    "        batch_y_semantic_list = []\n",
    "        for y_sem in self.y_semantic_list:\n",
    "            shape = tuple([len(index_array)] + list(y_sem.shape[1:]))\n",
    "            batch_y_semantic_list.append(np.zeros(shape, dtype=y_sem.dtype))\n",
    "\n",
    "        annotations_list = []\n",
    "\n",
    "        max_shape = []\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            x = self.x[j]\n",
    "            y = self.y[j]\n",
    "\n",
    "            y_semantic_list = [y_sem[j] for y_sem in self.y_semantic_list]\n",
    "\n",
    "            # Apply transformation\n",
    "            x, y_list = self.image_data_generator.random_transform(\n",
    "                x, [y] + y_semantic_list)\n",
    "\n",
    "            y = y_list[0]\n",
    "            y_semantic_list = y_list[1:]\n",
    "\n",
    "            # Find max shape of image data.  Used for masking.\n",
    "            if not max_shape:\n",
    "                max_shape = list(x.shape)\n",
    "            else:\n",
    "                for k in range(len(x.shape)):\n",
    "                    if x.shape[k] > max_shape[k]:\n",
    "                        max_shape[k] = x.shape[k]\n",
    "\n",
    "            # Get the bounding boxes from the transformed masks!\n",
    "            annotations = self.load_annotations(y)\n",
    "            annotations_list.append(annotations)\n",
    "\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "\n",
    "            batch_x[i] = x\n",
    "\n",
    "            for k, y_sem in enumerate(y_semantic_list):\n",
    "                batch_y_semantic_list[k][i] = y_sem\n",
    "\n",
    "        anchors = anchors_for_shape(\n",
    "            batch_x.shape[1:],\n",
    "            pyramid_levels=self.pyramid_levels,\n",
    "            anchor_params=self.anchor_params,\n",
    "            shapes_callback=self.compute_shapes)\n",
    "\n",
    "        regressions, labels = anchor_targets_bbox(\n",
    "            anchors,\n",
    "            batch_x,\n",
    "            annotations_list,\n",
    "            self.num_classes)\n",
    "\n",
    "        max_shape = tuple(max_shape)  # was a list for max shape indexing\n",
    "\n",
    "        print(\"annotations_list: \", annotations_list)          \n",
    "\n",
    "        if self.include_masks:\n",
    "            # masks_batch has shape: (batch size, max_annotations,\n",
    "            #     bbox_x1 + bbox_y1 + bbox_x2 + bbox_y2 + label +\n",
    "            #     width + height + max_image_dimension)\n",
    "            max_annotations = max(len(a['masks']) for a in annotations_list)\n",
    "            masks_batch_shape = (len(index_array), max_annotations,\n",
    "                                 5 + 2 + max_shape[0] * max_shape[1])\n",
    "            masks_batch = np.zeros(masks_batch_shape, dtype=K.floatx())\n",
    "\n",
    "            for i, ann in enumerate(annotations_list):\n",
    "                masks_batch[i, :ann['bboxes'].shape[0], :4] = ann['bboxes']\n",
    "                masks_batch[i, :ann['labels'].shape[0], 4] = ann['labels']\n",
    "                masks_batch[i, :, 5] = max_shape[1]  # width\n",
    "                masks_batch[i, :, 6] = max_shape[0]  # height\n",
    "\n",
    "                # add flattened mask\n",
    "                for j, mask in enumerate(ann['masks']):\n",
    "                    masks_batch[i, j, 7:] = mask.flatten()\n",
    "\n",
    "        if self.save_to_dir:\n",
    "            for i, j in enumerate(index_array):\n",
    "                if self.data_format == 'channels_first':\n",
    "                    img_x = np.expand_dims(batch_x[i, 0, ...], 0)\n",
    "                else:\n",
    "                    img_x = np.expand_dims(batch_x[i, ..., 0], -1)\n",
    "                img = array_to_img(img_x, self.data_format, scale=True)\n",
    "                fname = '{prefix}_{index}_{hash}.{format}'.format(\n",
    "                    prefix=self.save_prefix,\n",
    "                    index=j,\n",
    "                    hash=np.random.randint(1e4),\n",
    "                    format=self.save_format)\n",
    "                img.save(os.path.join(self.save_to_dir, fname))\n",
    "\n",
    "        batch_inputs = batch_x\n",
    "        batch_outputs = [regressions, labels]\n",
    "        \n",
    "        if self.assoc_head:\n",
    "            # batch_inputs = [batch_x, batch_x_bbox]\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.include_masks:\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.include_final_detection_layer:\n",
    "            batch_outputs.append(masks_batch)\n",
    "\n",
    "        batch_outputs.extend(batch_y_semantic_list)\n",
    "\n",
    "        print(\"batch_inputs: \", batch_inputs)\n",
    "        print(\"batch_outputs: \", batch_outputs)\n",
    "\n",
    "        return batch_inputs, batch_outputs\n",
    "\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x. Returns the next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)\n",
    "\n",
    "\n",
    "class RetinaMovieIterator(Iterator):\n",
    "    \"\"\"Iterator yielding data from Numpy arrayss (`X and `y`).\n",
    "\n",
    "    Adapted from https://github.com/fizyr/keras-retinanet.\n",
    "\n",
    "    Args:\n",
    "        train_dict: dictionary consisting of numpy arrays for `X` and `y`.\n",
    "        image_data_generator: Instance of `ImageDataGenerator`\n",
    "            to use for random transformations and normalization.\n",
    "        compute_shapes: functor for generating shapes, based on the model.\n",
    "        min_objects: Integer, image with fewer than `min_objects` are ignored.\n",
    "        num_classes: Integer, number of classes for classification.\n",
    "        clear_borders: Boolean, whether to call `clear_border` on `y`.\n",
    "        include_masks: Boolean, whether to yield mask data.\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        data_format: String, one of `channels_first`, `channels_last`.\n",
    "        save_to_dir: Optional directory where to save the pictures\n",
    "            being yielded, in a viewable format. This is useful\n",
    "            for visualizing the random transformations being\n",
    "            applied, for debugging purposes.\n",
    "        save_prefix: String prefix to use for saving sample\n",
    "            images (if `save_to_dir` is set).\n",
    "        save_format: Format to use for saving sample images\n",
    "            (if `save_to_dir` is set).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_dict,\n",
    "                 movie_data_generator,\n",
    "                 compute_shapes=guess_shapes,\n",
    "                 anchor_params=None,\n",
    "                 pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "                 min_objects=3,\n",
    "                 num_classes=1,\n",
    "                 frames_per_batch=2,\n",
    "                 clear_borders=False,\n",
    "                 include_masks=False,\n",
    "                 include_final_detection_layer=False,\n",
    "                 assoc_head=False,\n",
    "                 panoptic=False,\n",
    "                 transforms=['watershed'],\n",
    "                 transforms_kwargs={},\n",
    "                 batch_size=32,\n",
    "                 shuffle=False,\n",
    "                 seed=None,\n",
    "                 data_format='channels_last',\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png'):\n",
    "        X, y = train_dict['X'], train_dict['y']\n",
    "\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError('Training batches and labels should have the same'\n",
    "                             'length. Found X.shape: {} y.shape: {}'.format(\n",
    "                                 X.shape, y.shape))\n",
    "\n",
    "        if X.ndim != 5:\n",
    "            raise ValueError('Input data in `RetinaNetIterator` '\n",
    "                             'should have rank 5. You passed an array '\n",
    "                             'with shape', X.shape)\n",
    "\n",
    "        self.x = np.asarray(X, dtype=K.floatx())\n",
    "        self.y = np.asarray(y, dtype='int32')\n",
    "\n",
    "        # `compute_shapes` changes based on the model backbone.\n",
    "        self.compute_shapes = compute_shapes\n",
    "        self.anchor_params = anchor_params\n",
    "        self.pyramid_levels = [int(l[1:]) for l in pyramid_levels]\n",
    "        self.min_objects = min_objects\n",
    "        self.num_classes = num_classes\n",
    "        self.frames_per_batch = frames_per_batch\n",
    "        self.include_masks = include_masks\n",
    "        self.include_final_detection_layer = include_final_detection_layer\n",
    "        self.assoc_head = assoc_head\n",
    "        self.panoptic = panoptic\n",
    "        self.transforms = transforms\n",
    "        self.transforms_kwargs = transforms_kwargs\n",
    "        self.channel_axis = 4 if data_format == 'channels_last' else 1\n",
    "        self.time_axis = 1 if data_format == 'channels_last' else 2\n",
    "        self.row_axis = 2 if data_format == 'channels_last' else 3\n",
    "        self.col_axis = 3 if data_format == 'channels_last' else 4\n",
    "        self.movie_data_generator = movie_data_generator\n",
    "        self.data_format = data_format\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "\n",
    "        self.y_semantic_list = []  # optional semantic segmentation targets\n",
    "\n",
    "        if X.shape[self.time_axis] - frames_per_batch < 0:\n",
    "            raise ValueError(\n",
    "                'The number of frames used in each training batch should '\n",
    "                'be less than the number of frames in the training data!')\n",
    "\n",
    "        # Add semantic segmentation targets if panoptic segmentation\n",
    "        # flag is True\n",
    "        print(\"train_dict keys :\")\n",
    "        for key in train_dict:\n",
    "            print(key)\n",
    "        if panoptic:\n",
    "            # Create a list of all the semantic targets. We need to be able\n",
    "            # to have multiple semantic heads\n",
    "            # Add all the keys that contain y_semantic\n",
    "            for key in train_dict:\n",
    "                if 'y_semantic' in key:\n",
    "                    self.y_semantic_list.append(train_dict[key])\n",
    "\n",
    "            # Add transformed masks\n",
    "            for transform in transforms:\n",
    "                transform_kwargs = transforms_kwargs.get(transform, dict())\n",
    "                y_transforms = []\n",
    "                for time in range(y.shape[self.time_axis]):\n",
    "                    if data_format == 'channels_first':\n",
    "                        y_temp = y[:, :, time, ...]\n",
    "                    else:\n",
    "                        y_temp = y[:, time, ...]\n",
    "                    y_temp_transform = _transform_masks(\n",
    "                        y_temp, transform,\n",
    "                        data_format=data_format,\n",
    "                        **transform_kwargs)\n",
    "                    y_temp_transform = np.asarray(y_temp_transform, dtype='int32')\n",
    "                    y_transforms.append(y_temp_transform)\n",
    "\n",
    "                y_transform = np.stack(y_transforms, axis=self.time_axis)\n",
    "                self.y_semantic_list.append(y_transform)\n",
    "\n",
    "        invalid_batches = []\n",
    "        # Remove images with small numbers of cells\n",
    "        for b in range(self.x.shape[0]):\n",
    "            y_batch = np.squeeze(self.y[b], axis=self.channel_axis - 1)\n",
    "            y_batch = clear_border(y_batch) if clear_borders else y_batch\n",
    "            y_batch = np.expand_dims(y_batch, axis=self.channel_axis - 1)\n",
    "\n",
    "            self.y[b] = y_batch\n",
    "\n",
    "            if len(np.unique(self.y[b])) - 1 < self.min_objects:\n",
    "                invalid_batches.append(b)\n",
    "\n",
    "        invalid_batches = np.array(invalid_batches, dtype='int')\n",
    "\n",
    "        if invalid_batches.size > 0:\n",
    "            logging.warning('Removing %s of %s images with fewer than %s '\n",
    "                            'objects.', invalid_batches.size, self.x.shape[0],\n",
    "                            self.min_objects)\n",
    "\n",
    "        self.y = np.delete(self.y, invalid_batches, axis=0)\n",
    "        self.x = np.delete(self.x, invalid_batches, axis=0)\n",
    "\n",
    "        self.y_semantic_list = [np.delete(y, invalid_batches, axis=0)\n",
    "                                for y in self.y_semantic_list]\n",
    "\n",
    "        super(RetinaMovieIterator, self).__init__(\n",
    "            self.x.shape[0], batch_size, shuffle, seed)\n",
    "\n",
    "    def filter_annotations(self, image, annotations):\n",
    "        \"\"\"Filter annotations by removing those that are outside of the\n",
    "        image bounds or whose width/height < 0.\n",
    "\n",
    "        Args:\n",
    "            image: ndarray, the raw image data.\n",
    "            annotations: dict of annotations including `labels` and `bboxes`\n",
    "        \"\"\"\n",
    "        row_axis = 1 if self.data_format == 'channels_first' else 0\n",
    "        invalid_indices = np.where(\n",
    "            (annotations['bboxes'][:, 2] <= annotations['bboxes'][:, 0]) |\n",
    "            (annotations['bboxes'][:, 3] <= annotations['bboxes'][:, 1]) |\n",
    "            (annotations['bboxes'][:, 0] < 0) |\n",
    "            (annotations['bboxes'][:, 1] < 0) |\n",
    "            (annotations['bboxes'][:, 2] > image.shape[row_axis + 1]) |\n",
    "            (annotations['bboxes'][:, 3] > image.shape[row_axis])\n",
    "        )[0]\n",
    "\n",
    "        # delete invalid indices\n",
    "        if invalid_indices.size > 0:\n",
    "            logging.warn('Image with shape {} contains the following invalid '\n",
    "                         'boxes: {}.'.format(\n",
    "                             image.shape,\n",
    "                             annotations['bboxes'][invalid_indices, :]))\n",
    "\n",
    "            for k in annotations.keys():\n",
    "                filtered = np.delete(annotations[k], invalid_indices, axis=0)\n",
    "                annotations[k] = filtered\n",
    "        return annotations\n",
    "\n",
    "    def load_annotations(self, y):\n",
    "        \"\"\"Generate bounding box and label annotations for a tensor\n",
    "\n",
    "        Args:\n",
    "            y: tensor to annotate\n",
    "\n",
    "        Returns:\n",
    "            annotations: dict of `bboxes` and `labels`\n",
    "        \"\"\"\n",
    "        labels, bboxes, masks, assoc_head = [], [], [], []\n",
    "        channel_axis = 1 if self.data_format == 'channels_first' else -1\n",
    "        \n",
    "        for prop in regionprops(np.squeeze(y.astype('int'))):\n",
    "            y1, x1, y2, x2 = prop.bbox\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            labels.append(0)  # boolean object detection\n",
    "            masks.append(np.where(y == prop.label, 1, 0))\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        bboxes = np.array(bboxes)\n",
    "        masks = np.array(masks).astype('uint8')\n",
    "        assoc_head = np.array(masks).astype('uint8')\n",
    "\n",
    "        # reshape bboxes in case it is empty.\n",
    "        bboxes = np.reshape(bboxes, (bboxes.shape[0], 4))\n",
    "\n",
    "        annotations = {'labels': labels, 'bboxes': bboxes}\n",
    "\n",
    "        if self.include_masks:\n",
    "            annotations['masks'] = masks\n",
    "\n",
    "        if self.assoc_head:\n",
    "#             y_transform = to_categorical(y.squeeze(channel_axis))\n",
    "#             if self.data_format == 'channels_first':\n",
    "#                 y_transform = np.rollaxis(y_transform, y.ndim - 1, 1)\n",
    "#             annotations['assoc_head'] = y_transform\n",
    "#             print(\"annotations['assoc_head'] shape: \", annotations['assoc_head'].shape)\n",
    "#             bboxes_transform = to_categorical(bboxes.squeeze(channel_axis))\n",
    "#             if self.data_format == 'channels_first':\n",
    "#                 bboxes_transform = np.rollaxis(bboxes_transform, masks.ndim - 1, 1)\n",
    "            annotations['assoc_head'] = bboxes\n",
    "            print(\"annotations['assoc_head'] shape: \", annotations['assoc_head'].shape)\n",
    "\n",
    "        annotations = self.filter_annotations(y, annotations)\n",
    "        return annotations\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        if self.data_format == 'channels_first':\n",
    "            batch_x = np.zeros((len(index_array),\n",
    "                                self.x.shape[1],\n",
    "                                self.frames_per_batch,\n",
    "                                self.x.shape[3],\n",
    "                                self.x.shape[4]))\n",
    "        else:\n",
    "            batch_x = np.zeros(tuple([len(index_array), self.frames_per_batch] +\n",
    "                                     list(self.x.shape)[2:]))\n",
    "\n",
    "        if self.panoptic:\n",
    "            if self.data_format == 'channels_first':\n",
    "                batch_y_semantic_list = [np.zeros(tuple([len(index_array),\n",
    "                                                         y_semantic.shape[1],\n",
    "                                                         self.frames_per_batch,\n",
    "                                                         y_semantic.shape[3],\n",
    "                                                         y_semantic.shape[4]]))\n",
    "                                         for y_semantic in self.y_semantic_list]\n",
    "            else:\n",
    "                batch_y_semantic_list = [\n",
    "                    np.zeros(tuple([len(index_array), self.frames_per_batch] +\n",
    "                                   list(y_semantic.shape[2:])))\n",
    "                    for y_semantic in self.y_semantic_list\n",
    "                ]\n",
    "\n",
    "        annotations_list = [[] for _ in range(self.frames_per_batch)]\n",
    "\n",
    "        max_shape = []\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            last_frame = self.x.shape[self.time_axis] - self.frames_per_batch\n",
    "            time_start = np.random.randint(0, high=last_frame)\n",
    "            time_end = time_start + self.frames_per_batch\n",
    "            times = list(np.arange(time_start, time_end))\n",
    "\n",
    "            if self.time_axis == 1:\n",
    "                x = self.x[j, time_start:time_end, ...]\n",
    "                y = self.y[j, time_start:time_end, ...]\n",
    "            elif self.time_axis == 2:\n",
    "                x = self.x[j, :, time_start:time_end, ...]\n",
    "                y = self.y[j, :, time_start:time_end, ...]\n",
    "\n",
    "            if self.panoptic:\n",
    "                if self.time_axis == 1:\n",
    "                    y_semantic_list = [y_semantic[j, time_start:time_end, ...]\n",
    "                                       for y_semantic in self.y_semantic_list]\n",
    "                elif self.time_axis == 2:\n",
    "                    y_semantic_list = [y_semantic[j, :, time_start:time_end, ...]\n",
    "                                       for y_semantic in self.y_semantic_list]\n",
    "\n",
    "            # Apply transformation\n",
    "            if self.panoptic:\n",
    "                x, y_list = self.movie_data_generator.random_transform(x, [y] + y_semantic_list)\n",
    "                y = y_list[0]\n",
    "                y_semantic_list = y_list[1:]\n",
    "            else:\n",
    "                x, y = self.movie_data_generator.random_transform(x, y)\n",
    "\n",
    "            x = self.movie_data_generator.standardize(x)\n",
    "\n",
    "            # Find max shape of image data.  Used for masking.\n",
    "            if not max_shape:\n",
    "                max_shape = list(x.shape)\n",
    "            else:\n",
    "                for k in range(len(x.shape)):\n",
    "                    if x.shape[k] > max_shape[k]:\n",
    "                        max_shape[k] = x.shape[k]\n",
    "\n",
    "            # Get the bounding boxes from the transformed masks!\n",
    "            for idx_time, time in enumerate(times):\n",
    "                if self.time_axis == 1:\n",
    "                    annotations = self.load_annotations(y[idx_time])\n",
    "                elif self.time_axis == 2:\n",
    "                    annotations = self.load_annotations(y[:, idx_time, ...])\n",
    "                annotations_list[idx_time].append(annotations)\n",
    "\n",
    "            batch_x[i] = x\n",
    "\n",
    "            if self.panoptic:\n",
    "                for k in range(len(y_semantic_list)):\n",
    "                    batch_y_semantic_list[k][i] = y_semantic_list[k]\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            batch_x_shape = [batch_x.shape[1], batch_x.shape[3], batch_x.shape[4]]\n",
    "        else:\n",
    "            batch_x_shape = batch_x.shape[2:]\n",
    "\n",
    "        anchors = anchors_for_shape(\n",
    "            batch_x_shape,\n",
    "            pyramid_levels=self.pyramid_levels,\n",
    "            anchor_params=self.anchor_params,\n",
    "            shapes_callback=self.compute_shapes)\n",
    "\n",
    "        regressions_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            batch_x_frame = batch_x[:, :, 0, ...]\n",
    "        else:\n",
    "            batch_x_frame = batch_x[:, 0, ...]\n",
    "        for idx, time in enumerate(times):\n",
    "            regressions, labels = anchor_targets_bbox(\n",
    "                anchors,\n",
    "                batch_x_frame,\n",
    "                annotations_list[idx],\n",
    "                self.num_classes)\n",
    "            regressions_list.append(regressions)\n",
    "            labels_list.append(labels)\n",
    "\n",
    "        regressions = np.stack(regressions_list, axis=self.time_axis)\n",
    "        # print(\"regressions.shape: \", regressions.shape)\n",
    "        labels = np.stack(labels_list, axis=self.time_axis)\n",
    "        # print(\"labels.shape: \", labels.shape)\n",
    "\n",
    "        # was a list for max shape indexing\n",
    "        max_shape = tuple([max_shape[self.row_axis - 1],\n",
    "                           max_shape[self.col_axis - 1]])\n",
    "\n",
    "\n",
    "        # print(\"annotations_list: \", annotations_list)\n",
    "\n",
    "        if self.assoc_head:\n",
    "            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "            annotations_list_flatten = flatten(annotations_list)\n",
    "#             print(\"len(annotations_list_flatten)\", len(annotations_list_flatten))\n",
    "            max_annotations = max(len(a['assoc_head']) for a in annotations_list_flatten)\n",
    "            assoc_heads_batch_shape = (len(index_array), self.frames_per_batch, \n",
    "                                max_annotations, 4) #2 * max_shape[0] * max_shape[1])\n",
    "            assoc_heads_batch = np.zeros(assoc_heads_batch_shape, dtype=K.floatx())\n",
    "#             print(\"assoc_heads_batch_shape: \", assoc_heads_batch_shape)\n",
    "            for idx_time, time in enumerate(times):\n",
    "                annotations_frame = annotations_list[idx_time]\n",
    "                for idx_batch, ann in enumerate(annotations_frame):\n",
    "                    # add flattened association head\n",
    "                    for idx_mask, assoc_head in enumerate(ann['assoc_head']):\n",
    "                        assoc_heads_batch[idx_batch, idx_time, idx_mask, :] = assoc_head.flatten()\n",
    "\n",
    "\n",
    "        if self.include_masks:\n",
    "            # masks_batch has shape: (batch size, max_annotations,\n",
    "            #     bbox_x1 + bbox_y1 + bbox_x2 + bbox_y2 + label +\n",
    "            #     width + height + max_image_dimension)\n",
    "\n",
    "            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "            annotations_list_flatten = flatten(annotations_list)\n",
    "            max_annotations = max(len(a['masks']) for a in annotations_list_flatten)\n",
    "            masks_batch_shape = (len(index_array), self.frames_per_batch, max_annotations,\n",
    "                                 5 + 2 + max_shape[0] * max_shape[1])\n",
    "            # print(\"masks_batch_shape: \", masks_batch_shape)\n",
    "            masks_batch = np.zeros(masks_batch_shape, dtype=K.floatx())\n",
    "\n",
    "            for idx_time, time in enumerate(times):\n",
    "                annotations_frame = annotations_list[idx_time]\n",
    "                for idx_batch, ann in enumerate(annotations_frame):\n",
    "                    masks_batch[idx_batch, idx_time, :ann['bboxes'].shape[0], :4] = ann['bboxes']\n",
    "                    masks_batch[idx_batch, idx_time, :ann['labels'].shape[0], 4] = ann['labels']\n",
    "                    masks_batch[idx_batch, idx_time, :, 5] = max_shape[1]  # width\n",
    "                    masks_batch[idx_batch, idx_time, :, 6] = max_shape[0]  # height\n",
    "\n",
    "                    # add flattened mask\n",
    "                    for idx_mask, mask in enumerate(ann['masks']):\n",
    "                        masks_batch[idx_batch, idx_time, idx_mask, 7:] = mask.flatten()\n",
    "\n",
    "        if self.save_to_dir:\n",
    "            for i, j in enumerate(index_array):\n",
    "                for frame in range(batch_x.shape[self.time_axis]):\n",
    "                    if self.time_axis == 2:\n",
    "                        img = array_to_img(batch_x[i, :, frame], self.data_format, scale=True)\n",
    "                    else:\n",
    "                        img = array_to_img(batch_x[i, frame], self.data_format, scale=True)\n",
    "                    fname = '{prefix}_{index}_{hash}.{format}'.format(\n",
    "                        prefix=self.save_prefix,\n",
    "                        index=j,\n",
    "                        hash=np.random.randint(1e4),\n",
    "                        format=self.save_format)\n",
    "                    img.save(os.path.join(self.save_to_dir, fname))\n",
    "\n",
    "        batch_inputs = batch_x\n",
    "        batch_outputs = [regressions, labels]\n",
    "        \n",
    "        if self.assoc_head:\n",
    "            batch_inputs = [batch_x, assoc_heads_batch]\n",
    "            batch_outputs.append(assoc_heads_batch)\n",
    "        if self.include_masks:\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.include_final_detection_layer:\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.panoptic:\n",
    "            batch_outputs += batch_y_semantic_list\n",
    "        return batch_inputs, batch_outputs\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x. Returns the next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)\n",
    "\n",
    "\n",
    "class RetinaMovieDataGenerator(MovieDataGenerator):\n",
    "    \"\"\"Generates batches of tensor image data with real-time data augmentation.\n",
    "    The data will be looped over (in batches).\n",
    "\n",
    "    Args:\n",
    "        featurewise_center: boolean, set input mean to 0 over the dataset,\n",
    "            feature-wise.\n",
    "        samplewise_center: boolean, set each sample mean to 0.\n",
    "        featurewise_std_normalization: boolean, divide inputs by std\n",
    "            of the dataset, feature-wise.\n",
    "        samplewise_std_normalization: boolean, divide each input by its std.\n",
    "        zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
    "        zca_whitening: boolean, apply ZCA whitening.\n",
    "        rotation_range: int, degree range for random rotations.\n",
    "        width_shift_range: float, 1-D array-like or int\n",
    "            float: fraction of total width, if < 1, or pixels if >= 1.\n",
    "            1-D array-like: random elements from the array.\n",
    "            int: integer number of pixels from interval\n",
    "                `(-width_shift_range, +width_shift_range)`\n",
    "            With `width_shift_range=2` possible values are ints [-1, 0, +1],\n",
    "            same as with `width_shift_range=[-1, 0, +1]`,\n",
    "            while with `width_shift_range=1.0` possible values are floats in\n",
    "            the interval [-1.0, +1.0).\n",
    "        shear_range: float, shear Intensity\n",
    "            (Shear angle in counter-clockwise direction in degrees)\n",
    "        zoom_range: float or [lower, upper], Range for random zoom.\n",
    "            If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
    "        channel_shift_range: float, range for random channel shifts.\n",
    "        fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n",
    "            Default is 'nearest'. Points outside the boundaries of the input\n",
    "            are filled according to the given mode:\n",
    "                'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
    "                'nearest':  aaaaaaaa|abcd|dddddddd\n",
    "                'reflect':  abcddcba|abcd|dcbaabcd\n",
    "                'wrap':  abcdabcd|abcd|abcdabcd\n",
    "        cval: float or int, value used for points outside the boundaries\n",
    "            when `fill_mode = \"constant\"`.\n",
    "        horizontal_flip: boolean, randomly flip inputs horizontally.\n",
    "        vertical_flip: boolean, randomly flip inputs vertically.\n",
    "        rescale: rescaling factor. Defaults to None. If None or 0, no rescaling\n",
    "            is applied, otherwise we multiply the data by the value provided\n",
    "            (before applying any other transformation).\n",
    "        preprocessing_function: function that will be implied on each input.\n",
    "            The function will run after the image is resized and augmented.\n",
    "            The function should take one argument:\n",
    "            one image (Numpy tensor with rank 3),\n",
    "            and should output a Numpy tensor with the same shape.\n",
    "        data_format: One of {\"channels_first\", \"channels_last\"}.\n",
    "            \"channels_last\" mode means that the images should have shape\n",
    "                `(samples, height, width, channels)`,\n",
    "            \"channels_first\" mode means that the images should have shape\n",
    "                `(samples, channels, height, width)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "                Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "        validation_split: float, fraction of images reserved for validation\n",
    "            (strictly between 0 and 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def flow(self,\n",
    "             train_dict,\n",
    "             batch_size=1,\n",
    "             frames_per_batch=5,\n",
    "             compute_shapes=guess_shapes,\n",
    "             num_classes=1,\n",
    "             clear_borders=False,\n",
    "             include_masks=False,\n",
    "             include_final_detection_layer=False,\n",
    "             panoptic=False,\n",
    "             assoc_head=False,\n",
    "             transforms=['watershed'],\n",
    "             transforms_kwargs={},\n",
    "             anchor_params=None,\n",
    "             pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "             shuffle=False,\n",
    "             seed=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png'):\n",
    "        \"\"\"Generates batches of augmented/normalized data with given arrays.\n",
    "\n",
    "        Args:\n",
    "            train_dict: dictionary of X and y tensors. Both should be rank 5.\n",
    "            frames_per_batch: int (default: 10).\n",
    "                size of z axis in generated batches\n",
    "            batch_size: int (default: 1).\n",
    "            shuffle: boolean (default: True).\n",
    "            seed: int (default: None).\n",
    "            save_to_dir: None or str (default: None).\n",
    "                This allows you to optionally specify a directory\n",
    "                to which to save the augmented pictures being generated\n",
    "                (useful for visualizing what you are doing).\n",
    "            save_prefix: str (default: `''`). Prefix to use for filenames of\n",
    "                saved pictures (only relevant if `save_to_dir` is set).\n",
    "            save_format: one of \"png\", \"jpeg\". Default: \"png\".\n",
    "                (only relevant if `save_to_dir` is set)\n",
    "\n",
    "        Returns:\n",
    "            An Iterator yielding tuples of `(x, y)` where `x` is a numpy array\n",
    "            of image data and `y` is a numpy array of labels of the same shape.\n",
    "        \"\"\"\n",
    "        return RetinaMovieIterator(\n",
    "            train_dict,\n",
    "            self,\n",
    "            compute_shapes=compute_shapes,\n",
    "            num_classes=num_classes,\n",
    "            clear_borders=clear_borders,\n",
    "            include_masks=include_masks,\n",
    "            include_final_detection_layer=include_final_detection_layer,\n",
    "            assoc_head=assoc_head,\n",
    "            panoptic=panoptic,\n",
    "            transforms=transforms,\n",
    "            transforms_kwargs=transforms_kwargs,\n",
    "            anchor_params=anchor_params,\n",
    "            pyramid_levels=pyramid_levels,\n",
    "            batch_size=batch_size,\n",
    "            frames_per_batch=frames_per_batch,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            data_format=self.data_format,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from deepcell.image_generators import RetinaMovieDataGenerator\n",
    "\n",
    "datagen = RetinaMovieDataGenerator(\n",
    "    rotation_range=180,\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "datagen_val = RetinaMovieDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dict keys :\n",
      "X\n",
      "y\n",
      "daughters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0209 16:40:38.825136 140181711468352 <ipython-input-110-6f555eae6e8d>:644] Removing 2 of 192 images with fewer than 3 objects.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dict keys :\n",
      "X\n",
      "y\n",
      "daughters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0209 16:40:40.209009 140181711468352 <ipython-input-110-6f555eae6e8d>:644] Removing 1 of 48 images with fewer than 3 objects.\n"
     ]
    }
   ],
   "source": [
    "train_data = datagen.flow(\n",
    "    train_dict,\n",
    "    batch_size=1,\n",
    "    include_masks=True,\n",
    "    include_final_detection_layer=True,\n",
    "    assoc_head=True,\n",
    "    frames_per_batch=fpb,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params)\n",
    "\n",
    "val_data = datagen_val.flow(\n",
    "    test_dict,\n",
    "    batch_size=1,\n",
    "    include_masks=True,\n",
    "    include_final_detection_layer=True,\n",
    "    assoc_head=True,\n",
    "    frames_per_batch=fpb,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations['assoc_head'] shape:  (11, 4)\n",
      "annotations['assoc_head'] shape:  (9, 4)\n",
      "annotations['assoc_head'] shape:  (8, 4)\n",
      "Epoch 1/10\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n",
      "annotations['assoc_head'] shape:  (2, 4)\n",
      "annotations['assoc_head'] shape:  (2, 4)\n",
      "annotations['assoc_head'] shape:  (2, 4)\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n",
      "annotations['assoc_head'] shape:  (5, 4)\n",
      "annotations['assoc_head'] shape:  (6, 4)\n",
      "annotations['assoc_head'] shape:  (7, 4)\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n",
      "annotations['assoc_head'] shape:  (8, 4)\n",
      "annotations['assoc_head'] shape:  (8, 4)\n",
      "annotations['assoc_head'] shape:  (8, 4)\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n",
      "annotations['assoc_head'] shape:  (5, 4)\n",
      "annotations['assoc_head'] shape:  (4, 4)\n",
      "annotations['assoc_head'] shape:  (6, 4)\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n",
      "annotations['assoc_head'] shape:  (5, 4)\n",
      "annotations['assoc_head'] shape:  (5, 4)\n",
      "annotations['assoc_head'] shape:  (3, 4)\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n",
      "annotations['assoc_head'] shape:  (8, 4)\n",
      "annotations['assoc_head'] shape:  (8, 4)\n",
      "annotations['assoc_head'] shape:  (7, 4)\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n",
      "annotations['assoc_head'] shape:  (11, 4)\n",
      "annotations['assoc_head'] shape:  (11, 4)\n",
      "annotations['assoc_head'] shape:  (11, 4)\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n",
      "annotations['assoc_head'] shape:  (7, 4)\n",
      "annotations['assoc_head'] shape:  (8, 4)\n",
      "annotations['assoc_head'] shape:  (7, 4)\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n",
      "annotations['assoc_head'] shape:  (12, 4)\n",
      "annotations['assoc_head'] shape:  (12, 4)\n",
      "annotations['assoc_head'] shape:  (12, 4)\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n",
      "annotations['assoc_head'] shape:  (7, 4)\n",
      "annotations['assoc_head'] shape:  (9, 4)\n",
      "annotations['assoc_head'] shape:  (9, 4)\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n",
      "annotations['assoc_head'] shape:  (7, 4)\n",
      "annotations['assoc_head'] shape:  (8, 4)\n",
      "annotations['assoc_head'] shape:  (7, 4)\n",
      "regressions.shape:  (1, 3, 83349, 5)\n",
      "labels.shape:  (1, 3, 83349, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument: slice index 5 of dimension 2 out of bounds.\n\t [[{{node loss/masks_loss/cond/strided_slice_11}}]]\n\t [[loss/Identity_2/_1821]]\n  (1) Invalid argument: slice index 5 of dimension 2 out of bounds.\n\t [[{{node loss/masks_loss/cond/strided_slice_11}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-b7b73d750888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                      \u001b[0mframes_per_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                      weighted_average=True),\n\u001b[0;32m---> 29\u001b[0;31m             prediction_model)]\n\u001b[0m\u001b[1;32m     30\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: slice index 5 of dimension 2 out of bounds.\n\t [[{{node loss/masks_loss/cond/strided_slice_11}}]]\n\t [[loss/Identity_2/_1821]]\n  (1) Invalid argument: slice index 5 of dimension 2 out of bounds.\n\t [[{{node loss/masks_loss/cond/strided_slice_11}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "from deepcell.callbacks import RedirectModel, Evaluate\n",
    "\n",
    "iou_threshold = 0.5\n",
    "score_threshold = 0.01\n",
    "max_detections = 100\n",
    "\n",
    "model.fit_generator(\n",
    "    train_data,\n",
    "    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "    epochs=n_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=X_test.shape[0] // batch_size,\n",
    "    callbacks=[\n",
    "        callbacks.LearningRateScheduler(lr_sched),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            os.path.join(MODEL_DIR, model_name + '.h5'),\n",
    "            monitor='val_loss',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False),\n",
    "        RedirectModel(\n",
    "            Evaluate(val_data,\n",
    "                     iou_threshold=iou_threshold,\n",
    "                     score_threshold=score_threshold,\n",
    "                     max_detections=max_detections,\n",
    "                     frames_per_batch=fpb,\n",
    "                     weighted_average=True),\n",
    "            prediction_model)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in batch_outputs:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 7.36526184e+01,  5.15165043e+00,  7.74914551e+01,\n",
       "           3.63908730e+01,  0.00000000e+00],\n",
       "         [ 5.89738693e+01,  4.60461617e+00,  6.09892578e+01,\n",
       "           2.83677044e+01,  0.00000000e+00],\n",
       "         [ 4.73233414e+01,  4.17043495e+00,  4.78914604e+01,\n",
       "           2.19997158e+01,  0.00000000e+00],\n",
       "         ...,\n",
       "         [-1.47747576e+00, -1.42937860e+01, -2.05805826e+00,\n",
       "          -1.47638836e+01, -1.00000000e+00],\n",
       "         [-6.56924367e-01, -1.08292360e+01, -2.14923072e+00,\n",
       "          -1.22338505e+01, -1.00000000e+00],\n",
       "         [-5.65267634e-03, -8.07942200e+00, -2.22159410e+00,\n",
       "          -1.02257624e+01, -1.00000000e+00]],\n",
       "\n",
       "        [[ 2.05805826e+00,  1.61611652e+00,  2.80330086e+00,\n",
       "           1.87132034e+01,  0.00000000e+00],\n",
       "         [ 2.14923072e+00,  1.79846120e+00,  1.70923245e+00,\n",
       "           1.43369303e+01,  0.00000000e+00],\n",
       "         [ 2.22159410e+00,  1.94318831e+00,  8.40870261e-01,\n",
       "           1.08634806e+01,  0.00000000e+00],\n",
       "         ...,\n",
       "         [-1.09792233e+01, -4.11650389e-02, -7.36135912e+00,\n",
       "          -2.72097087e+00, -1.00000000e+00],\n",
       "         [-8.19846630e+00,  4.83076125e-01, -6.35846329e+00,\n",
       "          -2.67538476e+00, -1.00000000e+00],\n",
       "         [-5.99137831e+00,  8.99166346e-01, -5.56246424e+00,\n",
       "          -2.63920283e+00, -1.00000000e+00]],\n",
       "\n",
       "        [[ 2.05805826e+00,  8.11656342e+01,  1.03163109e+01,\n",
       "           1.24779221e+02,  0.00000000e+00],\n",
       "         [ 2.14923072e+00,  6.49369507e+01,  7.67231178e+00,\n",
       "           9.85215759e+01,  0.00000000e+00],\n",
       "         [ 2.22159410e+00,  5.20562401e+01,  5.57376957e+00,\n",
       "           7.76808853e+01,  0.00000000e+00],\n",
       "         ...,\n",
       "         [-3.72747574e+01, -4.79203892e+00, -3.56456299e+01,\n",
       "          -3.71533990e+00, -1.00000000e+00],\n",
       "         [-2.90692444e+01, -3.28769469e+00, -2.88077030e+01,\n",
       "          -3.46461582e+00, -1.00000000e+00],\n",
       "         [-2.25565262e+01, -2.09369659e+00, -2.33804398e+01,\n",
       "          -3.26561618e+00, -1.00000000e+00]]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
