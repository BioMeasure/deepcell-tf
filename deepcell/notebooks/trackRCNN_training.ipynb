{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "https://github.com/vanvalenlab/deepcell-tf/blob/master/scripts/feature_pyramids/RetinaNet%20-%20Movie.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import errno\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -\n",
      "X.shape: (192, 30, 154, 182, 1)\n",
      "y.shape: (192, 30, 154, 182, 1)\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.data_utils import get_data\n",
    "from deepcell.utils.tracking_utils import load_trks\n",
    "\n",
    "DATA_DIR = '/data/training_data/cells/3T3/NIH/movie'\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'nuclear_movie_3T3_0-2_same.trks')\n",
    "\n",
    "# Load Information for hardcoded image size training\n",
    "seed = 1\n",
    "test_size = .2\n",
    "train_dict, test_dict = get_data(DATA_FILE, mode='siamese_daughters', seed=seed, test_size=test_size)\n",
    "X_train, y_train = train_dict['X'], train_dict['y']\n",
    "X_test, y_test = test_dict['X'], test_dict['y']\n",
    "\n",
    "print(' -\\nX.shape: {}\\ny.shape: {}'.format(train_dict['X'].shape, train_dict['y'].shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Download four different sets of data (saves to ~/.keras/datasets)\n",
    "filename_3T3 = '3T3_NIH.trks'\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.tracked.nih_3t3.load_tracked_data(filename_3T3)\n",
    "print('3T3 -\\nX.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Contants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up other required filepaths\n",
    "PREFIX = os.path.relpath(os.path.dirname(DATA_FILE), DATA_DIR)\n",
    "ROOT_DIR = '/data' # mounted volume\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models', PREFIX))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs', PREFIX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each head of the model uses its own loss\n",
    "from deepcell.losses import RetinaNetLosses\n",
    "\n",
    "sigma = 3.0\n",
    "alpha = 0.25\n",
    "gamma = 2.0\n",
    "iou_threshold = 0.5\n",
    "max_detections = 100\n",
    "mask_size = (28, 28)\n",
    "\n",
    "retinanet_losses = RetinaNetLosses(\n",
    "    sigma=sigma, alpha=alpha, gamma=gamma,\n",
    "    iou_threshold=iou_threshold,\n",
    "    mask_size=mask_size)\n",
    "\n",
    "loss = {\n",
    "    'regression': retinanet_losses.regress_loss,\n",
    "    'classification': retinanet_losses.classification_loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RetinaMask Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "model_name = 'trackrcnn_model'\n",
    "backbone = 'resnet50'  # vgg16, vgg19, resnet50, densenet121, densenet169, densenet201\n",
    "\n",
    "n_epoch = 10  # Number of training epochs\n",
    "lr = 1e-5\n",
    "\n",
    "optimizer = Adam(lr=lr, clipnorm=0.001)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=lr, decay=0.99)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "num_classes = 1  # \"object\" is the only class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.retinanet_anchor_utils import get_anchor_parameters\n",
    "\n",
    "flat_shape = [y_train.shape[0] * y_train.shape[1]] + list(y_train.shape[2:])\n",
    "flat_y = np.reshape(y_train, tuple(flat_shape)).astype('int')\n",
    "\n",
    "# Generate backbone information from the data\n",
    "backbone_levels, pyramid_levels, anchor_params = get_anchor_parameters(flat_y)\n",
    "\n",
    "fpb = 5  # number of frames in each training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"retinanet-bbox\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_input (InputLayer)        [(None, 5, 154, 182, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_30 (TimeDistri (None, 5, 154, 182,  0           image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_31 (TimeDistri (None, 5, 154, 182,  6           time_distributed_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_33 (TimeDistri (None, 5, 39, 46, 25 229760      time_distributed_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_34 (TimeDistri (None, 5, 20, 23, 51 1460096     time_distributed_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_32 (TimeDistri (None, 5, 77, 91, 64 9728        time_distributed_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "C2_reduced (Conv3D)             (None, 5, 39, 46, 25 65792       time_distributed_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "C3_reduced (Conv3D)             (None, 5, 20, 23, 25 131328      time_distributed_34[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "C1_reduced (Conv3D)             (None, 5, 77, 91, 25 16640       time_distributed_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "P2_upsampled (UpsampleLike)     (None, None, None, N 0           C2_reduced[0][0]                 \n",
      "                                                                 time_distributed_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "P3_upsampled (UpsampleLike)     (None, None, None, N 0           C3_reduced[0][0]                 \n",
      "                                                                 time_distributed_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "P1_merged (Add)                 (None, 5, 77, 91, 25 0           C1_reduced[0][0]                 \n",
      "                                                                 P2_upsampled[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "P2_merged (Add)                 (None, 5, 39, 46, 25 0           C2_reduced[0][0]                 \n",
      "                                                                 P3_upsampled[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "P1 (Conv3D)                     (None, 5, 77, 91, 25 590080      P1_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P2 (Conv3D)                     (None, 5, 39, 46, 25 590080      P2_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P3 (Conv3D)                     (None, 5, 20, 23, 25 590080      C3_reduced[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_37 (TimeDistri (None, 5, 63063, 4)  36          P1[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_38 (TimeDistri (None, 5, 16146, 4)  36          P2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_39 (TimeDistri (None, 5, 4140, 4)   36          P3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "regression_submodel (Model)     (None, 5, None, 4)   7327780     P1[0][0]                         \n",
      "                                                                 P2[0][0]                         \n",
      "                                                                 P3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "anchors (Concatenate)           (None, 5, 83349, 4)  0           time_distributed_37[0][0]        \n",
      "                                                                 time_distributed_38[0][0]        \n",
      "                                                                 time_distributed_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "regression (Concatenate)        (None, 5, None, 4)   0           regression_submodel[1][0]        \n",
      "                                                                 regression_submodel[2][0]        \n",
      "                                                                 regression_submodel[3][0]        \n",
      "__________________________________________________________________________________________________\n",
      "boxes (RegressBoxes)            (None, 5, 83349, 4)  0           anchors[0][0]                    \n",
      "                                                                 regression[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "classification_submodel (Model) (None, 5, None, 1)   7141129     P1[0][0]                         \n",
      "                                                                 P2[0][0]                         \n",
      "                                                                 P3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "clipped_boxes (ClipBoxes)       (None, 5, 83349, 4)  0           image_input[0][0]                \n",
      "                                                                 boxes[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "classification (Concatenate)    (None, 5, None, 1)   0           classification_submodel[1][0]    \n",
      "                                                                 classification_submodel[2][0]    \n",
      "                                                                 classification_submodel[3][0]    \n",
      "__________________________________________________________________________________________________\n",
      "filtered_detections (FilterDete [(None, None, 100, 4 0           clipped_boxes[0][0]              \n",
      "                                                                 classification[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 17,913,119\n",
      "Trainable params: 17,903,007\n",
      "Non-trainable params: 10,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "# Pass frames_per_batch > 1 to enable 3D mode!\n",
    "model = model_zoo.RetinaNet(\n",
    "    backbone=backbone,\n",
    "    use_imagenet=True,\n",
    "    panoptic=False,\n",
    "    frames_per_batch=fpb,\n",
    "    num_classes=num_classes,\n",
    "    input_shape=X_train.shape[2:],\n",
    "    backbone_levels=backbone_levels,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    num_anchors=anchor_params.num_anchors())\n",
    "\n",
    "prediction_model = model_zoo.retinanet_bbox(\n",
    "    model,\n",
    "    panoptic=False,\n",
    "    frames_per_batch=fpb,\n",
    "    max_detections=100,\n",
    "    anchor_params=anchor_params)\n",
    "\n",
    "prediction_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RetinaMask Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deepcell.image_generators import RetinaMovieDataGenerator\n",
    "\n",
    "datagen = RetinaMovieDataGenerator(\n",
    "    rotation_range=180,\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "datagen_val = RetinaMovieDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0120 06:00:29.835370 140716510050112 retinanet.py:633] Removing 2 of 192 images with fewer than 3 objects.\n",
      "W0120 06:00:31.296959 140716510050112 retinanet.py:633] Removing 1 of 48 images with fewer than 3 objects.\n"
     ]
    }
   ],
   "source": [
    "train_data = datagen.flow(\n",
    "    {'X': X_train, 'y': y_train},\n",
    "    batch_size=1,\n",
    "    include_masks=False,\n",
    "    include_final_detection_layer=False,\n",
    "    frames_per_batch=fpb,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params)\n",
    "\n",
    "val_data = datagen_val.flow(\n",
    "    {'X': X_test, 'y': y_test},\n",
    "    batch_size=1,\n",
    "    include_masks=False,\n",
    "    include_final_detection_layer=False,\n",
    "    frames_per_batch=fpb,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/192 [============================>.] - ETA: 0s - loss: 3.0883 - regression_loss: 2.5309 - classification_loss: 0.5574\n",
      "Epoch 00001: val_loss improved from inf to 2.53996, saving model to /data/models/retinamovie_model.h5\n",
      "525 instances of class 0 with average precision: 0.5584\n",
      "mAP: 0.5584\n",
      "192/192 [==============================] - 201s 1s/step - loss: 3.0847 - regression_loss: 2.5285 - classification_loss: 0.5562 - val_loss: 2.5400 - val_regression_loss: 2.1741 - val_classification_loss: 0.3658\n",
      "Epoch 2/10\n",
      "191/192 [============================>.] - ETA: 0s - loss: 2.2182 - regression_loss: 1.9423 - classification_loss: 0.2759\n",
      "Epoch 00002: val_loss improved from 2.53996 to 1.98636, saving model to /data/models/retinamovie_model.h5\n",
      "525 instances of class 0 with average precision: 0.8220\n",
      "mAP: 0.8220\n",
      "192/192 [==============================] - 147s 764ms/step - loss: 2.2169 - regression_loss: 1.9414 - classification_loss: 0.2755 - val_loss: 1.9864 - val_regression_loss: 1.7613 - val_classification_loss: 0.2250\n",
      "Epoch 3/10\n",
      "191/192 [============================>.] - ETA: 0s - loss: 1.8794 - regression_loss: 1.6681 - classification_loss: 0.2113\n",
      "Epoch 00003: val_loss improved from 1.98636 to 1.80670, saving model to /data/models/retinamovie_model.h5\n",
      "525 instances of class 0 with average precision: 0.8831\n",
      "mAP: 0.8831\n",
      "192/192 [==============================] - 149s 777ms/step - loss: 1.8779 - regression_loss: 1.6668 - classification_loss: 0.2111 - val_loss: 1.8067 - val_regression_loss: 1.6120 - val_classification_loss: 0.1947\n",
      "Epoch 4/10\n",
      "191/192 [============================>.] - ETA: 0s - loss: 1.7043 - regression_loss: 1.5212 - classification_loss: 0.1831\n",
      "Epoch 00004: val_loss improved from 1.80670 to 1.68652, saving model to /data/models/retinamovie_model.h5\n",
      "525 instances of class 0 with average precision: 0.9055\n",
      "mAP: 0.9055\n",
      "192/192 [==============================] - 148s 770ms/step - loss: 1.7031 - regression_loss: 1.5203 - classification_loss: 0.1828 - val_loss: 1.6865 - val_regression_loss: 1.5008 - val_classification_loss: 0.1857\n",
      "Epoch 5/10\n",
      "191/192 [============================>.] - ETA: 0s - loss: 1.5892 - regression_loss: 1.4115 - classification_loss: 0.1777\n",
      "Epoch 00005: val_loss improved from 1.68652 to 1.55781, saving model to /data/models/retinamovie_model.h5\n",
      "525 instances of class 0 with average precision: 0.9413\n",
      "mAP: 0.9413\n",
      "192/192 [==============================] - 146s 760ms/step - loss: 1.5878 - regression_loss: 1.4102 - classification_loss: 0.1776 - val_loss: 1.5578 - val_regression_loss: 1.3730 - val_classification_loss: 0.1848\n",
      "Epoch 6/10\n",
      "191/192 [============================>.] - ETA: 0s - loss: 1.4658 - regression_loss: 1.3035 - classification_loss: 0.1622\n",
      "Epoch 00006: val_loss improved from 1.55781 to 1.46852, saving model to /data/models/retinamovie_model.h5\n",
      "525 instances of class 0 with average precision: 0.9517\n",
      "mAP: 0.9517\n",
      "192/192 [==============================] - 150s 780ms/step - loss: 1.4633 - regression_loss: 1.3015 - classification_loss: 0.1618 - val_loss: 1.4685 - val_regression_loss: 1.3130 - val_classification_loss: 0.1556\n",
      "Epoch 7/10\n",
      "191/192 [============================>.] - ETA: 0s - loss: 1.4138 - regression_loss: 1.2560 - classification_loss: 0.1579\n",
      "Epoch 00007: val_loss improved from 1.46852 to 1.42497, saving model to /data/models/retinamovie_model.h5\n",
      "525 instances of class 0 with average precision: 0.9609\n",
      "mAP: 0.9609\n",
      "192/192 [==============================] - 146s 763ms/step - loss: 1.4131 - regression_loss: 1.2554 - classification_loss: 0.1576 - val_loss: 1.4250 - val_regression_loss: 1.2736 - val_classification_loss: 0.1514\n",
      "Epoch 8/10\n",
      "191/192 [============================>.] - ETA: 0s - loss: 1.3372 - regression_loss: 1.1884 - classification_loss: 0.1487\n",
      "Epoch 00008: val_loss improved from 1.42497 to 1.37648, saving model to /data/models/retinamovie_model.h5\n",
      "525 instances of class 0 with average precision: 0.9580\n",
      "mAP: 0.9580\n",
      "192/192 [==============================] - 148s 768ms/step - loss: 1.3357 - regression_loss: 1.1873 - classification_loss: 0.1485 - val_loss: 1.3765 - val_regression_loss: 1.2345 - val_classification_loss: 0.1420\n",
      "Epoch 9/10\n",
      "191/192 [============================>.] - ETA: 0s - loss: 1.2896 - regression_loss: 1.1425 - classification_loss: 0.1471\n",
      "Epoch 00009: val_loss improved from 1.37648 to 1.33323, saving model to /data/models/retinamovie_model.h5\n",
      "525 instances of class 0 with average precision: 0.9618\n",
      "mAP: 0.9618\n",
      "192/192 [==============================] - 146s 759ms/step - loss: 1.2894 - regression_loss: 1.1424 - classification_loss: 0.1470 - val_loss: 1.3332 - val_regression_loss: 1.1953 - val_classification_loss: 0.1379\n",
      "Epoch 10/10\n",
      "191/192 [============================>.] - ETA: 0s - loss: 1.2696 - regression_loss: 1.1234 - classification_loss: 0.1462\n",
      "Epoch 00010: val_loss improved from 1.33323 to 1.28087, saving model to /data/models/retinamovie_model.h5\n",
      "525 instances of class 0 with average precision: 0.9676\n",
      "mAP: 0.9676\n",
      "192/192 [==============================] - 149s 776ms/step - loss: 1.2702 - regression_loss: 1.1241 - classification_loss: 0.1462 - val_loss: 1.2809 - val_regression_loss: 1.1587 - val_classification_loss: 0.1222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff2a1f580b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "from deepcell.callbacks import RedirectModel, Evaluate\n",
    "\n",
    "iou_threshold = 0.5\n",
    "score_threshold = 0.01\n",
    "max_detections = 100\n",
    "\n",
    "model.fit_generator(\n",
    "    train_data,\n",
    "    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "    epochs=n_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=X_test.shape[0] // batch_size,\n",
    "    callbacks=[\n",
    "        callbacks.LearningRateScheduler(lr_sched),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            os.path.join(MODEL_DIR, model_name + '.h5'),\n",
    "            monitor='val_loss',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False),\n",
    "        RedirectModel(\n",
    "            Evaluate(val_data,\n",
    "                     iou_threshold=iou_threshold,\n",
    "                     score_threshold=score_threshold,\n",
    "                     max_detections=max_detections,\n",
    "                     frames_per_batch=fpb,\n",
    "                     weighted_average=True),\n",
    "            prediction_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
