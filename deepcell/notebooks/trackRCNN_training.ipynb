{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "https://github.com/vanvalenlab/deepcell-tf/blob/master/scripts/feature_pyramids/RetinaNet%20-%20Movie.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import errno\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -\n",
      "X.shape: (192, 30, 154, 182, 1)\n",
      "y.shape: (192, 30, 154, 182, 1)\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.data_utils import get_data\n",
    "from deepcell.utils.tracking_utils import load_trks\n",
    "\n",
    "DATA_DIR = '/data/training_data/cells/3T3/NIH/movie'\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'nuclear_movie_3T3_0-2_same.trks')\n",
    "\n",
    "# Load Information for hardcoded image size training\n",
    "seed = 1\n",
    "test_size = .2\n",
    "train_dict, test_dict = get_data(DATA_FILE, mode='siamese_daughters', seed=seed, test_size=test_size)\n",
    "X_train, y_train = train_dict['X'], train_dict['y']\n",
    "X_test, y_test = test_dict['X'], test_dict['y']\n",
    "\n",
    "print(' -\\nX.shape: {}\\ny.shape: {}'.format(train_dict['X'].shape, train_dict['y'].shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Download four different sets of data (saves to ~/.keras/datasets)\n",
    "filename_3T3 = '3T3_NIH.trks'\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.tracked.nih_3t3.load_tracked_data(filename_3T3)\n",
    "print('3T3 -\\nX.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Contants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up other required filepaths\n",
    "PREFIX = os.path.relpath(os.path.dirname(DATA_FILE), DATA_DIR)\n",
    "ROOT_DIR = '/data' # mounted volume\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models', PREFIX))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs', PREFIX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each head of the model uses its own loss\n",
    "from deepcell.losses import RetinaNetLosses\n",
    "#from deepcell.losses import discriminative_instance_loss\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "sigma = 3.0\n",
    "alpha = 0.25\n",
    "gamma = 2.0\n",
    "iou_threshold = 0.5\n",
    "max_detections = 100\n",
    "mask_size = (28, 28)\n",
    "\n",
    "retinanet_losses = RetinaNetLosses(\n",
    "    sigma=sigma, alpha=alpha, gamma=gamma,\n",
    "    iou_threshold=iou_threshold,\n",
    "    mask_size=mask_size)\n",
    "\n",
    "loss = {\n",
    "    'regression': retinanet_losses.regress_loss,\n",
    "    'classification': retinanet_losses.classification_loss,\n",
    "    #'association_features': discriminative_instance_loss, #losses.kullback_leibler_divergence,\n",
    "    'association_features_cat': discriminative_instance_loss, #losses.kullback_leibler_divergence,\n",
    "    'masks': retinanet_losses.mask_loss,\n",
    "    'final_detection': retinanet_losses.final_detection_loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RetinaMask Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "model_name = 'trackrcnn_model'\n",
    "backbone = 'resnet50'  # vgg16, vgg19, resnet50, densenet121, densenet169, densenet201\n",
    "\n",
    "n_epoch = 10  # Number of training epochs\n",
    "lr = 1e-5\n",
    "\n",
    "optimizer = Adam(lr=lr, clipnorm=0.001)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=lr, decay=0.99)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "num_classes = 1  # \"object\" is the only class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.retinanet_anchor_utils import get_anchor_parameters\n",
    "\n",
    "flat_shape = [y_train.shape[0] * y_train.shape[1]] + list(y_train.shape[2:])\n",
    "flat_y = np.reshape(y_train, tuple(flat_shape)).astype('int')\n",
    "\n",
    "# Generate backbone information from the data\n",
    "backbone_levels, pyramid_levels, anchor_params = get_anchor_parameters(flat_y)\n",
    "\n",
    "fpb = 3  # number of frames in each training batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016-2019 The Van Valen Lab at the California Institute of\n",
    "# Technology (Caltech), with support from the Paul Allen Family Foundation,\n",
    "# Google, & National Institutes of Health (NIH) under Grant U24CA224309-01.\n",
    "# All rights reserved.\n",
    "#\n",
    "# Licensed under a modified Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.github.com/vanvalenlab/deepcell-tf/LICENSE\n",
    "#\n",
    "# The Work provided may be used for non-commercial academic purposes only.\n",
    "# For any other use of the Work, including commercial use, please contact:\n",
    "# vanvalenlab@gmail.com\n",
    "#\n",
    "# Neither the name of Caltech nor the names of its contributors may be used\n",
    "# to endorse or promote products derived from this software without specific\n",
    "# prior written permission.\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"TrackRCNN models adapted from MaskRCNN and https://github.com/fizyr/keras-maskrcnn\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.layers import Add, Activation, Flatten, Dense\n",
    "from tensorflow.python.keras.layers import Input, Concatenate\n",
    "from tensorflow.python.keras.layers import TimeDistributed, Conv2D, Conv3D\n",
    "from tensorflow.python.keras.layers import AveragePooling2D, AveragePooling3D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling2D, GlobalAveragePooling3D\n",
    "from tensorflow.python.keras.layers import MaxPool2D, MaxPool3D, Lambda\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.initializers import normal\n",
    "\n",
    "from deepcell.layers import Cast, Shape, UpsampleLike\n",
    "from deepcell.layers import Upsample, RoiAlign, ConcatenateBoxes\n",
    "from deepcell.layers import ClipBoxes, RegressBoxes, FilterDetections\n",
    "from deepcell.layers import TensorProduct, ImageNormalization2D, Location2D\n",
    "from deepcell.layers import ImageNormalization3D, Location3D\n",
    "from deepcell.model_zoo.retinanet import retinanet, __build_anchors\n",
    "from deepcell.utils.retinanet_anchor_utils import AnchorParameters\n",
    "from deepcell.utils.backbone_utils import get_backbone\n",
    "\n",
    "\n",
    "def default_mask_model(num_classes,\n",
    "                       pyramid_feature_size=256,\n",
    "                       mask_feature_size=256,\n",
    "                       roi_size=(14, 14),\n",
    "                       mask_size=(28, 28),\n",
    "                       name='mask_submodel',\n",
    "                       mask_dtype=K.floatx(),\n",
    "                       retinanet_dtype=K.floatx()):\n",
    "    \"\"\"Creates the default mask submodel.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of classes to predict a score for at each\n",
    "            feature level.\n",
    "        pyramid_feature_size (int): The number of filters to expect from the\n",
    "            feature pyramid levels.\n",
    "        mask_feature_size (int): The number of filters to expect from the masks.\n",
    "        roi_size (tuple): The number of filters to use in the Roi Layers.\n",
    "        mask_size (tuple): The size of the masks.\n",
    "        mask_dtype (str): Dtype to use for mask tensors.\n",
    "        retinanet_dtype (str): Dtype retinanet models expect.\n",
    "        name (str): The name of the submodel.\n",
    "\n",
    "    Returns:\n",
    "        tensorflow.keras.Model: a Model that predicts classes for\n",
    "            each anchor.\n",
    "    \"\"\"\n",
    "    options = {\n",
    "        'kernel_size': 3,\n",
    "        'strides': 1,\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': normal(mean=0.0, stddev=0.01, seed=None),\n",
    "        'bias_initializer': 'zeros',\n",
    "        'activation': 'relu',\n",
    "    }\n",
    "\n",
    "    inputs = Input(shape=(None, roi_size[0], roi_size[1], pyramid_feature_size))\n",
    "    outputs = inputs\n",
    "\n",
    "    # casting to the desidered data type, which may be different than\n",
    "    # the one used for the underlying keras-retinanet model\n",
    "    if mask_dtype != retinanet_dtype:\n",
    "        outputs = TimeDistributed(\n",
    "            Cast(dtype=mask_dtype),\n",
    "            name='cast_masks')(outputs)\n",
    "\n",
    "    for i in range(4):\n",
    "        outputs = TimeDistributed(Conv2D(\n",
    "            filters=mask_feature_size,\n",
    "            **options\n",
    "        ), name='roi_mask_{}'.format(i))(outputs)\n",
    "\n",
    "    # perform upsampling + conv instead of deconv as in the paper\n",
    "    # https://distill.pub/2016/deconv-checkerboard/\n",
    "    outputs = TimeDistributed(\n",
    "        Upsample(mask_size),\n",
    "        name='roi_mask_upsample')(outputs)\n",
    "    outputs = TimeDistributed(Conv2D(\n",
    "        filters=mask_feature_size,\n",
    "        **options\n",
    "    ), name='roi_mask_features')(outputs)\n",
    "\n",
    "    outputs = TimeDistributed(Conv2D(\n",
    "        filters=num_classes,\n",
    "        kernel_size=1,\n",
    "        activation='sigmoid'\n",
    "    ), name='roi_mask')(outputs)\n",
    "\n",
    "    # casting back to the underlying keras-retinanet model data type\n",
    "    if mask_dtype != retinanet_dtype:\n",
    "        outputs = TimeDistributed(\n",
    "            Cast(dtype=retinanet_dtype),\n",
    "            name='recast_masks')(outputs)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "def default_final_detection_model(pyramid_feature_size=256,\n",
    "                                  final_detection_feature_size=256,\n",
    "                                  roi_size=(14, 14),\n",
    "                                  name='final_detection_submodel'):\n",
    "    options = {\n",
    "        'kernel_size': 3,\n",
    "        'strides': 1,\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': normal(mean=0.0, stddev=0.01, seed=None),\n",
    "        'bias_initializer': 'zeros',\n",
    "        'activation': 'relu'\n",
    "    }\n",
    "\n",
    "    inputs = Input(shape=(None, roi_size[0], roi_size[1], pyramid_feature_size))\n",
    "    outputs = inputs\n",
    "\n",
    "    for i in range(2):\n",
    "        outputs = TimeDistributed(Conv2D(\n",
    "            filters=final_detection_feature_size,\n",
    "            **options\n",
    "        ), name='final_detection_submodel_conv1_block{}'.format(i))(outputs)\n",
    "        outputs = TimeDistributed(Conv2D(\n",
    "            filters=final_detection_feature_size,\n",
    "            **options\n",
    "        ), name='final_detection_submodel_conv2_block{}'.format(i))(outputs)\n",
    "        outputs = TimeDistributed(MaxPool2D(\n",
    "        ), name='final_detection_submodel_pool1_block{}'.format(i))(outputs)\n",
    "\n",
    "    outputs = TimeDistributed(Conv2D(filters=final_detection_feature_size,\n",
    "                                     kernel_size=3,\n",
    "                                     padding='valid',\n",
    "                                     kernel_initializer=normal(mean=0.0, stddev=0.01, seed=None),\n",
    "                                     bias_initializer='zeros',\n",
    "                                     activation='relu'))(outputs)\n",
    "\n",
    "    outputs = TimeDistributed(Conv2D(filters=1,\n",
    "                                     kernel_size=1,\n",
    "                                     activation='sigmoid'))(outputs)\n",
    "\n",
    "    outputs = Lambda(lambda x: tf.squeeze(x, axis=[2, 3]))(outputs)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "def default_roi_submodels(num_classes,\n",
    "                          num_association_features,\n",
    "                          roi_size=(14, 14),\n",
    "                          mask_size=(28, 28),\n",
    "                          frames_per_batch=1,\n",
    "                          mask_dtype=K.floatx(),\n",
    "                          retinanet_dtype=K.floatx()):\n",
    "    \"\"\"Create a list of default roi submodels.\n",
    "\n",
    "    The default submodels contains a single mask model.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of classes to use.\n",
    "        roi_size (tuple): The number of filters to use in the Roi Layers.\n",
    "        mask_size (tuple): The size of the masks.\n",
    "        mask_dtype (str): Dtype to use for mask tensors.\n",
    "        retinanet_dtype (str): Dtype retinanet models expect.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuple, where the first element is the name of the\n",
    "            submodel and the second element is the submodel itself.\n",
    "    \"\"\"\n",
    "    if frames_per_batch > 1:\n",
    "        return [\n",
    "            ('masks', TimeDistributed(\n",
    "                default_mask_model(num_classes,\n",
    "                                   name='mask_submodel_0',\n",
    "                                   roi_size=roi_size,\n",
    "                                   mask_size=mask_size,\n",
    "                                   mask_dtype=mask_dtype,\n",
    "                                   retinanet_dtype=retinanet_dtype), name='mask_submodel')),\n",
    "            ('final_detection', TimeDistributed(default_final_detection_model(roi_size=roi_size), \n",
    "                                                name = 'final_detection_submodel')),\n",
    "            ('association_features', TimeDistributed(\n",
    "                                     association_vector_model(num_association_features,\n",
    "                                         roi_size=roi_size,\n",
    "                                         name='assoc_vec_submodel_0',\n",
    "                                         frames_per_batch=frames_per_batch),\n",
    "                                     name='assoc_head_submodel'))\n",
    "        ]\n",
    "    return [\n",
    "        ('masks', default_mask_model(num_classes,\n",
    "                                     roi_size=roi_size,\n",
    "                                     mask_size=mask_size,\n",
    "                                     mask_dtype=mask_dtype,\n",
    "                                     retinanet_dtype=retinanet_dtype))\n",
    "        # ('final_detection', default_final_detection_model(roi_size=roi_size))\n",
    "        ]\n",
    "\n",
    "\n",
    "def association_vector_model(num_association_features,\n",
    "                             roi_size=(14, 14),\n",
    "                             pyramid_feature_size=256,\n",
    "                             frames_per_batch=1,\n",
    "                             name='assoc_head_submodel'):\n",
    "    options = {\n",
    "        'kernel_size': 3,\n",
    "        'strides': 1,\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': normal(mean=0.0, stddev=0.01, seed=None),\n",
    "        'bias_initializer': 'zeros',\n",
    "        'activation': 'relu'\n",
    "    }\n",
    "\n",
    "    inputs = Input(shape=(None, roi_size[0], roi_size[1], pyramid_feature_size))\n",
    "    # inputs = Input(shape=(None, None, None, num_association_features))\n",
    "\n",
    "    conv1 = TimeDistributed(Conv2D(\n",
    "        filters=pyramid_feature_size,\n",
    "        **options\n",
    "    ), name='association_vector_submodel_conv1')(inputs)\n",
    "    conv2 = TimeDistributed(Conv2D(\n",
    "        filters=pyramid_feature_size,\n",
    "        **options\n",
    "    ), name='association_vector_submodel_conv2')(conv1)\n",
    "    x = conv2\n",
    "    x = TimeDistributed(MaxPool2D(\n",
    "    ), name='association_vector_submodel_pool1')(conv2)\n",
    "\n",
    "    # Residuals\n",
    "    for i in range(2):\n",
    "        x = TimeDistributed(Conv2D(filters=pyramid_feature_size,\n",
    "                                   kernel_size=3,\n",
    "                                   padding='valid',\n",
    "                                   kernel_initializer=normal(mean=0.0, stddev=0.01, seed=None),\n",
    "                                   bias_initializer='zeros',\n",
    "                                   activation='relu', \n",
    "                                   name='association_vector_residual_conv1_block{}'.format(i)))(x)\n",
    "        y = TimeDistributed(Conv2D(filters=pyramid_feature_size,\n",
    "                                   kernel_size=3,\n",
    "                                   padding='same',\n",
    "                                   kernel_initializer=normal(mean=0.0, stddev=0.01, seed=None),\n",
    "                                   bias_initializer='zeros',\n",
    "                                   activation='relu',\n",
    "                                   name='association_vector_residual_conv2_block{}'.format(i)))(x)\n",
    "#         x = TimeDistributed(Add())([x, y])\n",
    "#         x = TimeDistributed(Activation('relu', name='association_vector_residual_relu_block{}'.format(i)))(x)     \n",
    "        x = Add(name='association_vector_residual_add_block{}'.format(i))([x, y])\n",
    "        x = Activation('relu', name='association_vector_residual_relu_block{}'.format(i))(x)     \n",
    "\n",
    "\n",
    "    y = TimeDistributed(AveragePooling2D(pool_size=3,\n",
    "                        name='association_vector_averagepooling'))(x)\n",
    "#     y = TimeDistributed(Flatten(data_format='channels_last',\n",
    "#                                 name='association_vector_flatten'))(y)\n",
    "#     y = TimeDistributed(GlobalAveragePooling2D(data_format='channels_last',\n",
    "#                               name='association_vector_globalavgpooling'))(x)\n",
    "#     print(\"GlobalAveragePooling3D.shape: \", y.shape)\n",
    "    \n",
    "    outputs = TimeDistributed(Dense(num_association_features,\n",
    "                    activation='softmax',\n",
    "#                     kernel_initializer='he_normal',\n",
    "                    name='association_vector_dense_output'))(y)\n",
    "    outputs = Lambda(lambda x: tf.squeeze(x, axis=[2, 3]))(outputs)\n",
    "\n",
    "    print(\"outputs.shape\", outputs.shape)\n",
    "        \n",
    "    return Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "def retinanet_mask(inputs,\n",
    "                   backbone_dict,\n",
    "                   num_classes,\n",
    "                   num_association_features,\n",
    "                   frames_per_batch=1,\n",
    "                   backbone_levels=['C3', 'C4', 'C5'],\n",
    "                   pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "                   retinanet_model=None,\n",
    "                   anchor_params=None,\n",
    "                   nms=True,\n",
    "                   panoptic=False,\n",
    "                   use_assoc_head=False,\n",
    "                   class_specific_filter=True,\n",
    "                   crop_size=(14, 14),\n",
    "                   mask_size=(28, 28),\n",
    "                   name='retinanet-mask',\n",
    "                   roi_submodels=None,\n",
    "                   max_detections=100,\n",
    "                   score_threshold=0.05,\n",
    "                   nms_threshold=0.5,\n",
    "                   mask_dtype=K.floatx(),\n",
    "                   **kwargs):\n",
    "    \"\"\"Construct a RetinaNet mask model on top of a retinanet bbox model.\n",
    "    Uses the retinanet bbox model and appends layers to compute masks.\n",
    "\n",
    "    Args:\n",
    "        inputs (tensor): List of tensorflow.keras.layers.Input.\n",
    "            The first input is the image, the second input the blob of masks.\n",
    "        num_classes (int): Integer, number of classes to classify.\n",
    "        retinanet_model (tensorflow.keras.Model): RetinaNet model that predicts\n",
    "            regression and classification values.\n",
    "        anchor_params (AnchorParameters): Struct containing anchor parameters.\n",
    "        nms (bool): Whether to use NMS.\n",
    "        class_specific_filter (bool): Use class specific filtering.\n",
    "        roi_submodels (list): Submodels for processing ROIs.\n",
    "        name (str): Name of the model.\n",
    "        mask_dtype (str): Dtype to use for mask tensors.\n",
    "        kwargs (dict): Additional kwargs to pass to the retinanet bbox model.\n",
    "\n",
    "    Returns:\n",
    "        tensorflow.keras.Model: Model with inputs as input and as output\n",
    "            the output of each submodel for each pyramid level and the\n",
    "            detections. The order is as defined in submodels.\n",
    "\n",
    "            ```\n",
    "            [\n",
    "                regression, classification, other[0], ...,\n",
    "                boxes_masks, boxes, scores, labels, masks, other[0], ...\n",
    "            ]\n",
    "            ```\n",
    "\n",
    "    \"\"\"\n",
    "    if anchor_params is None:\n",
    "        anchor_params = AnchorParameters.default\n",
    "\n",
    "    if roi_submodels is None:\n",
    "        retinanet_dtype = K.floatx()\n",
    "        K.set_floatx(mask_dtype)\n",
    "        roi_submodels = default_roi_submodels(\n",
    "            num_classes, num_association_features, crop_size, mask_size,\n",
    "            frames_per_batch, mask_dtype, retinanet_dtype)\n",
    "        K.set_floatx(retinanet_dtype)\n",
    "\n",
    "    image = inputs\n",
    "    image_shape = Shape()(image)\n",
    "\n",
    "    if retinanet_model is None:\n",
    "        retinanet_model = retinanet(\n",
    "            inputs=image,\n",
    "            backbone_dict=backbone_dict,\n",
    "            num_classes=num_classes,\n",
    "            backbone_levels=backbone_levels,\n",
    "            pyramid_levels=pyramid_levels,\n",
    "            panoptic=panoptic,\n",
    "            num_anchors=anchor_params.num_anchors(),\n",
    "            frames_per_batch=frames_per_batch,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    # parse outputs\n",
    "    regression = retinanet_model.outputs[0]\n",
    "    classification = retinanet_model.outputs[1]\n",
    "\n",
    "    if panoptic:\n",
    "        # Determine the number of semantic heads\n",
    "        n_semantic_heads = len([1 for layer in retinanet_model.layers if 'semantic' in layer.name])\n",
    "\n",
    "        # The  panoptic output should not be sent to filter detections\n",
    "        other = retinanet_model.outputs[2:-n_semantic_heads]\n",
    "        semantic = retinanet_model.outputs[-n_semantic_heads:]\n",
    "    else:\n",
    "        other = retinanet_model.outputs[2:]\n",
    "\n",
    "\n",
    "    features = [retinanet_model.get_layer(name).output\n",
    "                for name in pyramid_levels]\n",
    "\n",
    "    # build boxes\n",
    "    anchors = __build_anchors(anchor_params, features,\n",
    "                              frames_per_batch=frames_per_batch)\n",
    "    boxes = RegressBoxes(name='boxes')([anchors, regression])\n",
    "    boxes = ClipBoxes(name='clipped_boxes')([image, boxes])\n",
    "\n",
    "    # filter detections (apply NMS / score threshold / select top-k)\n",
    "    detections = FilterDetections(\n",
    "        nms=nms,\n",
    "        nms_threshold=nms_threshold,\n",
    "        score_threshold=score_threshold,\n",
    "        class_specific_filter=class_specific_filter,\n",
    "        max_detections=max_detections,\n",
    "        name='filtered_detections'\n",
    "    )([boxes, classification] + other)\n",
    "\n",
    "    # split up in known outputs and \"other\"\n",
    "    boxes = detections[0]\n",
    "    scores = detections[1]\n",
    "\n",
    "    # get the region of interest features\n",
    "    #\n",
    "    # roi_input = [image_shape, boxes, classification] + features\n",
    "    # rois = _RoiAlign(crop_size=crop_size)(roi_input)\n",
    "\n",
    "    fpn = features[0]\n",
    "    fpn = UpsampleLike()([fpn, image])\n",
    "    rois = RoiAlign(crop_size=crop_size)([boxes, fpn])\n",
    "\n",
    "    # execute trackrcnn submodels\n",
    "    trackrcnn_outputs = [submodel(rois) for _, submodel in roi_submodels]\n",
    "    print(\"trackrcnn_outputs:\")\n",
    "    for x in trackrcnn_outputs:\n",
    "        print(x.name, x.shape, x.dtype)\n",
    "\n",
    "    # concatenate boxes for loss computation\n",
    "    trainable_outputs = [ConcatenateBoxes(name=name)([boxes, output])\n",
    "                         for (name, _), output in zip(\n",
    "                             roi_submodels, trackrcnn_outputs)]\n",
    "    \n",
    "    trainable_outputs[-1] = Concatenate(name='association_features_cat')([trainable_outputs[-1], trainable_outputs[-2]])\n",
    "\n",
    "\n",
    "    print(\"roi_submodels names in roi_submodels:\")\n",
    "    for (name, _) in roi_submodels:\n",
    "        print(name)\n",
    "    \n",
    "    print(\"trainable_outputs:\")\n",
    "    for x in trainable_outputs:\n",
    "        print(x.name, x.shape, x.dtype)\n",
    "\n",
    "\n",
    "    outputs = [regression, classification] + other + trainable_outputs + \\\n",
    "        detections + trackrcnn_outputs\n",
    "\n",
    "    if panoptic:\n",
    "        print(\"is panoptic\")\n",
    "        outputs += list(semantic)\n",
    "\n",
    "    print(\"outputs:\", outputs)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=name)\n",
    "    model.backbone_levels = backbone_levels\n",
    "    model.pyramid_levels = pyramid_levels\n",
    "\n",
    "    return model\n",
    "\n",
    "def RetinaMask(backbone,\n",
    "               num_classes,\n",
    "               input_shape,\n",
    "               inputs=None,\n",
    "               backbone_levels=['C3', 'C4', 'C5'],\n",
    "               pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "               norm_method='whole_image',\n",
    "               location=False,\n",
    "               use_imagenet=False,\n",
    "               crop_size=(14, 14),\n",
    "               pooling=None,\n",
    "               mask_dtype=K.floatx(),\n",
    "               required_channels=3,\n",
    "               frames_per_batch=1,\n",
    "               use_assoc_head=False,\n",
    "               num_association_features=2,\n",
    "               **kwargs):\n",
    "    \"\"\"Constructs a mrcnn model using a backbone from keras-applications.\n",
    "\n",
    "    Args:\n",
    "        backbone (str): Name of backbone to use.\n",
    "        num_classes (int): Number of classes to classify.\n",
    "        input_shape (tuple): The shape of the input data.\n",
    "        weights (str): one of None (random initialization),\n",
    "            'imagenet' (pre-training on ImageNet),\n",
    "            or the path to the weights file to be loaded.\n",
    "        pooling (str): optional pooling mode for feature extraction\n",
    "            when include_top is False.\n",
    "            - None means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - 'avg' means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - 'max' means that global max pooling will\n",
    "                be applied.\n",
    "        required_channels (int): The required number of channels of the\n",
    "            backbone.  3 is the default for all current backbones.\n",
    "\n",
    "    Returns:\n",
    "        tensorflow.keras.Model: RetinaNet model with a backbone.\n",
    "    \"\"\"\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    if inputs is None:\n",
    "        if frames_per_batch > 1:\n",
    "            if channel_axis == 1:\n",
    "                input_shape_with_time = tuple(\n",
    "                    [input_shape[0], frames_per_batch] + list(input_shape)[1:])\n",
    "            else:\n",
    "                input_shape_with_time = tuple(\n",
    "                    [frames_per_batch] + list(input_shape))\n",
    "            inputs = Input(shape=input_shape_with_time, name='image_input')\n",
    "        else:\n",
    "            inputs = Input(shape=input_shape, name='image_input')\n",
    "\n",
    "    if location:\n",
    "        if frames_per_batch > 1:\n",
    "            # TODO: TimeDistributed is incompatible with channels_first\n",
    "            loc = TimeDistributed(Location2D(in_shape=input_shape))(inputs)\n",
    "        else:\n",
    "            loc = Location2D(in_shape=input_shape)(inputs)\n",
    "        concat = Concatenate(axis=channel_axis)([inputs, loc])\n",
    "    else:\n",
    "        concat = inputs\n",
    "\n",
    "    # force the channel size for backbone input to be `required_channels`\n",
    "    if frames_per_batch > 1:\n",
    "        norm = TimeDistributed(ImageNormalization2D(norm_method=norm_method))(concat)\n",
    "        fixed_inputs = TimeDistributed(TensorProduct(required_channels))(norm)\n",
    "    else:\n",
    "        norm = ImageNormalization2D(norm_method=norm_method)(concat)\n",
    "        fixed_inputs = TensorProduct(required_channels)(norm)\n",
    "\n",
    "    # force the input shape\n",
    "    axis = 0 if K.image_data_format() == 'channels_first' else -1\n",
    "    fixed_input_shape = list(input_shape)\n",
    "    fixed_input_shape[axis] = required_channels\n",
    "    fixed_input_shape = tuple(fixed_input_shape)\n",
    "\n",
    "    model_kwargs = {\n",
    "        'include_top': False,\n",
    "        'weights': None,\n",
    "        'input_shape': fixed_input_shape,\n",
    "        'pooling': pooling\n",
    "    }\n",
    "\n",
    "    _, backbone_dict = get_backbone(backbone, fixed_inputs,\n",
    "                                    use_imagenet=use_imagenet,\n",
    "                                    frames_per_batch=frames_per_batch,\n",
    "                                    return_dict=True, **model_kwargs)\n",
    "\n",
    "    # create the full model\n",
    "    return retinanet_mask(\n",
    "        inputs=inputs,\n",
    "        num_classes=num_classes,\n",
    "        backbone_dict=backbone_dict,\n",
    "        crop_size=crop_size,\n",
    "        backbone_levels=backbone_levels,\n",
    "        pyramid_levels=pyramid_levels,\n",
    "        name='{}_retinanet_mask'.format(backbone),\n",
    "        mask_dtype=mask_dtype,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        use_assoc_head=use_assoc_head,\n",
    "        num_association_features=num_association_features,\n",
    "        **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape (?, ?, 2)\n",
      "trackrcnn_outputs:\n",
      "mask_submodel_3/Reshape_1:0 (?, ?, ?, 28, 28, 1) <dtype: 'float32'>\n",
      "final_detection_submodel_3/Reshape_1:0 (?, ?, ?, 1) <dtype: 'float32'>\n",
      "assoc_head_submodel_5/Reshape_1:0 (?, ?, ?, 2) <dtype: 'float32'>\n",
      "roi_submodels names in roi_submodels:\n",
      "masks\n",
      "final_detection\n",
      "association_features\n",
      "trainable_outputs:\n",
      "masks_3/concat:0 (?, ?, 100, ?) <dtype: 'float32'>\n",
      "final_detection_3/concat:0 (?, ?, 100, ?) <dtype: 'float32'>\n",
      "association_features_cat/concat:0 (?, ?, 100, ?) <dtype: 'float32'>\n",
      "outputs: [<tf.Tensor 'regression_3/concat:0' shape=(?, 3, ?, 4) dtype=float32>, <tf.Tensor 'classification_3/concat:0' shape=(?, 3, ?, 1) dtype=float32>, <tf.Tensor 'masks_3/concat:0' shape=(?, ?, 100, ?) dtype=float32>, <tf.Tensor 'final_detection_3/concat:0' shape=(?, ?, 100, ?) dtype=float32>, <tf.Tensor 'association_features_cat/concat:0' shape=(?, ?, 100, ?) dtype=float32>, <tf.Tensor 'filtered_detections_3/Reshape_2:0' shape=(?, ?, 100, 4) dtype=float32>, <tf.Tensor 'filtered_detections_3/Reshape_3:0' shape=(?, ?, 100) dtype=float32>, <tf.Tensor 'filtered_detections_3/Reshape_4:0' shape=(?, ?, 100) dtype=int32>, <tf.Tensor 'mask_submodel_3/Reshape_1:0' shape=(?, ?, ?, 28, 28, 1) dtype=float32>, <tf.Tensor 'final_detection_submodel_3/Reshape_1:0' shape=(?, ?, ?, 1) dtype=float32>, <tf.Tensor 'assoc_head_submodel_5/Reshape_1:0' shape=(?, ?, ?, 2) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "# Pass frames_per_batch > 1 to enable 3D mode!\n",
    "model = RetinaMask(\n",
    "    backbone=backbone,\n",
    "    input_shape=X_train.shape[2:],\n",
    "    frames_per_batch=fpb,\n",
    "    class_specific_filter=False,\n",
    "    panoptic=False,\n",
    "    num_classes=num_classes,\n",
    "    backbone_levels=backbone_levels,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params,\n",
    "    use_assoc_head=True,\n",
    ")\n",
    "\n",
    "prediction_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0310 21:05:36.533762 140161754629952 training_utils.py:1101] Output filtered_detections missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to filtered_detections.\n",
      "W0310 21:05:36.535824 140161754629952 training_utils.py:1101] Output filtered_detections_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to filtered_detections_1.\n",
      "W0310 21:05:36.537166 140161754629952 training_utils.py:1101] Output filtered_detections_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to filtered_detections_2.\n",
      "W0310 21:05:36.538431 140161754629952 training_utils.py:1101] Output mask_submodel missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to mask_submodel.\n",
      "W0310 21:05:36.539574 140161754629952 training_utils.py:1101] Output final_detection_submodel missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to final_detection_submodel.\n",
      "W0310 21:05:36.540783 140161754629952 training_utils.py:1101] Output assoc_head_submodel missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to assoc_head_submodel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_y_pred_shape (?, ?, ?)\n",
      "new_y_true_shape (?, ?, ?)\n",
      "n_detections Tensor(\"loss_6/association_features_cat_loss/strided_slice_18:0\", shape=(), dtype=int32)\n",
      "boxes shape (?, ?, ?)\n",
      "annotations shape (?, ?, ?)\n",
      "assoc_heads shape (?, ?, ?)\n",
      "top_indices_shape [3, None]\n",
      "top_indices_shape [3, None, 2]\n",
      "gather filtered_y_pred shape (3, ?, ?)\n",
      "filtered_y_pred shape (3, ?, ?)\n",
      "y_true shape (?, ?, ?)\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RetinaMask Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016-2019 The Van Valen Lab at the California Institute of\n",
    "# Technology (Caltech), with support from the Paul Allen Family Foundation,\n",
    "# Google, & National Institutes of Health (NIH) under Grant U24CA224309-01.\n",
    "# All rights reserved.\n",
    "#\n",
    "# Licensed under a modified Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.github.com/vanvalenlab/deepcell-tf/LICENSE\n",
    "#\n",
    "# The Work provided may be used for non-commercial academic purposes only.\n",
    "# For any other use of the Work, including commercial use, please contact:\n",
    "# vanvalenlab@gmail.com\n",
    "#\n",
    "# Neither the name of Caltech nor the names of its contributors may be used\n",
    "# to endorse or promote products derived from this software without specific\n",
    "# prior written permission.\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Image generators for training convolutional neural networks.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from skimage.measure import regionprops\n",
    "from skimage.segmentation import clear_border\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.python.keras.preprocessing.image import Iterator\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "from deepcell.utils.retinanet_anchor_utils import anchor_targets_bbox\n",
    "from deepcell.utils.retinanet_anchor_utils import anchors_for_shape\n",
    "from deepcell.utils.retinanet_anchor_utils import guess_shapes\n",
    "\n",
    "from deepcell.image_generators import _transform_masks\n",
    "from deepcell.image_generators import ImageFullyConvDataGenerator\n",
    "from deepcell.image_generators import MovieDataGenerator\n",
    "\n",
    "class RetinaNetGenerator(ImageFullyConvDataGenerator):\n",
    "    \"\"\"Generates batches of tensor image data with real-time data augmentation.\n",
    "    The data will be looped over (in batches).\n",
    "\n",
    "    Args:\n",
    "        featurewise_center: boolean, set input mean to 0 over the dataset,\n",
    "            feature-wise.\n",
    "        samplewise_center: boolean, set each sample mean to 0.\n",
    "        featurewise_std_normalization: boolean, divide inputs by std\n",
    "            of the dataset, feature-wise.\n",
    "        samplewise_std_normalization: boolean, divide each input by its std.\n",
    "        zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
    "        zca_whitening: boolean, apply ZCA whitening.\n",
    "        rotation_range: int, degree range for random rotations.\n",
    "        width_shift_range: float, 1-D array-like or int\n",
    "            float: fraction of total width, if < 1, or pixels if >= 1.\n",
    "            1-D array-like: random elements from the array.\n",
    "            int: integer number of pixels from interval\n",
    "                (-width_shift_range, +width_shift_range)\n",
    "            With width_shift_range=2 possible values are ints [-1, 0, +1],\n",
    "            same as with width_shift_range=[-1, 0, +1],\n",
    "            while with width_shift_range=1.0 possible values are floats in\n",
    "            the interval [-1.0, +1.0).\n",
    "        shear_range: float, shear Intensity\n",
    "            (Shear angle in counter-clockwise direction in degrees)\n",
    "        zoom_range: float or [lower, upper], Range for random zoom.\n",
    "            If a float, [lower, upper] = [1-zoom_range, 1+zoom_range].\n",
    "        channel_shift_range: float, range for random channel shifts.\n",
    "        fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n",
    "            Default is 'nearest'. Points outside the boundaries of the input\n",
    "            are filled according to the given mode:\n",
    "                'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
    "                'nearest':  aaaaaaaa|abcd|dddddddd\n",
    "                'reflect':  abcddcba|abcd|dcbaabcd\n",
    "                'wrap':  abcdabcd|abcd|abcdabcd\n",
    "        cval: float or int, value used for points outside the boundaries\n",
    "            when fill_mode = \"constant\".\n",
    "        horizontal_flip: boolean, randomly flip inputs horizontally.\n",
    "        vertical_flip: boolean, randomly flip inputs vertically.\n",
    "        rescale: rescaling factor. Defaults to None. If None or 0, no rescaling\n",
    "            is applied, otherwise we multiply the data by the value provided\n",
    "            (before applying any other transformation).\n",
    "        preprocessing_function: function that will be implied on each input.\n",
    "            The function will run after the image is resized and augmented.\n",
    "            The function should take one argument:\n",
    "            one image (Numpy tensor with rank 3),\n",
    "            and should output a Numpy tensor with the same shape.\n",
    "        data_format: One of {\"channels_first\", \"channels_last\"}.\n",
    "            \"channels_last\" mode means that the images should have shape\n",
    "                (samples, height, width, channels),\n",
    "            \"channels_first\" mode means that the images should have shape\n",
    "                (samples, channels, height, width).\n",
    "            It defaults to the image_data_format value found in your\n",
    "                Keras config file at \"~/.keras/keras.json\".\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "        validation_split: float, fraction of images reserved for validation\n",
    "            (strictly between 0 and 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def flow(self,\n",
    "             train_dict,\n",
    "             compute_shapes=guess_shapes,\n",
    "             min_objects=3,\n",
    "             num_classes=1,\n",
    "             clear_borders=False,\n",
    "             include_masks=False,\n",
    "             include_final_detection_layer=False,\n",
    "             panoptic=False,\n",
    "             transforms=['watershed'],\n",
    "             transforms_kwargs={},\n",
    "             assoc_head=False,\n",
    "             anchor_params=None,\n",
    "             pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "             batch_size=32,\n",
    "             shuffle=False,\n",
    "             seed=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png'):\n",
    "        \"\"\"Generates batches of augmented/normalized data with given arrays.\n",
    "\n",
    "        Args:\n",
    "            train_dict: dictionary of X and y tensors. Both should be rank 4.\n",
    "            compute_shapes: function to determine the shapes of the anchors\n",
    "            min_classes: images with fewer than 'min_objects' are ignored\n",
    "            num_classes: number of classes to predict\n",
    "            clear_borders: boolean, whether to use clear_border on y.\n",
    "            include_masks: boolean, train on mask data (MaskRCNN).\n",
    "            batch_size: int (default: 1).\n",
    "            shuffle: boolean (default: True).\n",
    "            seed: int (default: None).\n",
    "            save_to_dir: None or str (default: None).\n",
    "                This allows you to optionally specify a directory\n",
    "                to which to save the augmented pictures being generated\n",
    "                (useful for visualizing what you are doing).\n",
    "            save_prefix: str (default: \"\"). Prefix to use for filenames of\n",
    "                saved pictures (only relevant if save_to_dir is set).\n",
    "            save_format: one of \"png\", \"jpeg\". Default: \"png\".\n",
    "                (only relevant if save_to_dir is set)\n",
    "\n",
    "        Returns:\n",
    "            An Iterator yielding tuples of (x, y) where x is a numpy array\n",
    "            of image data and y is a numpy array of labels of the same shape.\n",
    "        \"\"\"\n",
    "        return RetinaNetIterator(\n",
    "            train_dict,\n",
    "            self,\n",
    "            compute_shapes=compute_shapes,\n",
    "            min_objects=min_objects,\n",
    "            num_classes=num_classes,\n",
    "            clear_borders=clear_borders,\n",
    "            include_masks=include_masks,\n",
    "            include_final_detection_layer=include_final_detection_layer,\n",
    "            panoptic=panoptic,\n",
    "            transforms=transforms,\n",
    "            transforms_kwargs=transforms_kwargs,\n",
    "            assoc_head=assoc_head,\n",
    "            anchor_params=anchor_params,\n",
    "            pyramid_levels=pyramid_levels,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            data_format=self.data_format,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format)\n",
    "\n",
    "\n",
    "class RetinaNetIterator(Iterator):\n",
    "    \"\"\"Iterator yielding data from Numpy arrayss (X and y).\n",
    "\n",
    "    Adapted from https://github.com/fizyr/keras-retinanet.\n",
    "\n",
    "    Args:\n",
    "        train_dict: dictionary consisting of numpy arrays for X and y.\n",
    "        image_data_generator: Instance of ImageDataGenerator\n",
    "            to use for random transformations and normalization.\n",
    "        compute_shapes: functor for generating shapes, based on the model.\n",
    "        min_objects: Integer, image with fewer than min_objects are ignored.\n",
    "        num_classes: Integer, number of classes for classification.\n",
    "        clear_borders: Boolean, whether to call clear_border on y.\n",
    "        include_masks: Boolean, whether to yield mask data.\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        data_format: String, one of 'channels_first', 'channels_last'.\n",
    "        save_to_dir: Optional directory where to save the pictures\n",
    "            being yielded, in a viewable format. This is useful\n",
    "            for visualizing the random transformations being\n",
    "            applied, for debugging purposes.\n",
    "        save_prefix: String prefix to use for saving sample\n",
    "            images (if save_to_dir is set).\n",
    "        save_format: Format to use for saving sample images\n",
    "            (if save_to_dir is set).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_dict,\n",
    "                 image_data_generator,\n",
    "                 compute_shapes=guess_shapes,\n",
    "                 anchor_params=None,\n",
    "                 pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "                 min_objects=3,\n",
    "                 num_classes=1,\n",
    "                 clear_borders=False,\n",
    "                 include_masks=False,\n",
    "                 panoptic=False,\n",
    "                 include_final_detection_layer=False,\n",
    "                 transforms=['watershed'],\n",
    "                 transforms_kwargs={},\n",
    "                 assoc_head=False,\n",
    "                 batch_size=32,\n",
    "                 shuffle=False,\n",
    "                 seed=None,\n",
    "                 data_format='channels_last',\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png'):\n",
    "        X, y = train_dict['X'], train_dict['y']\n",
    "\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError('Training batches and labels should have the same'\n",
    "                             'length. Found X.shape: {} y.shape: {}'.format(\n",
    "                                 X.shape, y.shape))\n",
    "\n",
    "        if X.ndim != 4:\n",
    "            raise ValueError('Input data in `RetinaNetIterator` '\n",
    "                             'should have rank 4. You passed an array '\n",
    "                             'with shape', X.shape)\n",
    "\n",
    "        self.x = np.asarray(X, dtype=K.floatx())\n",
    "        self.y = np.asarray(y, dtype='int32')\n",
    "\n",
    "        # `compute_shapes` changes based on the model backbone.\n",
    "        self.compute_shapes = compute_shapes\n",
    "        self.anchor_params = anchor_params\n",
    "        self.pyramid_levels = [int(l[1:]) for l in pyramid_levels]\n",
    "        self.min_objects = min_objects\n",
    "        self.num_classes = num_classes\n",
    "        self.include_masks = include_masks\n",
    "        self.include_final_detection_layer = include_final_detection_layer\n",
    "        self.panoptic = panoptic\n",
    "        self.transforms = transforms\n",
    "        self.transforms_kwargs = transforms_kwargs\n",
    "        self.assoc_head = assoc_head\n",
    "        self.channel_axis = 3 if data_format == 'channels_last' else 1\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.data_format = data_format\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "\n",
    "        self.y_semantic_list = []  # optional semantic segmentation targets\n",
    "\n",
    "        # Add semantic segmentation targets if panoptic segmentation\n",
    "        # flag is True\n",
    "        if panoptic:\n",
    "            # Create a list of all the semantic targets. We need to be able\n",
    "            # to have multiple semantic heads\n",
    "            # Add all the keys that contain y_semantic\n",
    "            for key in train_dict:\n",
    "                if 'y_semantic' in key:\n",
    "                    self.y_semantic_list.append(train_dict[key])\n",
    "\n",
    "            # Add transformed masks\n",
    "            for transform in transforms:\n",
    "                transform_kwargs = transforms_kwargs.get(transform, dict())\n",
    "                y_transform = _transform_masks(y, transform,\n",
    "                                               data_format=data_format,\n",
    "                                               **transform_kwargs)\n",
    "                y_transform = np.asarray(y_transform, dtype='int32')\n",
    "                self.y_semantic_list.append(y_transform)\n",
    "\n",
    "        invalid_batches = []\n",
    "        # Remove images with small numbers of cells\n",
    "        for b in range(self.x.shape[0]):\n",
    "            y_batch = np.squeeze(self.y[b], axis=self.channel_axis - 1)\n",
    "            y_batch = clear_border(y_batch) if clear_borders else y_batch\n",
    "            y_batch = np.expand_dims(y_batch, axis=self.channel_axis - 1)\n",
    "\n",
    "            self.y[b] = y_batch\n",
    "\n",
    "            if len(np.unique(self.y[b])) - 1 < self.min_objects:\n",
    "                invalid_batches.append(b)\n",
    "\n",
    "        invalid_batches = np.array(invalid_batches, dtype='int')\n",
    "\n",
    "        if invalid_batches.size > 0:\n",
    "            logging.warning('Removing %s of %s images with fewer than %s '\n",
    "                            'objects.', invalid_batches.size, self.x.shape[0],\n",
    "                            self.min_objects)\n",
    "\n",
    "        self.y = np.delete(self.y, invalid_batches, axis=0)\n",
    "        self.x = np.delete(self.x, invalid_batches, axis=0)\n",
    "\n",
    "        self.y_semantic_list = [np.delete(y, invalid_batches, axis=0)\n",
    "                                for y in self.y_semantic_list]\n",
    "\n",
    "        super(RetinaNetIterator, self).__init__(\n",
    "            self.x.shape[0], batch_size, shuffle, seed)\n",
    "\n",
    "    def filter_annotations(self, image, annotations):\n",
    "        \"\"\"Filter annotations by removing those that are outside of the\n",
    "        image bounds or whose width/height < 0.\n",
    "\n",
    "        Args:\n",
    "            image: ndarray, the raw image data.\n",
    "            annotations: dict of annotations including labels and bboxes\n",
    "        \"\"\"\n",
    "        row_axis = 1 if self.data_format == 'channels_first' else 0\n",
    "        invalid_indices = np.where(\n",
    "            (annotations['bboxes'][:, 2] <= annotations['bboxes'][:, 0]) |\n",
    "            (annotations['bboxes'][:, 3] <= annotations['bboxes'][:, 1]) |\n",
    "            (annotations['bboxes'][:, 0] < 0) |\n",
    "            (annotations['bboxes'][:, 1] < 0) |\n",
    "            (annotations['bboxes'][:, 2] > image.shape[row_axis + 1]) |\n",
    "            (annotations['bboxes'][:, 3] > image.shape[row_axis])\n",
    "        )[0]\n",
    "\n",
    "        # delete invalid indices\n",
    "        if invalid_indices.size > 0:\n",
    "            logging.warn('Image with shape {} contains the following invalid '\n",
    "                         'boxes: {}.'.format(\n",
    "                             image.shape,\n",
    "                             annotations['bboxes'][invalid_indices, :]))\n",
    "\n",
    "            for k in annotations.keys():\n",
    "                filtered = np.delete(annotations[k], invalid_indices, axis=0)\n",
    "                annotations[k] = filtered\n",
    "        return annotations\n",
    "\n",
    "    def load_annotations(self, y):\n",
    "        \"\"\"Generate bounding box and label annotations for a tensor\n",
    "\n",
    "        Args:\n",
    "            y: tensor to annotate\n",
    "\n",
    "        Returns:\n",
    "            dict: annotations of bboxes and labels\n",
    "        \"\"\"\n",
    "        labels, bboxes, masks = [], [], []\n",
    "        for prop in regionprops(np.squeeze(y.astype('int'))):\n",
    "            y1, x1, y2, x2 = prop.bbox\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            labels.append(0)  # boolean object detection\n",
    "            masks.append(np.where(y == prop.label, 1, 0))\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        bboxes = np.array(bboxes)\n",
    "        masks = np.array(masks).astype('uint8')\n",
    "\n",
    "        # reshape bboxes in case it is empty.\n",
    "        bboxes = np.reshape(bboxes, (bboxes.shape[0], 4))\n",
    "\n",
    "        annotations = {'labels': labels, 'bboxes': bboxes}\n",
    "        if self.include_masks:\n",
    "            annotations['masks'] = masks\n",
    "\n",
    "        annotations = self.filter_annotations(y, annotations)\n",
    "        return annotations\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros(tuple([len(index_array)] + list(self.x.shape)[1:]))\n",
    "\n",
    "        batch_y_semantic_list = []\n",
    "        for y_sem in self.y_semantic_list:\n",
    "            shape = tuple([len(index_array)] + list(y_sem.shape[1:]))\n",
    "            batch_y_semantic_list.append(np.zeros(shape, dtype=y_sem.dtype))\n",
    "\n",
    "        annotations_list = []\n",
    "\n",
    "        max_shape = []\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            x = self.x[j]\n",
    "            y = self.y[j]\n",
    "\n",
    "            y_semantic_list = [y_sem[j] for y_sem in self.y_semantic_list]\n",
    "\n",
    "            # Apply transformation\n",
    "            x, y_list = self.image_data_generator.random_transform(\n",
    "                x, [y] + y_semantic_list)\n",
    "\n",
    "            y = y_list[0]\n",
    "            y_semantic_list = y_list[1:]\n",
    "\n",
    "            # Find max shape of image data.  Used for masking.\n",
    "            if not max_shape:\n",
    "                max_shape = list(x.shape)\n",
    "            else:\n",
    "                for k in range(len(x.shape)):\n",
    "                    if x.shape[k] > max_shape[k]:\n",
    "                        max_shape[k] = x.shape[k]\n",
    "\n",
    "            # Get the bounding boxes from the transformed masks!\n",
    "            annotations = self.load_annotations(y)\n",
    "            annotations_list.append(annotations)\n",
    "\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "\n",
    "            batch_x[i] = x\n",
    "\n",
    "            for k, y_sem in enumerate(y_semantic_list):\n",
    "                batch_y_semantic_list[k][i] = y_sem\n",
    "\n",
    "        anchors = anchors_for_shape(\n",
    "            batch_x.shape[1:],\n",
    "            pyramid_levels=self.pyramid_levels,\n",
    "            anchor_params=self.anchor_params,\n",
    "            shapes_callback=self.compute_shapes)\n",
    "\n",
    "        regressions, labels = anchor_targets_bbox(\n",
    "            anchors,\n",
    "            batch_x,\n",
    "            annotations_list,\n",
    "            self.num_classes)\n",
    "\n",
    "        max_shape = tuple(max_shape)  # was a list for max shape indexing\n",
    "\n",
    "        print(\"annotations_list: \", annotations_list)          \n",
    "\n",
    "        if self.include_masks:\n",
    "            # masks_batch has shape: (batch size, max_annotations,\n",
    "            #     bbox_x1 + bbox_y1 + bbox_x2 + bbox_y2 + label +\n",
    "            #     width + height + max_image_dimension)\n",
    "            max_annotations = max(len(a['masks']) for a in annotations_list)\n",
    "            masks_batch_shape = (len(index_array), max_annotations,\n",
    "                                 5 + 2 + max_shape[0] * max_shape[1])\n",
    "            masks_batch = np.zeros(masks_batch_shape, dtype=K.floatx())\n",
    "\n",
    "            for i, ann in enumerate(annotations_list):\n",
    "                masks_batch[i, :ann['bboxes'].shape[0], :4] = ann['bboxes']\n",
    "                masks_batch[i, :ann['labels'].shape[0], 4] = ann['labels']\n",
    "                masks_batch[i, :, 5] = max_shape[1]  # width\n",
    "                masks_batch[i, :, 6] = max_shape[0]  # height\n",
    "\n",
    "                # add flattened mask\n",
    "                for j, mask in enumerate(ann['masks']):\n",
    "                    masks_batch[i, j, 7:] = mask.flatten()\n",
    "\n",
    "        if self.save_to_dir:\n",
    "            for i, j in enumerate(index_array):\n",
    "                if self.data_format == 'channels_first':\n",
    "                    img_x = np.expand_dims(batch_x[i, 0, ...], 0)\n",
    "                else:\n",
    "                    img_x = np.expand_dims(batch_x[i, ..., 0], -1)\n",
    "                img = array_to_img(img_x, self.data_format, scale=True)\n",
    "                fname = '{prefix}_{index}_{hash}.{format}'.format(\n",
    "                    prefix=self.save_prefix,\n",
    "                    index=j,\n",
    "                    hash=np.random.randint(1e4),\n",
    "                    format=self.save_format)\n",
    "                img.save(os.path.join(self.save_to_dir, fname))\n",
    "\n",
    "        batch_inputs = batch_x\n",
    "        batch_outputs = [regressions, labels]\n",
    "        \n",
    "        if self.assoc_head:\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.include_masks:\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.include_final_detection_layer:\n",
    "            batch_outputs.append(masks_batch)\n",
    "\n",
    "        batch_outputs.extend(batch_y_semantic_list)\n",
    "\n",
    "        print(\"batch_inputs: \", batch_inputs)\n",
    "        print(\"batch_outputs: \", batch_outputs)\n",
    "\n",
    "        return batch_inputs, batch_outputs\n",
    "\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x. Returns the next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)\n",
    "\n",
    "\n",
    "    \n",
    "class RetinaMovieIterator(Iterator):\n",
    "    \"\"\"Iterator yielding data from Numpy arrayss (`X and `y`).\n",
    "\n",
    "    Adapted from https://github.com/fizyr/keras-retinanet.\n",
    "\n",
    "    Args:\n",
    "        train_dict: dictionary consisting of numpy arrays for `X` and `y`.\n",
    "        image_data_generator: Instance of `ImageDataGenerator`\n",
    "            to use for random transformations and normalization.\n",
    "        compute_shapes: functor for generating shapes, based on the model.\n",
    "        min_objects: Integer, image with fewer than `min_objects` are ignored.\n",
    "        num_classes: Integer, number of classes for classification.\n",
    "        clear_borders: Boolean, whether to call `clear_border` on `y`.\n",
    "        include_masks: Boolean, whether to yield mask data.\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        data_format: String, one of `channels_first`, `channels_last`.\n",
    "        save_to_dir: Optional directory where to save the pictures\n",
    "            being yielded, in a viewable format. This is useful\n",
    "            for visualizing the random transformations being\n",
    "            applied, for debugging purposes.\n",
    "        save_prefix: String prefix to use for saving sample\n",
    "            images (if `save_to_dir` is set).\n",
    "        save_format: Format to use for saving sample images\n",
    "            (if `save_to_dir` is set).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_dict,\n",
    "                 movie_data_generator,\n",
    "                 compute_shapes=guess_shapes,\n",
    "                 anchor_params=None,\n",
    "                 pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "                 min_objects=3,\n",
    "                 num_classes=1,\n",
    "                 frames_per_batch=2,\n",
    "                 clear_borders=False,\n",
    "                 include_masks=False,\n",
    "                 include_final_detection_layer=False,\n",
    "                 assoc_head=False,\n",
    "                 panoptic=False,\n",
    "                 transforms=['watershed'],\n",
    "                 transforms_kwargs={},\n",
    "                 batch_size=32,\n",
    "                 shuffle=False,\n",
    "                 seed=None,\n",
    "                 data_format='channels_last',\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png'):\n",
    "        X, y = train_dict['X'], train_dict['y']\n",
    "\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError('Training batches and labels should have the same'\n",
    "                             'length. Found X.shape: {} y.shape: {}'.format(\n",
    "                                 X.shape, y.shape))\n",
    "\n",
    "        if X.ndim != 5:\n",
    "            raise ValueError('Input data in `RetinaNetIterator` '\n",
    "                             'should have rank 5. You passed an array '\n",
    "                             'with shape', X.shape)\n",
    "\n",
    "        self.x = np.asarray(X, dtype=K.floatx())\n",
    "        self.y = np.asarray(y, dtype='int32')\n",
    "\n",
    "        # `compute_shapes` changes based on the model backbone.\n",
    "        self.compute_shapes = compute_shapes\n",
    "        self.anchor_params = anchor_params\n",
    "        self.pyramid_levels = [int(l[1:]) for l in pyramid_levels]\n",
    "        self.min_objects = min_objects\n",
    "        self.num_classes = num_classes\n",
    "        self.frames_per_batch = frames_per_batch\n",
    "        self.include_masks = include_masks\n",
    "        self.include_final_detection_layer = include_final_detection_layer\n",
    "        self.assoc_head = assoc_head\n",
    "        self.panoptic = panoptic\n",
    "        self.transforms = transforms\n",
    "        self.transforms_kwargs = transforms_kwargs\n",
    "        self.channel_axis = 4 if data_format == 'channels_last' else 1\n",
    "        self.time_axis = 1 if data_format == 'channels_last' else 2\n",
    "        self.row_axis = 2 if data_format == 'channels_last' else 3\n",
    "        self.col_axis = 3 if data_format == 'channels_last' else 4\n",
    "        self.movie_data_generator = movie_data_generator\n",
    "        self.data_format = data_format\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "\n",
    "        self.y_semantic_list = []  # optional semantic segmentation targets\n",
    "\n",
    "        if X.shape[self.time_axis] - frames_per_batch < 0:\n",
    "            raise ValueError(\n",
    "                'The number of frames used in each training batch should '\n",
    "                'be less than the number of frames in the training data!')\n",
    "\n",
    "        # Add semantic segmentation targets if panoptic segmentation\n",
    "        # flag is True\n",
    "        print(\"train_dict keys :\")\n",
    "        for key in train_dict:\n",
    "            print(key)\n",
    "        if panoptic:\n",
    "            # Create a list of all the semantic targets. We need to be able\n",
    "            # to have multiple semantic heads\n",
    "            # Add all the keys that contain y_semantic\n",
    "            for key in train_dict:\n",
    "                if 'y_semantic' in key:\n",
    "                    self.y_semantic_list.append(train_dict[key])\n",
    "\n",
    "            # Add transformed masks\n",
    "            for transform in transforms:\n",
    "                transform_kwargs = transforms_kwargs.get(transform, dict())\n",
    "                y_transforms = []\n",
    "                for time in range(y.shape[self.time_axis]):\n",
    "                    if data_format == 'channels_first':\n",
    "                        y_temp = y[:, :, time, ...]\n",
    "                    else:\n",
    "                        y_temp = y[:, time, ...]\n",
    "                    y_temp_transform = _transform_masks(\n",
    "                        y_temp, transform,\n",
    "                        data_format=data_format,\n",
    "                        **transform_kwargs)\n",
    "                    y_temp_transform = np.asarray(y_temp_transform, dtype='int32')\n",
    "                    y_transforms.append(y_temp_transform)\n",
    "\n",
    "                y_transform = np.stack(y_transforms, axis=self.time_axis)\n",
    "                self.y_semantic_list.append(y_transform)\n",
    "\n",
    "        invalid_batches = []\n",
    "        # Remove images with small numbers of cells\n",
    "        for b in range(self.x.shape[0]):\n",
    "            y_batch = np.squeeze(self.y[b], axis=self.channel_axis - 1)\n",
    "            y_batch = clear_border(y_batch) if clear_borders else y_batch\n",
    "            y_batch = np.expand_dims(y_batch, axis=self.channel_axis - 1)\n",
    "\n",
    "            self.y[b] = y_batch\n",
    "\n",
    "            if len(np.unique(self.y[b])) - 1 < self.min_objects:\n",
    "                invalid_batches.append(b)\n",
    "\n",
    "        invalid_batches = np.array(invalid_batches, dtype='int')\n",
    "\n",
    "        if invalid_batches.size > 0:\n",
    "            logging.warning('Removing %s of %s images with fewer than %s '\n",
    "                            'objects.', invalid_batches.size, self.x.shape[0],\n",
    "                            self.min_objects)\n",
    "\n",
    "        self.y = np.delete(self.y, invalid_batches, axis=0)\n",
    "        self.x = np.delete(self.x, invalid_batches, axis=0)\n",
    "\n",
    "        self.y_semantic_list = [np.delete(y, invalid_batches, axis=0)\n",
    "                                for y in self.y_semantic_list]\n",
    "\n",
    "        super(RetinaMovieIterator, self).__init__(\n",
    "            self.x.shape[0], batch_size, shuffle, seed)\n",
    "\n",
    "    def filter_annotations(self, image, annotations):\n",
    "        \"\"\"Filter annotations by removing those that are outside of the\n",
    "        image bounds or whose width/height < 0.\n",
    "\n",
    "        Args:\n",
    "            image: ndarray, the raw image data.\n",
    "            annotations: dict of annotations including `labels` and `bboxes`\n",
    "        \"\"\"\n",
    "        row_axis = 1 if self.data_format == 'channels_first' else 0\n",
    "        invalid_indices = np.where(\n",
    "            (annotations['bboxes'][:, 2] <= annotations['bboxes'][:, 0]) |\n",
    "            (annotations['bboxes'][:, 3] <= annotations['bboxes'][:, 1]) |\n",
    "            (annotations['bboxes'][:, 0] < 0) |\n",
    "            (annotations['bboxes'][:, 1] < 0) |\n",
    "            (annotations['bboxes'][:, 2] > image.shape[row_axis + 1]) |\n",
    "            (annotations['bboxes'][:, 3] > image.shape[row_axis])\n",
    "        )[0]\n",
    "\n",
    "        # delete invalid indices\n",
    "        if invalid_indices.size > 0:\n",
    "            logging.warn('Image with shape {} contains the following invalid '\n",
    "                         'boxes: {}.'.format(\n",
    "                             image.shape,\n",
    "                             annotations['bboxes'][invalid_indices, :]))\n",
    "\n",
    "            for k in annotations.keys():\n",
    "                filtered = np.delete(annotations[k], invalid_indices, axis=0)\n",
    "                annotations[k] = filtered\n",
    "        return annotations\n",
    "\n",
    "    def load_annotations(self, y):\n",
    "        \"\"\"Generate bounding box and label annotations for a tensor\n",
    "\n",
    "        Args:\n",
    "            y: tensor to annotate\n",
    "\n",
    "        Returns:\n",
    "            annotations: dict of `bboxes` and `labels`\n",
    "        \"\"\"\n",
    "        labels, bboxes, masks = [], [], []\n",
    "        channel_axis = 1 if self.data_format == 'channels_first' else -1\n",
    "\n",
    "        y_cropped = []\n",
    "        max_width = 0\n",
    "        max_height = 0\n",
    "        \n",
    "        for prop in regionprops(np.squeeze(y.astype('int'))):\n",
    "            y1, x1, y2, x2 = prop.bbox\n",
    "            if x2-x1 > max_width:\n",
    "                max_width = x2-x1\n",
    "            if y2-y1 > max_height:\n",
    "                max_height = y2-y1\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            labels.append(0)  # boolean object detection\n",
    "            masks.append(np.where(y == prop.label, 1, 0))\n",
    "            \n",
    "        labels = np.array(labels)\n",
    "        bboxes = np.array(bboxes)\n",
    "        masks = np.array(masks).astype('uint8')\n",
    "\n",
    "        # reshape bboxes in case it is empty.\n",
    "        bboxes = np.reshape(bboxes, (bboxes.shape[0], 4))\n",
    "\n",
    "        annotations = {'labels': labels, 'bboxes': bboxes}\n",
    "\n",
    "        if self.include_masks:\n",
    "            annotations['masks'] = masks\n",
    "\n",
    "\n",
    "        if self.assoc_head:\n",
    "            num_unique = len(np.unique(self.y)) - 1\n",
    "            y_transform = to_categorical(y.squeeze(channel_axis), num_classes=num_unique)\n",
    "            if self.data_format == 'channels_first':\n",
    "                y_transform = np.rollaxis(y_transform, y.ndim - 1, 1)\n",
    "            N = y_transform.shape[-1]\n",
    "            # print(\"y_transform.shape\", y_transform.shape)\n",
    "            \n",
    "            assoc_head = np.zeros((bboxes.shape[0], N))\n",
    "            # print(\"bboxes.shape[0]\", bboxes.shape[0])\n",
    "            for i in range(bboxes.shape[0]):\n",
    "                x1, y1, x2, y2 = bboxes[i]\n",
    "                assoc_head[i] = y_transform[int((y1+y2)/2), int((x1+x2)/2)]\n",
    "                \n",
    "            annotations['assoc_head'] = assoc_head\n",
    "\n",
    "        annotations = self.filter_annotations(y, annotations)\n",
    "        return annotations\n",
    "\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        if self.data_format == 'channels_first':\n",
    "            batch_x = np.zeros((len(index_array),\n",
    "                                self.x.shape[1],\n",
    "                                self.frames_per_batch,\n",
    "                                self.x.shape[3],\n",
    "                                self.x.shape[4]))\n",
    "        else:\n",
    "            batch_x = np.zeros(tuple([len(index_array), self.frames_per_batch] +\n",
    "                                     list(self.x.shape)[2:]))\n",
    "\n",
    "        if self.panoptic:\n",
    "            if self.data_format == 'channels_first':\n",
    "                batch_y_semantic_list = [np.zeros(tuple([len(index_array),\n",
    "                                                         y_semantic.shape[1],\n",
    "                                                         self.frames_per_batch,\n",
    "                                                         y_semantic.shape[3],\n",
    "                                                         y_semantic.shape[4]]))\n",
    "                                         for y_semantic in self.y_semantic_list]\n",
    "            else:\n",
    "                batch_y_semantic_list = [\n",
    "                    np.zeros(tuple([len(index_array), self.frames_per_batch] +\n",
    "                                   list(y_semantic.shape[2:])))\n",
    "                    for y_semantic in self.y_semantic_list\n",
    "                ]\n",
    "\n",
    "        annotations_list = [[] for _ in range(self.frames_per_batch)]\n",
    "\n",
    "        max_shape = []\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            last_frame = self.x.shape[self.time_axis] - self.frames_per_batch\n",
    "            time_start = np.random.randint(0, high=last_frame)\n",
    "            time_end = time_start + self.frames_per_batch\n",
    "            times = list(np.arange(time_start, time_end))\n",
    "\n",
    "            if self.time_axis == 1:\n",
    "                x = self.x[j, time_start:time_end, ...]\n",
    "                y = self.y[j, time_start:time_end, ...]\n",
    "            elif self.time_axis == 2:\n",
    "                x = self.x[j, :, time_start:time_end, ...]\n",
    "                y = self.y[j, :, time_start:time_end, ...]\n",
    "\n",
    "            if self.panoptic:\n",
    "                if self.time_axis == 1:\n",
    "                    y_semantic_list = [y_semantic[j, time_start:time_end, ...]\n",
    "                                       for y_semantic in self.y_semantic_list]\n",
    "                elif self.time_axis == 2:\n",
    "                    y_semantic_list = [y_semantic[j, :, time_start:time_end, ...]\n",
    "                                       for y_semantic in self.y_semantic_list]\n",
    "\n",
    "            # Apply transformation\n",
    "            if self.panoptic:\n",
    "                x, y_list = self.movie_data_generator.random_transform(x, [y] + y_semantic_list)\n",
    "                y = y_list[0]\n",
    "                y_semantic_list = y_list[1:]\n",
    "            else:\n",
    "                x, y = self.movie_data_generator.random_transform(x, y)\n",
    "\n",
    "            x = self.movie_data_generator.standardize(x)\n",
    "\n",
    "            # Find max shape of image data.  Used for masking.\n",
    "            if not max_shape:\n",
    "                max_shape = list(x.shape)\n",
    "            else:\n",
    "                for k in range(len(x.shape)):\n",
    "                    if x.shape[k] > max_shape[k]:\n",
    "                        max_shape[k] = x.shape[k]\n",
    "\n",
    "            # Get the bounding boxes from the transformed masks!\n",
    "            for idx_time, time in enumerate(times):\n",
    "                if self.time_axis == 1:\n",
    "                    annotations = self.load_annotations(y[idx_time])\n",
    "                elif self.time_axis == 2:\n",
    "                    annotations = self.load_annotations(y[:, idx_time, ...])\n",
    "                annotations_list[idx_time].append(annotations)\n",
    "\n",
    "            batch_x[i] = x\n",
    "\n",
    "            if self.panoptic:\n",
    "                for k in range(len(y_semantic_list)):\n",
    "                    batch_y_semantic_list[k][i] = y_semantic_list[k]\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            batch_x_shape = [batch_x.shape[1], batch_x.shape[3], batch_x.shape[4]]\n",
    "        else:\n",
    "            batch_x_shape = batch_x.shape[2:]\n",
    "\n",
    "        anchors = anchors_for_shape(\n",
    "            batch_x_shape,\n",
    "            pyramid_levels=self.pyramid_levels,\n",
    "            anchor_params=self.anchor_params,\n",
    "            shapes_callback=self.compute_shapes)\n",
    "\n",
    "        regressions_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            batch_x_frame = batch_x[:, :, 0, ...]\n",
    "        else:\n",
    "            batch_x_frame = batch_x[:, 0, ...]\n",
    "        for idx, time in enumerate(times):\n",
    "            regressions, labels = anchor_targets_bbox(\n",
    "                anchors,\n",
    "                batch_x_frame,\n",
    "                annotations_list[idx],\n",
    "                self.num_classes)\n",
    "            regressions_list.append(regressions)\n",
    "            labels_list.append(labels)\n",
    "\n",
    "        regressions = np.stack(regressions_list, axis=self.time_axis)\n",
    "        labels = np.stack(labels_list, axis=self.time_axis)\n",
    "\n",
    "        # was a list for max shape indexing\n",
    "        max_shape = tuple([max_shape[self.row_axis - 1],\n",
    "                           max_shape[self.col_axis - 1]])\n",
    "\n",
    "        if self.assoc_head:\n",
    "            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "            annotations_list_flatten = flatten(annotations_list)\n",
    "            max_annotations = max(len(a['assoc_head']) for a in annotations_list_flatten)\n",
    "            batch_N_unique = max(a['assoc_head'].shape[-1] for a in annotations_list_flatten)\n",
    "            assoc_heads_batch_shape = (len(index_array), self.frames_per_batch, \n",
    "                                       max_annotations, 8 + batch_N_unique)\n",
    "            assoc_heads_batch = np.zeros(assoc_heads_batch_shape, dtype=K.floatx())\n",
    "            for idx_time, time in enumerate(times):\n",
    "                annotations_frame = annotations_list[idx_time]\n",
    "                for idx_batch, ann in enumerate(annotations_frame):\n",
    "                    assoc_heads_batch[idx_batch, idx_time, :ann['bboxes'].shape[0], :4] = ann['bboxes']\n",
    "                    assoc_heads_batch[idx_batch, idx_time, :ann['labels'].shape[0], 4] = ann['labels']\n",
    "                    assoc_heads_batch[idx_batch, idx_time, :, 5] = max_shape[1]  # width\n",
    "                    assoc_heads_batch[idx_batch, idx_time, :, 6] = max_shape[0]  # height\n",
    "                    assoc_heads_batch[idx_batch, idx_time, :, 7] = batch_N_unique  # batch_N_unique\n",
    "                    \n",
    "                    # add flattened association head\n",
    "                    for idx_mask, assoc_head in enumerate(ann['assoc_head']):\n",
    "                        assoc_heads_batch[idx_batch, idx_time, idx_mask, 8:] = ann['assoc_head'][idx_mask]\n",
    "\n",
    "\n",
    "        if self.include_masks:\n",
    "            # masks_batch has shape: (batch size, max_annotations,\n",
    "            #     bbox_x1 + bbox_y1 + bbox_x2 + bbox_y2 + label +\n",
    "            #     width + height + max_image_dimension)\n",
    "\n",
    "            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "            annotations_list_flatten = flatten(annotations_list)\n",
    "            max_annotations = max(len(a['masks']) for a in annotations_list_flatten)\n",
    "            masks_batch_shape = (len(index_array), self.frames_per_batch, max_annotations,\n",
    "                                 5 + 2 + max_shape[0] * max_shape[1])\n",
    "            # print(\"masks_batch_shape: \", masks_batch_shape)\n",
    "            masks_batch = np.zeros(masks_batch_shape, dtype=K.floatx())\n",
    "            \n",
    "            batch_x_bbox_shape = (len(index_array), self.frames_per_batch, max_annotations, 4)\n",
    "            batch_x_bbox = np.zeros(batch_x_bbox_shape, dtype=K.floatx())\n",
    "\n",
    "            for idx_time, time in enumerate(times):\n",
    "                annotations_frame = annotations_list[idx_time]\n",
    "                for idx_batch, ann in enumerate(annotations_frame):\n",
    "                    batch_x_bbox[idx_batch, idx_time, :ann['bboxes'].shape[0], :4] = ann['bboxes']\n",
    "                    masks_batch[idx_batch, idx_time, :ann['bboxes'].shape[0], :4] = ann['bboxes']\n",
    "                    masks_batch[idx_batch, idx_time, :ann['labels'].shape[0], 4] = ann['labels']\n",
    "                    masks_batch[idx_batch, idx_time, :, 5] = max_shape[1]  # width\n",
    "                    masks_batch[idx_batch, idx_time, :, 6] = max_shape[0]  # height\n",
    "\n",
    "                    # add flattened mask\n",
    "                    for idx_mask, mask in enumerate(ann['masks']):\n",
    "                        masks_batch[idx_batch, idx_time, idx_mask, 7:] = mask.flatten()\n",
    "\n",
    "        if self.save_to_dir:\n",
    "            for i, j in enumerate(index_array):\n",
    "                for frame in range(batch_x.shape[self.time_axis]):\n",
    "                    if self.time_axis == 2:\n",
    "                        img = array_to_img(batch_x[i, :, frame], self.data_format, scale=True)\n",
    "                    else:\n",
    "                        img = array_to_img(batch_x[i, frame], self.data_format, scale=True)\n",
    "                    fname = '{prefix}_{index}_{hash}.{format}'.format(\n",
    "                        prefix=self.save_prefix,\n",
    "                        index=j,\n",
    "                        hash=np.random.randint(1e4),\n",
    "                        format=self.save_format)\n",
    "                    img.save(os.path.join(self.save_to_dir, fname))\n",
    "\n",
    "        batch_inputs = batch_x\n",
    "        batch_outputs = [regressions, labels]\n",
    "        print(\"annotations['bboxes'].shape\", annotations['bboxes'].shape)\n",
    "    \n",
    "        if self.include_masks:\n",
    "            print(\"batch_outputs masks shape:\", masks_batch.shape)\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.include_final_detection_layer:\n",
    "            batch_outputs.append(masks_batch)\n",
    "        if self.panoptic:\n",
    "            batch_outputs += batch_y_semantic_list\n",
    "            print(\"batch_outputs batch_y_semantic_list shape:\", batch_y_semantic_list[0].shape)\n",
    "        if self.assoc_head:\n",
    "            batch_inputs = [batch_x, batch_x_bbox]\n",
    "            batch_outputs.append(assoc_heads_batch)\n",
    "            print(\"batch_outputs assoc_heads_batch shape:\", assoc_heads_batch.shape)\n",
    "\n",
    "        return batch_inputs, batch_outputs\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x. Returns the next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)\n",
    "\n",
    "\n",
    "class RetinaMovieDataGenerator(MovieDataGenerator):\n",
    "    \"\"\"Generates batches of tensor image data with real-time data augmentation.\n",
    "    The data will be looped over (in batches).\n",
    "\n",
    "    Args:\n",
    "        featurewise_center: boolean, set input mean to 0 over the dataset,\n",
    "            feature-wise.\n",
    "        samplewise_center: boolean, set each sample mean to 0.\n",
    "        featurewise_std_normalization: boolean, divide inputs by std\n",
    "            of the dataset, feature-wise.\n",
    "        samplewise_std_normalization: boolean, divide each input by its std.\n",
    "        zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
    "        zca_whitening: boolean, apply ZCA whitening.\n",
    "        rotation_range: int, degree range for random rotations.\n",
    "        width_shift_range: float, 1-D array-like or int\n",
    "            float: fraction of total width, if < 1, or pixels if >= 1.\n",
    "            1-D array-like: random elements from the array.\n",
    "            int: integer number of pixels from interval\n",
    "                `(-width_shift_range, +width_shift_range)`\n",
    "            With `width_shift_range=2` possible values are ints [-1, 0, +1],\n",
    "            same as with `width_shift_range=[-1, 0, +1]`,\n",
    "            while with `width_shift_range=1.0` possible values are floats in\n",
    "            the interval [-1.0, +1.0).\n",
    "        shear_range: float, shear Intensity\n",
    "            (Shear angle in counter-clockwise direction in degrees)\n",
    "        zoom_range: float or [lower, upper], Range for random zoom.\n",
    "            If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
    "        channel_shift_range: float, range for random channel shifts.\n",
    "        fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n",
    "            Default is 'nearest'. Points outside the boundaries of the input\n",
    "            are filled according to the given mode:\n",
    "                'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
    "                'nearest':  aaaaaaaa|abcd|dddddddd\n",
    "                'reflect':  abcddcba|abcd|dcbaabcd\n",
    "                'wrap':  abcdabcd|abcd|abcdabcd\n",
    "        cval: float or int, value used for points outside the boundaries\n",
    "            when `fill_mode = \"constant\"`.\n",
    "        horizontal_flip: boolean, randomly flip inputs horizontally.\n",
    "        vertical_flip: boolean, randomly flip inputs vertically.\n",
    "        rescale: rescaling factor. Defaults to None. If None or 0, no rescaling\n",
    "            is applied, otherwise we multiply the data by the value provided\n",
    "            (before applying any other transformation).\n",
    "        preprocessing_function: function that will be implied on each input.\n",
    "            The function will run after the image is resized and augmented.\n",
    "            The function should take one argument:\n",
    "            one image (Numpy tensor with rank 3),\n",
    "            and should output a Numpy tensor with the same shape.\n",
    "        data_format: One of {\"channels_first\", \"channels_last\"}.\n",
    "            \"channels_last\" mode means that the images should have shape\n",
    "                `(samples, height, width, channels)`,\n",
    "            \"channels_first\" mode means that the images should have shape\n",
    "                `(samples, channels, height, width)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "                Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "        validation_split: float, fraction of images reserved for validation\n",
    "            (strictly between 0 and 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def flow(self,\n",
    "             train_dict,\n",
    "             batch_size=1,\n",
    "             frames_per_batch=5,\n",
    "             compute_shapes=guess_shapes,\n",
    "             num_classes=1,\n",
    "             clear_borders=False,\n",
    "             include_masks=False,\n",
    "             include_final_detection_layer=False,\n",
    "             panoptic=False,\n",
    "             assoc_head=False,\n",
    "             transforms=['watershed'],\n",
    "             transforms_kwargs={},\n",
    "             anchor_params=None,\n",
    "             pyramid_levels=['P3', 'P4', 'P5', 'P6', 'P7'],\n",
    "             shuffle=False,\n",
    "             seed=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png'):\n",
    "        \"\"\"Generates batches of augmented/normalized data with given arrays.\n",
    "\n",
    "        Args:\n",
    "            train_dict: dictionary of X and y tensors. Both should be rank 5.\n",
    "            frames_per_batch: int (default: 10).\n",
    "                size of z axis in generated batches\n",
    "            batch_size: int (default: 1).\n",
    "            shuffle: boolean (default: True).\n",
    "            seed: int (default: None).\n",
    "            save_to_dir: None or str (default: None).\n",
    "                This allows you to optionally specify a directory\n",
    "                to which to save the augmented pictures being generated\n",
    "                (useful for visualizing what you are doing).\n",
    "            save_prefix: str (default: `''`). Prefix to use for filenames of\n",
    "                saved pictures (only relevant if `save_to_dir` is set).\n",
    "            save_format: one of \"png\", \"jpeg\". Default: \"png\".\n",
    "                (only relevant if `save_to_dir` is set)\n",
    "\n",
    "        Returns:\n",
    "            An Iterator yielding tuples of `(x, y)` where `x` is a numpy array\n",
    "            of image data and `y` is a numpy array of labels of the same shape.\n",
    "        \"\"\"\n",
    "        return RetinaMovieIterator(\n",
    "            train_dict,\n",
    "            self,\n",
    "            compute_shapes=compute_shapes,\n",
    "            num_classes=num_classes,\n",
    "            clear_borders=clear_borders,\n",
    "            include_masks=include_masks,\n",
    "            include_final_detection_layer=include_final_detection_layer,\n",
    "            assoc_head=assoc_head,\n",
    "            panoptic=panoptic,\n",
    "            transforms=transforms,\n",
    "            transforms_kwargs=transforms_kwargs,\n",
    "            anchor_params=anchor_params,\n",
    "            pyramid_levels=pyramid_levels,\n",
    "            batch_size=batch_size,\n",
    "            frames_per_batch=frames_per_batch,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            data_format=self.data_format,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from deepcell.image_generators import RetinaMovieDataGenerator\n",
    "\n",
    "datagen = RetinaMovieDataGenerator(\n",
    "    rotation_range=180,\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "datagen_val = RetinaMovieDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dict keys :\n",
      "X\n",
      "y\n",
      "daughters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0310 21:05:51.832756 140161754629952 <ipython-input-31-03c66a88a46a>:643] Removing 2 of 192 images with fewer than 3 objects.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dict keys :\n",
      "X\n",
      "y\n",
      "daughters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0310 21:05:53.233552 140161754629952 <ipython-input-31-03c66a88a46a>:643] Removing 1 of 48 images with fewer than 3 objects.\n"
     ]
    }
   ],
   "source": [
    "train_data = datagen.flow(\n",
    "    train_dict,\n",
    "    batch_size=1,\n",
    "    include_masks=True,\n",
    "    include_final_detection_layer=True,\n",
    "    assoc_head=True,\n",
    "    panoptic=False,\n",
    "    frames_per_batch=fpb,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params)\n",
    "\n",
    "val_data = datagen_val.flow(\n",
    "    test_dict,\n",
    "    batch_size=1,\n",
    "    include_masks=True,\n",
    "    include_final_detection_layer=True,\n",
    "    assoc_head=True,\n",
    "    panoptic=False,\n",
    "    frames_per_batch=fpb,\n",
    "    pyramid_levels=pyramid_levels,\n",
    "    anchor_params=anchor_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations['bboxes'].shape (10, 4)\n",
      "batch_outputs masks shape: (1, 3, 10, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 10, 40)\n"
     ]
    }
   ],
   "source": [
    "next_data_x, next_data_y = train_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations['bboxes'].shape (6, 4)\n",
      "batch_outputs masks shape: (1, 3, 7, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 7, 40)\n",
      "annotations['bboxes'].shape (9, 4)\n",
      "batch_outputs masks shape: (1, 3, 9, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 9, 40)\n",
      "annotations['bboxes'].shape (11, 4)\n",
      "batch_outputs masks shape: (1, 3, 11, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 11, 40)\n",
      "annotations['bboxes'].shape (2, 4)\n",
      "batch_outputs masks shape: (1, 3, 4, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 4, 40)\n",
      "annotations['bboxes'].shape (8, 4)\n",
      "batch_outputs masks shape: (1, 3, 8, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 8, 40)\n",
      "annotations['bboxes'].shape (8, 4)\n",
      "batch_outputs masks shape: (1, 3, 11, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 11, 40)\n",
      "annotations['bboxes'].shape (9, 4)\n",
      "batch_outputs masks shape: (1, 3, 11, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 11, 40)\n",
      "annotations['bboxes'].shape (7, 4)\n",
      "batch_outputs masks shape: (1, 3, 8, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 8, 40)\n",
      "annotations['bboxes'].shape (6, 4)\n",
      "batch_outputs masks shape: (1, 3, 7, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 7, 40)\n",
      "annotations['bboxes'].shape (7, 4)\n",
      "batch_outputs masks shape: (1, 3, 7, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 7, 40)\n",
      "annotations['bboxes'].shape (6, 4)\n",
      "batch_outputs masks shape: (1, 3, 6, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 6, 40)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations['bboxes'].shape (13, 4)\n",
      "batch_outputs masks shape: (1, 3, 13, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 13, 40)\n",
      " 12/192 [>.............................] - ETA: 14:59 - loss: 14060843114.6667 - regression_loss: 2.9331 - classification_loss: 1.0978 - masks_loss: 0.0000e+00 - final_detection_loss: 1.6850 - association_features_cat_loss: 14060843008.0000annotations['bboxes'].shape (13, 4)\n",
      "batch_outputs masks shape: (1, 3, 13, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 13, 40)\n",
      " 13/192 [=>............................] - ETA: 14:39 - loss: 13273416467.6923 - regression_loss: 2.9362 - classification_loss: 1.0954 - masks_loss: 0.0178 - final_detection_loss: 1.6317 - association_features_cat_loss: 13273416704.0000    annotations['bboxes'].shape (5, 4)\n",
      "batch_outputs masks shape: (1, 3, 5, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 5, 40)\n",
      " 14/192 [=>............................] - ETA: 15:32 - loss: 14037811492.5714 - regression_loss: 2.9425 - classification_loss: 1.0940 - masks_loss: 0.0166 - final_detection_loss: 1.6999 - association_features_cat_loss: 14037812224.0000annotations['bboxes'].shape (8, 4)\n",
      "batch_outputs masks shape: (1, 3, 11, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 11, 40)\n",
      " 15/192 [=>............................] - ETA: 16:17 - loss: 13524099345.0667 - regression_loss: 2.9407 - classification_loss: 1.0898 - masks_loss: 0.0309 - final_detection_loss: 1.6648 - association_features_cat_loss: 13524101120.0000annotations['bboxes'].shape (5, 4)\n",
      "batch_outputs masks shape: (1, 3, 8, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 8, 40)\n",
      " 16/192 [=>............................] - ETA: 16:54 - loss: 13357803648.0000 - regression_loss: 2.9356 - classification_loss: 1.0850 - masks_loss: 0.0434 - final_detection_loss: 1.6609 - association_features_cat_loss: 13357804544.0000annotations['bboxes'].shape (7, 4)\n",
      "batch_outputs masks shape: (1, 3, 7, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 7, 40)\n",
      " 17/192 [=>............................] - ETA: 17:27 - loss: 13497902802.8235 - regression_loss: 2.9274 - classification_loss: 1.0795 - masks_loss: 0.0678 - final_detection_loss: 1.6706 - association_features_cat_loss: 13497903104.0000annotations['bboxes'].shape (9, 4)\n",
      "batch_outputs masks shape: (1, 3, 9, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 9, 40)\n",
      " 18/192 [=>............................] - ETA: 17:54 - loss: 13306956288.0000 - regression_loss: 2.9236 - classification_loss: 1.0730 - masks_loss: 0.0895 - final_detection_loss: 1.6547 - association_features_cat_loss: 13306957824.0000annotations['bboxes'].shape (4, 4)\n",
      "batch_outputs masks shape: (1, 3, 5, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 5, 40)\n",
      " 19/192 [=>............................] - ETA: 18:20 - loss: 13918482917.0526 - regression_loss: 2.9163 - classification_loss: 1.0643 - masks_loss: 0.0969 - final_detection_loss: 1.6973 - association_features_cat_loss: 13918483456.0000annotations['bboxes'].shape (10, 4)\n",
      "batch_outputs masks shape: (1, 3, 12, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 12, 40)\n",
      " 20/192 [==>...........................] - ETA: 18:42 - loss: 13488845414.4000 - regression_loss: 2.9173 - classification_loss: 1.0613 - masks_loss: 0.1146 - final_detection_loss: 1.6624 - association_features_cat_loss: 13488846848.0000annotations['bboxes'].shape (3, 4)\n",
      "batch_outputs masks shape: (1, 3, 3, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 3, 40)\n",
      " 21/192 [==>...........................] - ETA: 19:02 - loss: 16361308355.0476 - regression_loss: 2.9254 - classification_loss: 1.0538 - masks_loss: 0.1415 - final_detection_loss: 1.7629 - association_features_cat_loss: 16361309184.0000annotations['bboxes'].shape (9, 4)\n",
      "batch_outputs masks shape: (1, 3, 11, 28035)\n",
      "batch_outputs assoc_heads_batch shape: (1, 3, 11, 40)\n",
      " 22/192 [==>...........................] - ETA: 19:18 - loss: 15885115415.2727 - regression_loss: 2.9240 - classification_loss: 1.0488 - masks_loss: 0.1556 - final_detection_loss: 1.7304 - association_features_cat_loss: 15885116416.0000"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "from deepcell.callbacks import RedirectModel, Evaluate\n",
    "\n",
    "iou_threshold = 0.5\n",
    "score_threshold = 0.01\n",
    "max_detections = 100\n",
    "\n",
    "model.run_eagerly=False\n",
    "\n",
    "model.fit_generator(\n",
    "    train_data,\n",
    "    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "    epochs=n_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=X_test.shape[0] // batch_size,\n",
    "    callbacks=[\n",
    "        callbacks.LearningRateScheduler(lr_sched),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            os.path.join(MODEL_DIR, model_name + '.h5'),\n",
    "            monitor='val_loss',\n",
    "            verbose=3,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False),\n",
    "        RedirectModel(\n",
    "            Evaluate(val_data,\n",
    "                     iou_threshold=iou_threshold,\n",
    "                     score_threshold=score_threshold,\n",
    "                     max_detections=max_detections,\n",
    "                     frames_per_batch=fpb,\n",
    "                     weighted_average=True),\n",
    "            prediction_model)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in next_data_y:\n",
    "    print(x.shape)\n",
    "print(\"\\n\")\n",
    "for x in next_data_x:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminative_instance_loss(y_true, \n",
    "                                 y_pred,\n",
    "                                 delta_v=0.5,\n",
    "                                 delta_d=1.5,\n",
    "                                 gamma=1e-3):\n",
    "    \"\"\"Discriminative loss between an output tensor and a target tensor.\n",
    "\n",
    "    Args:\n",
    "        y_true: A tensor of the same shape as y_pred.\n",
    "        y_pred: A tensor of the vector embedding\n",
    "\n",
    "    Returns:\n",
    "        tensor: Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def temp_norm(ten, axis=None):\n",
    "        if axis is None:\n",
    "            axis = 1 if K.image_data_format() == 'channels_first' else K.ndim(ten) - 1\n",
    "        return K.sqrt(K.epsilon() + K.sum(K.square(ten), axis=axis))\n",
    "\n",
    "    if K.ndim(y_pred) == 4:\n",
    "        y_pred_shape = tf.shape(y_pred)\n",
    "        new_y_pred_shape = [y_pred_shape[0] * y_pred_shape[1],\n",
    "                            y_pred_shape[2], y_pred_shape[3]]\n",
    "        y_pred = tf.reshape(y_pred, new_y_pred_shape)\n",
    "        print(\"new_y_pred_shape\", y_pred.shape)\n",
    "\n",
    "        y_true_shape = tf.shape(y_true)\n",
    "        new_y_true_shape = [y_true_shape[0] * y_true_shape[1],\n",
    "                            y_true_shape[2], y_true_shape[3]]\n",
    "        y_true = tf.reshape(y_true, new_y_true_shape)\n",
    "        print(\"new_y_true_shape\", y_true.shape)\n",
    "    \n",
    "    # split up the different predicted blobs\n",
    "    assoc_feature_channel_shape = y_pred_shape[-1] - 5\n",
    "    boxes = y_pred[:, :, :4]\n",
    "    assoc_heads = y_pred[:, :, 4:assoc_feature_channel_shape]\n",
    "    channel_dim = y_pred.shape[-1]\n",
    "    final_detection_scores = final_detection_pred[..., -1:]\n",
    "\n",
    "    # split up the different blobs\n",
    "    annotations = y_true[:, :, :4]\n",
    "    labels = K.cast(y_true[:, :, 4:5], dtype='int32')\n",
    "    width = K.cast(y_true[0, 0, 5], dtype='int32')\n",
    "    height = K.cast(y_true[0, 0, 6], dtype='int32')\n",
    "    max_N = K.cast(y_true[0, 0, 7], dtype='int32')\n",
    "    assoc_heads_target = y_true[:, :, 8:]\n",
    "    n_detections = y_true_shape[2]\n",
    "    print(\"n_detections\", n_detections)\n",
    "    \n",
    "    print(\"boxes shape\", boxes.shape)\n",
    "    print(\"annotations shape\", annotations.shape)\n",
    "    \n",
    "    # reshape the assoc_heads back to their original size\n",
    "    assoc_heads_target = K.reshape(assoc_heads_target, (K.shape(assoc_heads_target)[0],\n",
    "                                            K.shape(assoc_heads_target)[1], max_N))\n",
    "    assoc_heads = K.reshape(assoc_heads, (K.shape(assoc_heads)[0], K.shape(assoc_heads)[1], \n",
    "                                          K.shape(assoc_heads)[2]))\n",
    "    print(\"assoc_heads shape\", assoc_heads.shape)\n",
    "\n",
    "   \n",
    "    temp = final_detection_scores[0,...,0]\n",
    "    top_vals, top_indices = tf.math.top_k(temp, k=n_detections, sorted=False)\n",
    "\n",
    "    top_indices_shape = top_indices.get_shape().as_list()\n",
    "    print(\"top_indices_shape\", top_indices_shape)\n",
    "    \n",
    "    top_indices = tf.stack([top_indices, top_indices], axis=-1)\n",
    "    top_indices_shape = top_indices.get_shape().as_list()\n",
    "    print(\"top_indices_shape\", top_indices_shape)\n",
    "    \n",
    "    filtered_y_pred = tf.gather_nd(assoc_heads, top_indices, batch_dims=0)\n",
    "    print(\"gather filtered_y_pred shape\", filtered_y_pred.shape)\n",
    "    y_pred = filtered_y_pred\n",
    "    \n",
    "#     for i in range(top_indices_shape[0]):\n",
    "#         for j in range(top_indices_shape[1]):\n",
    "#             filtered_y_pred[i, j, :] = K.eval(assoc_heads[i, top_indices[i, j], :])\n",
    "\n",
    "#     y_pred = tf.convert_to_tensor(filtered_y_pred, dtype='float32')\n",
    "    print(\"filtered_y_pred shape\", y_pred.shape)\n",
    "    print(\"y_true shape\", y_true.shape)\n",
    "     \n",
    "    rank = K.ndim(y_pred)\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else rank - 1\n",
    "    axes = [x for x in list(range(rank)) if x != channel_axis]\n",
    "\n",
    "    # Compute variance loss\n",
    "    cells_summed = tf.tensordot(y_true, y_pred, axes=[axes, axes])\n",
    "    n_pixels = K.cast(tf.count_nonzero(y_true, axis=axes), dtype=K.floatx()) + K.epsilon()\n",
    "    n_pixels_expand = K.expand_dims(n_pixels, axis=1) + K.epsilon()\n",
    "    mu = tf.divide(cells_summed, n_pixels_expand)\n",
    "\n",
    "    delta_v = K.constant(delta_v, dtype=K.floatx())\n",
    "    mu_tensor = tf.tensordot(y_true, mu, axes=[[channel_axis], [0]])\n",
    "    L_var_1 = y_pred - mu_tensor\n",
    "    L_var_2 = K.square(K.relu(temp_norm(L_var_1) - delta_v))\n",
    "    L_var_3 = tf.tensordot(L_var_2, y_true, axes=[axes, axes])\n",
    "    L_var_4 = tf.divide(L_var_3, n_pixels)\n",
    "    L_var = K.mean(L_var_4)\n",
    "\n",
    "    # Compute distance loss\n",
    "    mu_a = K.expand_dims(mu, axis=0)\n",
    "    mu_b = K.expand_dims(mu, axis=1)\n",
    "\n",
    "    diff_matrix = tf.subtract(mu_b, mu_a)\n",
    "    L_dist_1 = temp_norm(diff_matrix)\n",
    "    L_dist_2 = K.square(K.relu(K.constant(2 * delta_d, dtype=K.floatx()) - L_dist_1))\n",
    "    diag = K.constant(0, dtype=K.floatx()) * tf.diag_part(L_dist_2)\n",
    "    L_dist_3 = tf.matrix_set_diag(L_dist_2, diag)\n",
    "    L_dist = K.mean(L_dist_3)\n",
    "\n",
    "    # Compute regularization loss\n",
    "    L_reg = gamma * temp_norm(mu)\n",
    "    L = L_var + L_dist + K.mean(L_reg)\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prediction = model.predict_generator(\n",
    "    train_data,\n",
    "    steps=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "final_detection_pred = tf.convert_to_tensor(prediction[3])\n",
    "final_detection_scores = final_detection_pred[..., 4:5]\n",
    "tf.shape(final_detection_scores)\n",
    "print(tf.keras.backend.eval(tf.math.top_k(final_detection_scores[0,...,-1], k=9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_y_pred_shape (3, 100, 11)\n",
      "new_y_true_shape (3, 10, 40)\n",
      "n_detections Tensor(\"strided_slice_914:0\", shape=(), dtype=int32)\n",
      "boxes shape (3, 100, 4)\n",
      "annotations shape (3, 10, 4)\n",
      "assoc_heads shape (3, 100, 2)\n",
      "zeros filtered_y_pred shape (3, 10, 2)\n",
      "top_indices_shape [3, 10]\n",
      "top_indices_shape [3, 10, 2]\n",
      "gather filtered_y_pred shape (3, 10, 2)\n",
      "filtered_y_pred shape (3, 10, 2)\n",
      "y_true shape (3, 10, 40)\n",
      "7509474300.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = tf.convert_to_tensor(prediction[4])\n",
    "y_true = tf.convert_to_tensor(next_data_y[-1])\n",
    "final_detection_pred = tf.convert_to_tensor(prediction[3])\n",
    "result = discriminative_instance_loss(y_true, y_pred)\n",
    "print(tf.keras.backend.eval(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Outputs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "regression: (1, 3, 83349, 4)\n",
    "classification: (1, 3, 83349, 1)\n",
    "masks: (1, 3, 100, 788)\n",
    "final_detection: (1, 3, 100, 5)\n",
    "association_features: (1, 3, 100, 6)\n",
    "filtered_detections: (1, 3, 100, 4)\n",
    "filtered_detections: (1, 3, 100)\n",
    "filtered_detections: (1, 3, 100)\n",
    "mask_submodel: (1, 3, 100, 28, 28, 1)\n",
    "time_distributed: (1, 3, 100, 1)\n",
    "assoc_head_submodel: (1, 3, 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Outputs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "regressions: (1, 3, 83349, 5)\n",
    "labels: (1, 3, 83349, 2)\n",
    "masks: (1, 3, 9, 28035)\n",
    "masks: (1, 3, 9, 28035) <-- 2X because include final detection layer = True\n",
    "assoc_heads_batch: (1, 3, 9, 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
