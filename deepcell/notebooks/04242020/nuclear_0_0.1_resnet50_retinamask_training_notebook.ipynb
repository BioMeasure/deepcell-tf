{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026492,
     "end_time": "2020-04-25T01:51:57.188855",
     "exception": false,
     "start_time": "2020-04-25T01:51:57.162363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# General nuclear segmentation training notebook\n",
    "\n",
    "This notebook trains a model to perform nuclear segmentation of fluorescent microscopy images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014409,
     "end_time": "2020-04-25T01:51:57.218758",
     "exception": false,
     "start_time": "2020-04-25T01:51:57.204349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part 1: Import relevant python packages\n",
    "\n",
    "This section imports python packages that are used throughout the notebook and defines parameters that can be used with papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 3.460284,
     "end_time": "2020-04-25T01:52:00.693378",
     "exception": false,
     "start_time": "2020-04-25T01:51:57.233094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.029527,
     "end_time": "2020-04-25T01:52:00.744324",
     "exception": false,
     "start_time": "2020-04-25T01:52:00.714797",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Define parameters that can be set with papermill\n",
    "backbone = 'resnet50'\n",
    "dataset_name='general_nuclear'\n",
    "n_epochs = 8\n",
    "date = '04242020'\n",
    "dataset_fraction = .01\n",
    "train_test_seed = 0\n",
    "train_permutation_seed = 0\n",
    "train_val_seed=314\n",
    "model_type = 'watershed'\n",
    "batch_size = 4 if model_type == 'retinamask' else 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.026929,
     "end_time": "2020-04-25T01:52:00.789652",
     "exception": false,
     "start_time": "2020-04-25T01:52:00.762723",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_epochs = 16\n",
    "batch_size = 4\n",
    "date = \"04242020\"\n",
    "filename = \"nuclear_0_0.1_resnet50_retinamask\"\n",
    "train_permutation_seed = 0\n",
    "dataset_fraction = 0.1\n",
    "backbone = \"resnet50\"\n",
    "model_type = \"retinamask\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01575,
     "end_time": "2020-04-25T01:52:00.824249",
     "exception": false,
     "start_time": "2020-04-25T01:52:00.808499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part 2: Load training data and create data generators\n",
    "\n",
    "Data generators augment data and feed it into the model training process. Here, we define the generators that will be used to train our segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 622.230957,
     "end_time": "2020-04-25T02:02:23.071246",
     "exception": false,
     "start_time": "2020-04-25T01:52:00.840289",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 40, 216, 256, 1) (180, 40, 216, 256, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(259, 30, 135, 160, 1) (259, 30, 135, 160, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 30, 154, 182, 1) (240, 30, 154, 182, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 30, 202, 240, 1) (124, 30, 202, 240, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (10, 40, 216, 256, 1) to (40, 40, 128, 128, 1)\n",
      "Reshaped training data from (10, 40, 216, 256, 1) to (40, 40, 128, 128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (13, 30, 135, 160, 1) to (52, 30, 128, 128, 1)\n",
      "Reshaped training data from (13, 30, 135, 160, 1) to (52, 30, 128, 128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (19, 30, 154, 182, 1) to (76, 30, 128, 128, 1)\n",
      "Reshaped training data from (19, 30, 154, 182, 1) to (76, 30, 128, 128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (10, 30, 202, 240, 1) to (40, 30, 128, 128, 1)\n",
      "Reshaped training data from (10, 30, 202, 240, 1) to (40, 30, 128, 128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (2, 40, 216, 256, 1) to (8, 40, 128, 128, 1)\n",
      "Reshaped training data from (2, 40, 216, 256, 1) to (8, 40, 128, 128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (7, 30, 135, 160, 1) to (28, 30, 128, 128, 1)\n",
      "Reshaped training data from (7, 30, 135, 160, 1) to (28, 30, 128, 128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (2, 30, 154, 182, 1) to (8, 30, 128, 128, 1)\n",
      "Reshaped training data from (2, 30, 154, 182, 1) to (8, 30, 128, 128, 1)\n",
      "Reshaped feature data from (1, 30, 202, 240, 1) to (4, 30, 128, 128, 1)\n",
      "Reshaped training data from (1, 30, 202, 240, 1) to (4, 30, 128, 128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (36, 40, 128, 128, 1) to (36, 40, 128, 128, 1)\n",
      "Reshaped training data from (36, 40, 128, 128, 1) to (36, 40, 128, 128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (52, 30, 128, 128, 1) to (52, 30, 128, 128, 1)\n",
      "Reshaped training data from (52, 30, 128, 128, 1) to (52, 30, 128, 128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (48, 30, 128, 128, 1) to (48, 30, 128, 128, 1)\n",
      "Reshaped training data from (48, 30, 128, 128, 1) to (48, 30, 128, 128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (25, 30, 202, 240, 1) to (100, 30, 128, 128, 1)\n",
      "Reshaped training data from (25, 30, 202, 240, 1) to (100, 30, 128, 128, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6640, 128, 128, 1) (6640, 128, 128, 1) (1520, 128, 128, 1) (1520, 128, 128, 1)\n",
      "Number of training tracks 719\n",
      "Number of validation tracks 231\n",
      "Number of testing tracks 1925\n",
      "Number of training cells 17248\n",
      "Number of validation cells 5345\n",
      "Number of testing cells 42731\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.data_utils import get_data, reshape_movie\n",
    "from deepcell import image_generators\n",
    "from deepcell.utils import train_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepcell_tracking.utils import load_trks\n",
    "\n",
    "# Helper function - get unique tracks\n",
    "def get_n_tracks(array):\n",
    "    # array with dims (batch, time, x, y, c)\n",
    "    n_tracks = 0\n",
    "    for b in array:\n",
    "        n_tracks += len(np.unique(b)) - 1\n",
    "    return n_tracks\n",
    "\n",
    "# Helper function - get unique cells\n",
    "def get_n_cells(array):\n",
    "    # array with dims (batch, time, x, y, c)\n",
    "    n_cells = 0\n",
    "    for b in array:\n",
    "        for t in b:\n",
    "            n_cells += len(np.unique(t)) - 1\n",
    "    return n_cells\n",
    "\n",
    "# Helper function - convert indices\n",
    "def convert_indices(indices, X_list):\n",
    "    N_batches = np.array([x.shape[0] for x in X_list])\n",
    "    N_cumsum = np.cumsum(N_batches)\n",
    "    converted_index = []\n",
    "    for i in range(len(X_list)):\n",
    "        if i==0:\n",
    "            i_index = indices[indices < N_cumsum[i]]\n",
    "            converted_index.append(i_index)\n",
    "        else:\n",
    "            i_index = indices[np.logical_and(indices < N_cumsum[i], indices >= N_cumsum[i-1])] - N_cumsum[i-1]\n",
    "            converted_index.append(i_index)    \n",
    "    return converted_index\n",
    "\n",
    "# Helper function - reshape movies\n",
    "def reshape_list_of_movies(X_list, y_list, reshape_size=128, crop=False):\n",
    "    X_reshape = []\n",
    "    y_reshape = []\n",
    "    for Xl, yl in zip(X_list, y_list):\n",
    "        Xr, yr = reshape_movie(Xl, yl, reshape_size=reshape_size)\n",
    "        Xr = Xr.reshape((-1, reshape_size, reshape_size, Xr.shape[-1]))\n",
    "        yr = yr.reshape((-1, reshape_size, reshape_size, yr.shape[-1]))\n",
    "        X_reshape.append(Xr)\n",
    "        y_reshape.append(yr)\n",
    "    X_reshape = np.concatenate(X_reshape, axis=0)\n",
    "    y_reshape = np.concatenate(y_reshape, axis=0)\n",
    "    \n",
    "    return X_reshape, y_reshape\n",
    "\n",
    "# Load datasets \n",
    "dataset_direc = '/data/training_data/'\n",
    "\n",
    "hela_filename = 'HeLa_S3.trks'\n",
    "hek_filename = 'HEK293.trks'\n",
    "nih_filename = '3T3_NIH.trks'\n",
    "raw_filename = 'RAW2647.trks'\n",
    "\n",
    "filenames = [hela_filename, hek_filename, nih_filename, raw_filename]\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for filename in filenames:\n",
    "    path = os.path.join(dataset_direc, filename)\n",
    "    training_data = load_trks(path)\n",
    "    X = training_data['X']\n",
    "    y = training_data['y']\n",
    "    \n",
    "    print(X.shape, y.shape)\n",
    "    \n",
    "    # Split into training and testing dataset\n",
    "    Xt, Xtest, yt, ytest = train_test_split(X, y, test_size=0.2, random_state=train_test_seed)\n",
    "    \n",
    "    # Crop test dataset\n",
    "    X_test = [arr[:,:,0:128,0:128,:] for arr in X_test]\n",
    "    y_test = [arr[:,:,0:128,0:128,:] for arr in y_test]\n",
    "    \n",
    "    X_train.append(Xt)\n",
    "    y_train.append(yt)\n",
    "    X_test.append(Xtest)\n",
    "    y_test.append(ytest)\n",
    "    \n",
    "# Select subset of the training data\n",
    "N_batches = sum([x.shape[0] for x in X_train])\n",
    "index = np.arange(N_batches)\n",
    "dataset_size = int(dataset_fraction * N_batches)\n",
    "permuted_index = np.random.RandomState(seed=train_permutation_seed).permutation(index)\n",
    "reduced_index = permuted_index[0:dataset_size]\n",
    "converted_index = convert_indices(reduced_index, X_train)\n",
    "\n",
    "X_reduced = [x[ci] for x, ci in zip(X_train, converted_index)]\n",
    "y_reduced = [y[ci] for y, ci in zip(y_train, converted_index)]\n",
    "\n",
    "# Split into training and validation datasets\n",
    "N_batches = sum([x.shape[0] for x in X_reduced])\n",
    "index = np.arange(N_batches)\n",
    "val_size = int(0.2 * N_batches)\n",
    "permuted_index = np.random.RandomState(seed=train_val_seed).permutation(index)\n",
    "val_index = permuted_index[0:val_size]\n",
    "train_index = permuted_index[val_size:]\n",
    "\n",
    "val_ci = convert_indices(val_index, X_reduced)\n",
    "train_ci = convert_indices(train_index, X_reduced)\n",
    "\n",
    "X_train = [x[ci] for x, ci in zip(X_reduced, train_ci)]\n",
    "y_train = [y[ci] for y, ci in zip(y_reduced, train_ci)]\n",
    "\n",
    "X_val = [x[ci] for x, ci in zip(X_reduced, val_ci)]\n",
    "y_val = [y[ci] for y, ci in zip(y_reduced, val_ci)]\n",
    "    \n",
    "# Record the number of tracks and cells\n",
    "n_tracks_train = sum([get_n_tracks(y) for y in y_train])\n",
    "n_tracks_val = sum([get_n_tracks(y) for y in y_val])\n",
    "n_tracks_test = sum([get_n_tracks(y) for y in y_test])\n",
    "\n",
    "n_cells_train = sum([get_n_cells(y) for y in y_train])\n",
    "n_cells_val = sum([get_n_cells(y) for y in y_val])\n",
    "n_cells_test = sum([get_n_cells(y) for y in y_test])\n",
    "    \n",
    "# Reshape the datasets\n",
    "X_train, y_train = reshape_list_of_movies(X_train, y_train)\n",
    "X_val, y_val = reshape_list_of_movies(X_val, y_val)\n",
    "X_test, y_test = reshape_list_of_movies(X_test, y_test)\n",
    "\n",
    "train_dict = {'X':X_train, 'y':y_train}\n",
    "val_dict = {'X':X_val, 'y':y_val}\n",
    "test_dict = {'X':X_test, 'y':y_test}\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "print('Number of training tracks {}'.format(n_tracks_train))\n",
    "print('Number of validation tracks {}'.format(n_tracks_val))\n",
    "print('Number of testing tracks {}'.format(n_tracks_test))\n",
    "\n",
    "print('Number of training cells {}'.format(n_cells_train))\n",
    "print('Number of validation cells {}'.format(n_cells_val))\n",
    "print('Number of testing cells {}'.format(n_cells_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 2.147992,
     "end_time": "2020-04-25T02:02:25.247902",
     "exception": false,
     "start_time": "2020-04-25T02:02:23.099910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0425 02:02:24.455123 139887465035584 retinanet.py:357] Removing 1744 of 6640 images with fewer than 3 objects.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0425 02:02:25.148359 139887465035584 retinanet.py:357] Removing 300 of 1520 images with fewer than 3 objects.\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.retinanet_anchor_utils import generate_anchor_params\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "# Get anchor settings for RetinaMask models\n",
    "pyramid_levels = ['P3', 'P4']\n",
    "anchor_size_dict = {'P3':16, 'P4':32}\n",
    "anchor_params = generate_anchor_params(pyramid_levels, anchor_size_dict)\n",
    "\n",
    "# Data augmentation parameters\n",
    "generator_kwargs = {}\n",
    "generator_kwargs['rotation_range'] = 180\n",
    "generator_kwargs['shear_range'] = 0\n",
    "generator_kwargs['zoom_range'] = 0.25\n",
    "generator_kwargs['horizontal_flip'] = True\n",
    "generator_kwargs['vertical_flip'] = True\n",
    "\n",
    "generator_val_kwargs = {}\n",
    "generator_val_kwargs['rotation_range'] = 0\n",
    "generator_val_kwargs['shear_range'] = 0\n",
    "generator_val_kwargs['zoom_range'] = 0\n",
    "generator_val_kwargs['horizontal_flip'] = False\n",
    "generator_val_kwargs['vertical_flip'] = False\n",
    "\n",
    "# Minimum number of objects in an image\n",
    "min_objects = 3 if model_type == 'retinamask' else 0\n",
    "\n",
    "# Random seed\n",
    "seed=808\n",
    "\n",
    "if model_type == 'watershed':\n",
    "    datagen = image_generators.SemanticDataGenerator(**generator_kwargs)\n",
    "    datagen_val = image_generators.SemanticDataGenerator(**generator_val_kwargs)\n",
    "    \n",
    "    train_data = datagen.flow(\n",
    "        train_dict,\n",
    "        seed=seed,\n",
    "        transforms=['centroid', 'watershed-cont', 'fgbg'],\n",
    "        transforms_kwargs={'watershed-cont': {'erosion_width': 0}, 'centroid': {'alpha':'auto', 'beta':0.5}},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    val_data = datagen_val.flow(\n",
    "        val_dict,\n",
    "        seed=seed,\n",
    "        transforms=['centroid', 'watershed-cont', 'fgbg'],\n",
    "        transforms_kwargs={'watershed-cont': {'erosion_width': 0}, 'centroid': {'alpha':'auto', 'beta':0.5}},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "elif model_type == 'pixelwise':\n",
    "    datagen = image_generators.SemanticDataGenerator(**generator_kwargs)\n",
    "    datagen_val = image_generators.SemanticDataGenerator(**generator_val_kwargs)\n",
    "    \n",
    "    train_data = datagen.flow(\n",
    "        train_dict,\n",
    "        seed=seed,\n",
    "        transforms=['pixelwise'],\n",
    "        transforms_kwargs={},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    val_data = datagen_val.flow(\n",
    "        val_dict,\n",
    "        seed=seed,\n",
    "        transforms=['pixelwise'],\n",
    "        transforms_kwargs={},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "elif model_type == 'retinamask':\n",
    "    datagen = image_generators.RetinaNetGenerator(**generator_kwargs)\n",
    "    datagen_val = image_generators.RetinaNetGenerator(**generator_val_kwargs)\n",
    "    \n",
    "    train_data = datagen.flow(\n",
    "        train_dict=train_dict,\n",
    "        seed=seed,\n",
    "        transforms=[],\n",
    "        transforms_kwargs={},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size,\n",
    "        anchor_params=anchor_params,\n",
    "        pyramid_levels=pyramid_levels,\n",
    "        include_masks=True)\n",
    "    \n",
    "    val_data = datagen_val.flow(\n",
    "        train_dict=val_dict,\n",
    "        seed=seed,\n",
    "        transforms=[],\n",
    "        transforms_kwargs={},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size,\n",
    "        anchor_params=anchor_params,\n",
    "        pyramid_levels=pyramid_levels,\n",
    "        include_masks=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02444,
     "end_time": "2020-04-25T02:02:25.299705",
     "exception": false,
     "start_time": "2020-04-25T02:02:25.275265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part 3: Define model\n",
    "\n",
    "Here we define a PanopticNet to perform the image segmentation. This model will predict the inner distance and outer distance transform (as done in ), as well as the foreground-background transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 41.37917,
     "end_time": "2020-04-25T02:03:06.703727",
     "exception": false,
     "start_time": "2020-04-25T02:02:25.324557",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0425 02:02:25.384077 139887465035584 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "    8192/94765736 [..............................] - ETA: 12:20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "   40960/94765736 [..............................] - ETA: 4:57 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  106496/94765736 [..............................] - ETA: 2:51"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  245760/94765736 [..............................] - ETA: 1:39"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  491520/94765736 [..............................] - ETA: 1:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  999424/94765736 [..............................] - ETA: 36s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 2007040/94765736 [..............................] - ETA: 20s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 3760128/94765736 [>.............................] - ETA: 13s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 5873664/94765736 [>.............................] - ETA: 9s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 7938048/94765736 [=>............................] - ETA: 7s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "10608640/94765736 [==>...........................] - ETA: 5s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "13295616/94765736 [===>..........................] - ETA: 4s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "15998976/94765736 [====>.........................] - ETA: 4s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "18702336/94765736 [====>.........................] - ETA: 3s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "21438464/94765736 [=====>........................] - ETA: 3s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "24190976/94765736 [======>.......................] - ETA: 3s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "26959872/94765736 [=======>......................] - ETA: 2s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "29745152/94765736 [========>.....................] - ETA: 2s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "32546816/94765736 [=========>....................] - ETA: 2s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "35102720/94765736 [==========>...................] - ETA: 2s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "36970496/94765736 [==========>...................] - ETA: 2s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "39133184/94765736 [===========>..................] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "41230336/94765736 [============>.................] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "42508288/94765736 [============>.................] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "45637632/94765736 [=============>................] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "47652864/94765736 [==============>...............] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "49668096/94765736 [==============>...............] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "51699712/94765736 [===============>..............] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "53747712/94765736 [================>.............] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "55795712/94765736 [================>.............] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "57843712/94765736 [=================>............] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "59924480/94765736 [=================>............] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "60661760/94765736 [==================>...........] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "63496192/94765736 [===================>..........] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "64610304/94765736 [===================>..........] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "65478656/94765736 [===================>..........] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "66707456/94765736 [====================>.........] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "68182016/94765736 [====================>.........] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "69279744/94765736 [====================>.........] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "70770688/94765736 [=====================>........] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "72261632/94765736 [=====================>........] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "73752576/94765736 [======================>.......] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "75259904/94765736 [======================>.......] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "76767232/94765736 [=======================>......] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "78274560/94765736 [=======================>......] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "79798272/94765736 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "81338368/94765736 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "82862080/94765736 [=========================>....] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "84402176/94765736 [=========================>....] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "85958656/94765736 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "87515136/94765736 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "89071616/94765736 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "90628096/94765736 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "92200960/94765736 [============================>.] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "93773824/94765736 [============================>.] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "94773248/94765736 [==============================] - 4s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0425 02:02:59.005476 139887465035584 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0425 02:03:04.688713 139887465035584 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py:255: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0425 02:03:05.729822 139887465035584 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py:255: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0425 02:03:06.044609 139887465035584 training_utils.py:1101] Output filtered_detections missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to filtered_detections.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0425 02:03:06.045751 139887465035584 training_utils.py:1101] Output filtered_detections_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to filtered_detections_1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0425 02:03:06.046568 139887465035584 training_utils.py:1101] Output filtered_detections_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to filtered_detections_2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0425 02:03:06.047352 139887465035584 training_utils.py:1101] Output mask_submodel missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to mask_submodel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50_retinanet_mask\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_normalization2d (ImageNor (None, 128, 128, 1)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tensor_product (TensorProduct)  (None, 128, 128, 3)  6           image_normalization2d[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           tensor_product[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 64, 64, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 64, 64, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 64, 64, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 32, 32, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 32, 32, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 32, 32, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 32, 32, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 32, 32, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 32, 32, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 32, 32, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 32, 32, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 32, 32, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 32, 32, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 32, 32, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 32, 32, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 32, 32, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 32, 32, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 16, 16, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 16, 16, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 16, 16, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 16, 16, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 16, 16, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 16, 16, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 16, 16, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 16, 16, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 16, 16, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 16, 16, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 16, 16, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 16, 16, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 16, 16, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 16, 16, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 16, 16, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 16, 16, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 16, 16, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 16, 16, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 8, 8, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 8, 8, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 8, 8, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 8, 8, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 8, 8, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 8, 8, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 8, 8, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 8, 8, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 8, 8, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 8, 8, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 8, 8, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 8, 8, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 8, 8, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 8, 8, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 8, 8, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 8, 8, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 8, 8, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 8, 8, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 8, 8, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 8, 8, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 8, 8, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 8, 8, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 8, 8, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 8, 8, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "C5_reduced (Conv2D)             (None, 4, 4, 256)    524544      conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "C4_reduced (Conv2D)             (None, 8, 8, 256)    262400      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "P5_upsampled (UpsampleLike)     (None, None, None, 2 0           C5_reduced[0][0]                 \n",
      "                                                                 conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "P4_merged (Add)                 (None, 8, 8, 256)    0           C4_reduced[0][0]                 \n",
      "                                                                 P5_upsampled[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "C3_reduced (Conv2D)             (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "P4_upsampled (UpsampleLike)     (None, None, None, 2 0           P4_merged[0][0]                  \n",
      "                                                                 conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "P3_merged (Add)                 (None, 16, 16, 256)  0           C3_reduced[0][0]                 \n",
      "                                                                 P4_upsampled[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "P3 (Conv2D)                     (None, 16, 16, 256)  590080      P3_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "P4 (Conv2D)                     (None, 8, 8, 256)    590080      P4_merged[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "regression_submodel (Model)     (None, None, 4)      2443300     P3[0][0]                         \n",
      "                                                                 P4[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "anchors_0 (Anchors)             (None, None, 4)      36          P3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "anchors_1 (Anchors)             (None, None, 4)      36          P4[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "regression (Concatenate)        (None, None, 4)      0           regression_submodel[1][0]        \n",
      "                                                                 regression_submodel[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "anchors (Concatenate)           (None, None, 4)      0           anchors_0[0][0]                  \n",
      "                                                                 anchors_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "classification_submodel (Model) (None, None, 1)      2381065     P3[0][0]                         \n",
      "                                                                 P4[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "boxes (RegressBoxes)            (None, None, 4)      0           anchors[0][0]                    \n",
      "                                                                 regression[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "classification (Concatenate)    (None, None, 1)      0           classification_submodel[1][0]    \n",
      "                                                                 classification_submodel[2][0]    \n",
      "__________________________________________________________________________________________________\n",
      "clipped_boxes (ClipBoxes)       (None, None, 4)      0           input_1[0][0]                    \n",
      "                                                                 boxes[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "filtered_detections (FilterDete [(None, 100, 4), (No 0           clipped_boxes[0][0]              \n",
      "                                                                 classification[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "upsample_like (UpsampleLike)    (None, None, None, 2 0           P3[0][0]                         \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "roi_align (RoiAlign)            (None, 100, 14, 14,  0           filtered_detections[0][0]        \n",
      "                                                                 upsample_like[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask_submodel (Model)           multiple             2950657     roi_align[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masks (ConcatenateBoxes)        (None, 100, None)    0           filtered_detections[0][0]        \n",
      "                                                                 mask_submodel[1][0]              \n",
      "==================================================================================================\n",
      "Total params: 33,461,244\n",
      "Trainable params: 33,408,124\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from deepcell.model_zoo.panopticnet import PanopticNet\n",
    "from deepcell.model_zoo.retinamask import RetinaMask\n",
    "from deepcell import losses\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.losses import MSE\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = Adam(lr=1e-4, clipnorm=0.001)\n",
    "    \n",
    "if model_type == 'watershed':\n",
    "    model = PanopticNet(backbone,\n",
    "                       input_shape=train_data.x.shape[1:],\n",
    "                       norm_method='whole_image',\n",
    "                       num_semantic_heads=3,\n",
    "                       num_semantic_classes=[1,1,2],\n",
    "                       location=True,\n",
    "                       include_top=True,\n",
    "                       interpolation='bilinear',\n",
    "                       lite=True)\n",
    "\n",
    "    # Define loss\n",
    "    loss_layers = ['semantic_0', 'semantic_1', 'semantic_2']\n",
    "    loss = {}\n",
    "\n",
    "    for layer_name in loss_layers:\n",
    "        n_classes = model.get_layer(layer_name).output_shape[-1]\n",
    "        if n_classes > 1:\n",
    "            def loss_function(y_true, y_pred):\n",
    "                return 0.01 * losses.weighted_categorical_crossentropy(\n",
    "                    y_true, y_pred, n_classes=n_classes)\n",
    "            loss[layer_name] = loss_function\n",
    "        elif n_classes == 1:\n",
    "            loss[layer_name] = MSE\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "elif model_type == 'pixelwise':\n",
    "    model = PanopticNet(backbone,\n",
    "                       input_shape=train_data.x.shape[1:],\n",
    "                       norm_method='whole_image',\n",
    "                       num_semantic_heads=1,\n",
    "                       num_semantic_classes=[3],\n",
    "                       location=False,\n",
    "                       include_top=True)\n",
    "\n",
    "    # Define loss\n",
    "    loss = {}\n",
    "    \n",
    "    def loss_function(y_true, y_pred):\n",
    "        return losses.weighted_categorical_crossentropy(\n",
    "                    y_true, y_pred, n_classes=3)\n",
    "    \n",
    "    loss['semantic_0'] = loss_function\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "elif model_type == 'retinamask':\n",
    "    model = RetinaMask(backbone,\n",
    "                      num_classes=1,\n",
    "                      input_shape=train_data.x.shape[1:],\n",
    "                      norm_method='whole_image',\n",
    "                      use_imagenet=True,\n",
    "                      pyramid_levels=pyramid_levels,\n",
    "                      anchor_params=anchor_params)\n",
    "    \n",
    "    # Define loss\n",
    "    retinanet_losses = losses.RetinaNetLosses()\n",
    "    loss = {\n",
    "        'regression': retinanet_losses.regress_loss,\n",
    "        'classification': retinanet_losses.classification_loss,\n",
    "        'masks': retinanet_losses.mask_loss\n",
    "    }\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057974,
     "end_time": "2020-04-25T02:03:06.820715",
     "exception": false,
     "start_time": "2020-04-25T02:03:06.762741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part 4: Define model training parameters\n",
    "\n",
    "Here, we define all of the parameters needed for training. The model trainer objects will record these metadata after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.063542,
     "end_time": "2020-04-25T02:03:06.933231",
     "exception": false,
     "start_time": "2020-04-25T02:03:06.869689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'nuclear_{}_{}_{}_{}'.format(str(train_permutation_seed), dataset_fraction, backbone, model_type)\n",
    "model_path = os.path.join('/data', 'models', date, model_name)\n",
    "dataset_metadata={'name': dataset_name,\n",
    "                  'other': 'Pooled nuclear data from HEK293, HeLa-S3, NIH-3T3, and RAW264.7 cells.'}\n",
    "training_kwargs = {}\n",
    "training_kwargs['batch_size'] = batch_size\n",
    "training_kwargs['lr'] = 1e-5 if model_type=='retinamask' else 1e-4\n",
    "training_kwargs['lr_decay'] = 0.95\n",
    "training_kwargs['training_seed'] = 0\n",
    "training_kwargs['n_epochs'] = n_epochs\n",
    "training_kwargs['training_steps_per_epoch'] = 82800//16 if model_type == 'retinamask' else 82800 // training_kwargs['batch_size']\n",
    "training_kwargs['validation_steps_per_epoch'] = val_data.y.shape[0] // training_kwargs['batch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050079,
     "end_time": "2020-04-25T02:03:07.036254",
     "exception": false,
     "start_time": "2020-04-25T02:03:06.986175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part 5: Create the model trainer and train the model\n",
    "\n",
    "Here, we create the model trainer, train the model, and export the model - along with the metadata and benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.338175,
     "end_time": "2020-04-25T02:03:07.423229",
     "exception": false,
     "start_time": "2020-04-25T02:03:07.085054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "from skimage.morphology import remove_small_objects\n",
    "\n",
    "from deepcell.utils.export_utils import export_model\n",
    "from deepcell.utils.train_utils import rate_scheduler, get_callbacks\n",
    "from deepcell.metrics import Metrics\n",
    "\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "\n",
    "class ModelTrainer(object):\n",
    "    def __init__(self, \n",
    "                model,\n",
    "                model_name,\n",
    "                model_path,\n",
    "                train_generator,\n",
    "                validation_generator,\n",
    "                benchmarking_data=None,\n",
    "                log_dir=None,\n",
    "                tfserving_path=None,\n",
    "                training_callbacks='default',\n",
    "                max_batch_size=256,\n",
    "                export_precisions = ['fp16'],\n",
    "                postprocessing_fn=None,\n",
    "                postprocessing_kwargs={},\n",
    "                predict_batch_size=4,\n",
    "                model_version=0,\n",
    "                min_size=100,\n",
    "                dataset_metadata={},\n",
    "                training_kwargs={}):\n",
    "    \n",
    "        \"\"\"\n",
    "        Model trainer class for segmentation models. This class eases model development by\n",
    "        linking relevant metadata (dataset, training parameters, benchmarking) to the model\n",
    "        training process.\n",
    "        \"\"\"\n",
    "\n",
    "        # Add model information\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.model_path = model_path\n",
    "        self.model_version = model_version\n",
    "\n",
    "        # Add dataset information\n",
    "        self.train_generator = train_generator\n",
    "        self.validation_generator = validation_generator\n",
    "        self.benchmarking_data = benchmarking_data\n",
    "        self.dataset_metadata = dataset_metadata\n",
    "        self.postprocessing_fn = postprocessing_fn\n",
    "        self.postprocessing_kwargs = postprocessing_kwargs\n",
    "        self.predict_batch_size = predict_batch_size\n",
    "\n",
    "        # Add benchmarking information\n",
    "        self.min_size = min_size\n",
    "\n",
    "        # Add export information\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.export_precisions = export_precisions\n",
    "\n",
    "        # Add directories for logging and model export\n",
    "        if log_dir is None:\n",
    "            self.log_dir = os.path.join(model_path, 'logging')\n",
    "        else:\n",
    "            self.log_dir = log_dir\n",
    "        \n",
    "        if tfserving_path is None:\n",
    "            self.tfserving_path = os.path.join(model_path, 'serving')\n",
    "        else:\n",
    "            self.tfserving_path = tfserving_path\n",
    "\n",
    "        # Add training kwargs\n",
    "        self.batch_size = training_kwargs.pop('batch_size', 1)\n",
    "        self.training_steps_per_epoch = training_kwargs.pop('training_steps_per_epoch', \n",
    "                                                self.train_generator.y.shape[0] // self.batch_size)\n",
    "        self.validation_steps_per_epoch = training_kwargs.pop('validation_steps_per_epoch', \n",
    "                                                self.validation_generator.y.shape[0] // self.batch_size)\n",
    "        self.n_epochs = training_kwargs.pop('n_epochs', 8)\n",
    "        self.lr = training_kwargs.pop('lr', 1e-5)\n",
    "        self.lr_decay = training_kwargs.pop('lr_decay', 0.95)\n",
    "        self.lr_sched = training_kwargs.pop('lr_sched', rate_scheduler(lr=self.lr, decay=self.lr_decay))\n",
    "\n",
    "        # Add callbacks\n",
    "        if training_callbacks == 'default':\n",
    "            model_name = os.path.join(model_path, model_name + '.h5')\n",
    "            self.training_callbacks = get_callbacks(model_name, lr_sched=self.lr_sched,\n",
    "                                        tensorboard_log_dir=self.log_dir,\n",
    "                                        save_weights_only=False,\n",
    "                                        monitor='val_loss', verbose=1)\n",
    "        else:\n",
    "            self.training_callbacks = training_callbacks\n",
    "\n",
    "        self.trained = False\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _create_hash(self):\n",
    "        if not self.trained:\n",
    "            raise ValueError('Can only create a hash for a trained model')\n",
    "        else:\n",
    "            weights = []\n",
    "            for layer in self.model.layers:\n",
    "                weights += layer.get_weights()\n",
    "            summed_weights_list = [np.sum(w) for w in weights]\n",
    "            summed_weights = sum(summed_weights_list)\n",
    "            model_hash = hashlib.md5(str(summed_weights).encode())\n",
    "            self.model_hash = model_hash.hexdigest()\n",
    "\n",
    "    def _fit(self):\n",
    "        loss_history = self.model.fit_generator(\n",
    "        self.train_generator,\n",
    "        steps_per_epoch=self.training_steps_per_epoch,\n",
    "        epochs=self.n_epochs,\n",
    "        validation_data=self.validation_generator,\n",
    "        validation_steps=self.validation_steps_per_epoch,\n",
    "        callbacks=self.training_callbacks,\n",
    "        verbose=2)\n",
    "\n",
    "        self.trained = True\n",
    "        self.loss_history = loss_history\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _benchmark(self):\n",
    "        if not self.trained:\n",
    "            raise ValueError('Model training is not complete')\n",
    "        else:\n",
    "            if self.benchmarking_data is None:\n",
    "                x = self.validation_generator.x.copy()\n",
    "                y_true = self.validation_generator.y.copy()\n",
    "            else:\n",
    "                x = self.benchmarking_data['X']\n",
    "                y_true = self.benchmarking_data['y']\n",
    "\n",
    "            outputs = self.model.predict(x, batch_size=self.predict_batch_size)\n",
    "            y_pred = self.postprocessing_fn(outputs, **self.postprocessing_kwargs)\n",
    "\n",
    "            if len(y_pred.shape) == 3:\n",
    "                y_pred = np.expand_dims(y_pred, axis=-1)    #TODO: This is a hack because the postprocessing fn returns\n",
    "                                                            #masks with no channel dimensions. This should be fixed.\n",
    "            \n",
    "            benchmarks = Metrics(self.model_name, seg=False)\n",
    "            benchmarks.calc_object_stats(y_true, y_pred)\n",
    "\n",
    "            # Save benchmarks in dict\n",
    "            self.benchmarks = {}\n",
    "            for key in benchmarks.stats.keys():\n",
    "                self.benchmarks[key] = int(benchmarks.stats[key].sum())\n",
    "\n",
    "            for i in range(y_pred.shape[0]):\n",
    "                y_pred[i] = remove_small_objects(y_pred[i].astype(int), min_size=self.min_size)\n",
    "                y_true[i] = remove_small_objects(y_true[i].astype(int), min_size=self.min_size)\n",
    "\n",
    "            benchmarks = Metrics(self.model_name + ' - Removed objects less than {} pixels'.format(self.min_size), \n",
    "                                    seg=False)\n",
    "            benchmarks.calc_object_stats(y_true, y_pred)\n",
    "\n",
    "            # Save benchmarks in dict\n",
    "            self.benchmarks_remove_small_objects = {}\n",
    "            for key in benchmarks.stats.keys():\n",
    "                self.benchmarks_remove_small_objects[key] = int(benchmarks.stats[key].sum())\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _create_training_metadata(self):\n",
    "        training_metadata = {}\n",
    "        training_metadata['batch_size'] = self.batch_size\n",
    "        training_metadata['lr'] = self.lr\n",
    "        training_metadata['lr_decay'] = self.lr_decay\n",
    "        training_metadata['n_epochs'] = self.n_epochs\n",
    "        training_metadata['training_steps_per_epoch'] = self.training_steps_per_epoch\n",
    "        training_metadata['validation_steps_per_epoch'] = self.validation_steps_per_epoch\n",
    "\n",
    "        self.training_metadata = training_metadata\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _export_tf_serving(self):\n",
    "        export_model(self.model, self.tfserving_path, model_version=self.model_version)\n",
    "\n",
    "        # Convert model to TensorRT with float16\n",
    "        if 'fp16' in self.export_precisions:\n",
    "            export_model_dir = os.path.join(self.tfserving_path, str(self.model_version))\n",
    "            export_model_dir_fp16 = os.path.join(self.tfserving_path + '_fp16', str(self.model_version))\n",
    "\n",
    "            converter = trt.TrtGraphConverter(input_saved_model_dir=export_model_dir,\n",
    "                                            max_batch_size=self.max_batch_size,\n",
    "                                            precision_mode='fp16')\n",
    "            converter.convert()\n",
    "            converter.save(export_model_dir_fp16)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def create_model(self, export_serving=False, export_lite=False):\n",
    "\n",
    "        # Train model\n",
    "        self._fit()\n",
    "\n",
    "        # Load best performing weights\n",
    "        model_name = os.path.join(self.model_path, self.model_name + '.h5')\n",
    "        self.model.load_weights(model_name)\n",
    "\n",
    "        # Create model hash\n",
    "        self._create_hash()\n",
    "\n",
    "        # Create benchmarks\n",
    "        self._benchmark()\n",
    "\n",
    "        # Create model metadata\n",
    "        self._create_training_metadata()\n",
    "\n",
    "        # Save model\n",
    "        model_name = os.path.join(self.model_path, self.model_name + '_' + self.model_hash + '.h5')\n",
    "        self.model.save(model_name)\n",
    "\n",
    "        # Save loss history\n",
    "        loss_name = os.path.join(self.model_path, self.model_name + '_loss_' + self.model_hash + '.npz')\n",
    "        np.savez(loss_name, loss_history=self.loss_history.history)\n",
    "\n",
    "        # Save metadata (training and dataset) and benchmarks\n",
    "        metadata = {}\n",
    "        metadata['model_hash'] = self.model_hash\n",
    "        metadata['training_metadata'] = self.training_metadata\n",
    "        metadata['dataset_metadata'] = self.dataset_metadata\n",
    "        metadata['benchmarks'] = self.benchmarks\n",
    "        metadata['benchmarks_remove_small_objects'] = self.benchmarks_remove_small_objects\n",
    "\n",
    "        # TODO: Saving the benchmarking object in this way saves each individual benchmark.\n",
    "        # This should be refactored to save the sums.\n",
    "\n",
    "        metadata_name = os.path.join(self.model_path, self.model_name + '_' + self.model_hash + '.json')\n",
    "        \n",
    "        with open(metadata_name, 'w') as json_file:\n",
    "            json.dump(metadata, json_file)\n",
    "\n",
    "        # Export tf serving model\n",
    "        if export_serving:\n",
    "            self._export_tf_serving()\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2020-04-25T02:03:07.474235",
     "status": "running"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepcell_toolbox import retinamask_postprocess\n",
    "from deepcell_toolbox.deep_watershed import deep_watershed as watershed_postprocess \n",
    "from functools import partial\n",
    "from scipy import ndimage\n",
    "\n",
    "if model_type == 'watershed':\n",
    "    postprocessing_fn = watershed_postprocess\n",
    "elif model_type == 'pixelwise':\n",
    "    def pixelwise(prediction, threshold=.75):\n",
    "        \"\"\"Post-processing for pixelwise transform predictions.\n",
    "        Uses the interior predictions to uniquely label every instance.\n",
    "        Args:\n",
    "            prediction: pixelwise transform prediction\n",
    "            threshold: confidence threshold for interior predictions\n",
    "        Returns:\n",
    "            post-processed data with each cell uniquely annotated\n",
    "        \"\"\"\n",
    "        labeled = []\n",
    "        for b in range(prediction.shape[0]):\n",
    "            pred = prediction[b]\n",
    "            interior = pred[..., 1] > threshold\n",
    "            data = np.expand_dims(interior, axis=-1)\n",
    "            label_image = ndimage.label(data)[0]\n",
    "            labeled.append(label_image)\n",
    "        labeled = np.stack(labeled, axis=0)\n",
    "        return labeled\n",
    "    postprocessing_fn = pixelwise\n",
    "elif model_type == 'retinamask':\n",
    "    postprocessing_fn = partial(retinamask_postprocess, image_shape=(128,128))\n",
    "    \n",
    "model_trainer = ModelTrainer(model,\n",
    "                            model_name,\n",
    "                            model_path,\n",
    "                            train_data,\n",
    "                            val_data,\n",
    "                            benchmarking_data = test_dict,\n",
    "                            postprocessing_fn=postprocessing_fn,\n",
    "                            predict_batch_size=32,\n",
    "                            dataset_metadata=dataset_metadata,\n",
    "                            training_kwargs=training_kwargs)\n",
    "\n",
    "model_trainer.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "papermill": {
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/notebooks/papermill/Papermill - Nuclear Accuracy - Proper Split.ipynb",
   "output_path": "/notebooks/04242020/nuclear_0_0.1_resnet50_retinamask_training_notebook.ipynb",
   "parameters": {
    "backbone": "resnet50",
    "batch_size": 4,
    "dataset_fraction": 0.1,
    "date": "04242020",
    "filename": "nuclear_0_0.1_resnet50_retinamask",
    "model_type": "retinamask",
    "n_epochs": 16,
    "train_permutation_seed": 0
   },
   "start_time": "2020-04-25T01:51:56.250246",
   "version": "1.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}