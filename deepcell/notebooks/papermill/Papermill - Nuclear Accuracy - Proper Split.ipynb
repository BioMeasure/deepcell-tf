{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General nuclear segmentation training notebook\n",
    "\n",
    "This notebook trains a model to perform nuclear segmentation of fluorescent microscopy images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Import relevant python packages\n",
    "\n",
    "This section imports python packages that are used throughout the notebook and defines parameters that can be used with papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Define parameters that can be set with papermill\n",
    "backbone = 'resnet50'\n",
    "dataset_name='general_nuclear'\n",
    "n_epochs = 8\n",
    "date = '04242020'\n",
    "dataset_fraction = .01\n",
    "train_test_seed = 0\n",
    "train_permutation_seed = 0\n",
    "train_val_seed=314\n",
    "model_type = 'watershed'\n",
    "batch_size = 4 if model_type == 'retinamask' else 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load training data and create data generators\n",
    "\n",
    "Data generators augment data and feed it into the model training process. Here, we define the generators that will be used to train our segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from deepcell.utils.data_utils import get_data, reshape_movie\n",
    "from deepcell import image_generators\n",
    "from deepcell.utils import train_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepcell_tracking.utils import load_trks\n",
    "\n",
    "# Helper function - get unique tracks\n",
    "def get_n_tracks(array):\n",
    "    # array with dims (batch, time, x, y, c)\n",
    "    n_tracks = 0\n",
    "    for b in array:\n",
    "        n_tracks += len(np.unique(b)) - 1\n",
    "    return n_tracks\n",
    "\n",
    "# Helper function - get unique cells\n",
    "def get_n_cells(array):\n",
    "    # array with dims (batch, time, x, y, c)\n",
    "    n_cells = 0\n",
    "    for b in array:\n",
    "        for t in b:\n",
    "            n_cells += len(np.unique(t)) - 1\n",
    "    return n_cells\n",
    "\n",
    "# Helper function - convert indices\n",
    "def convert_indices(indices, X_list):\n",
    "    N_batches = np.array([x.shape[0] for x in X_list])\n",
    "    N_cumsum = np.cumsum(N_batches)\n",
    "    converted_index = []\n",
    "    for i in range(len(X_list)):\n",
    "        if i==0:\n",
    "            i_index = indices[indices < N_cumsum[i]]\n",
    "            converted_index.append(i_index)\n",
    "        else:\n",
    "            i_index = indices[np.logical_and(indices < N_cumsum[i], indices >= N_cumsum[i-1])] - N_cumsum[i-1]\n",
    "            converted_index.append(i_index)    \n",
    "    return converted_index\n",
    "\n",
    "# Helper function - reshape movies\n",
    "def reshape_list_of_movies(X_list, y_list, reshape_size=128, crop=False):\n",
    "    X_reshape = []\n",
    "    y_reshape = []\n",
    "    for Xl, yl in zip(X_list, y_list):\n",
    "        Xr, yr = reshape_movie(Xl, yl, reshape_size=reshape_size)\n",
    "        Xr = Xr.reshape((-1, reshape_size, reshape_size, Xr.shape[-1]))\n",
    "        yr = yr.reshape((-1, reshape_size, reshape_size, yr.shape[-1]))\n",
    "        X_reshape.append(Xr)\n",
    "        y_reshape.append(yr)\n",
    "    X_reshape = np.concatenate(X_reshape, axis=0)\n",
    "    y_reshape = np.concatenate(y_reshape, axis=0)\n",
    "    \n",
    "    return X_reshape, y_reshape\n",
    "\n",
    "# Load datasets \n",
    "dataset_direc = '/data/training_data/'\n",
    "\n",
    "hela_filename = 'HeLa_S3.trks'\n",
    "hek_filename = 'HEK293.trks'\n",
    "nih_filename = '3T3_NIH.trks'\n",
    "raw_filename = 'RAW2647.trks'\n",
    "\n",
    "filenames = [hela_filename, hek_filename, nih_filename, raw_filename]\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for filename in filenames:\n",
    "    path = os.path.join(dataset_direc, filename)\n",
    "    training_data = load_trks(path)\n",
    "    X = training_data['X']\n",
    "    y = training_data['y']\n",
    "    \n",
    "    print(X.shape, y.shape)\n",
    "    \n",
    "    # Split into training and testing dataset\n",
    "    Xt, Xtest, yt, ytest = train_test_split(X, y, test_size=0.2, random_state=train_test_seed)\n",
    "    \n",
    "    # Crop test dataset\n",
    "    X_test = [arr[:,:,0:128,0:128,:] for arr in X_test]\n",
    "    y_test = [arr[:,:,0:128,0:128,:] for arr in y_test]\n",
    "    \n",
    "    X_train.append(Xt)\n",
    "    y_train.append(yt)\n",
    "    X_test.append(Xtest)\n",
    "    y_test.append(ytest)\n",
    "    \n",
    "# Select subset of the training data\n",
    "N_batches = sum([x.shape[0] for x in X_train])\n",
    "index = np.arange(N_batches)\n",
    "dataset_size = int(dataset_fraction * N_batches)\n",
    "permuted_index = np.random.RandomState(seed=train_permutation_seed).permutation(index)\n",
    "reduced_index = permuted_index[0:dataset_size]\n",
    "converted_index = convert_indices(reduced_index, X_train)\n",
    "\n",
    "X_reduced = [x[ci] for x, ci in zip(X_train, converted_index)]\n",
    "y_reduced = [y[ci] for y, ci in zip(y_train, converted_index)]\n",
    "\n",
    "# Split into training and validation datasets\n",
    "N_batches = sum([x.shape[0] for x in X_reduced])\n",
    "index = np.arange(N_batches)\n",
    "val_size = int(0.2 * N_batches)\n",
    "permuted_index = np.random.RandomState(seed=train_val_seed).permutation(index)\n",
    "val_index = permuted_index[0:val_size]\n",
    "train_index = permuted_index[val_size:]\n",
    "\n",
    "val_ci = convert_indices(val_index, X_reduced)\n",
    "train_ci = convert_indices(train_index, X_reduced)\n",
    "\n",
    "X_train = [x[ci] for x, ci in zip(X_reduced, train_ci)]\n",
    "y_train = [y[ci] for y, ci in zip(y_reduced, train_ci)]\n",
    "\n",
    "X_val = [x[ci] for x, ci in zip(X_reduced, val_ci)]\n",
    "y_val = [y[ci] for y, ci in zip(y_reduced, val_ci)]\n",
    "    \n",
    "# Record the number of tracks and cells\n",
    "n_tracks_train = sum([get_n_tracks(y) for y in y_train])\n",
    "n_tracks_val = sum([get_n_tracks(y) for y in y_val])\n",
    "n_tracks_test = sum([get_n_tracks(y) for y in y_test])\n",
    "\n",
    "n_cells_train = sum([get_n_cells(y) for y in y_train])\n",
    "n_cells_val = sum([get_n_cells(y) for y in y_val])\n",
    "n_cells_test = sum([get_n_cells(y) for y in y_test])\n",
    "    \n",
    "# Reshape the datasets\n",
    "X_train, y_train = reshape_list_of_movies(X_train, y_train)\n",
    "X_val, y_val = reshape_list_of_movies(X_val, y_val)\n",
    "X_test, y_test = reshape_list_of_movies(X_test, y_test)\n",
    "\n",
    "train_dict = {'X':X_train, 'y':y_train}\n",
    "val_dict = {'X':X_val, 'y':y_val}\n",
    "test_dict = {'X':X_test, 'y':y_test}\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "print('Number of training tracks {}'.format(n_tracks_train))\n",
    "print('Number of validation tracks {}'.format(n_tracks_val))\n",
    "print('Number of testing tracks {}'.format(n_tracks_test))\n",
    "\n",
    "print('Number of training cells {}'.format(n_cells_train))\n",
    "print('Number of validation cells {}'.format(n_cells_val))\n",
    "print('Number of testing cells {}'.format(n_cells_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.retinanet_anchor_utils import generate_anchor_params\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "# Get anchor settings for RetinaMask models\n",
    "pyramid_levels = ['P3', 'P4']\n",
    "anchor_size_dict = {'P3':16, 'P4':32}\n",
    "anchor_params = generate_anchor_params(pyramid_levels, anchor_size_dict)\n",
    "\n",
    "# Data augmentation parameters\n",
    "generator_kwargs = {}\n",
    "generator_kwargs['rotation_range'] = 180\n",
    "generator_kwargs['shear_range'] = 0\n",
    "generator_kwargs['zoom_range'] = 0.25\n",
    "generator_kwargs['horizontal_flip'] = True\n",
    "generator_kwargs['vertical_flip'] = True\n",
    "\n",
    "generator_val_kwargs = {}\n",
    "generator_val_kwargs['rotation_range'] = 0\n",
    "generator_val_kwargs['shear_range'] = 0\n",
    "generator_val_kwargs['zoom_range'] = 0\n",
    "generator_val_kwargs['horizontal_flip'] = False\n",
    "generator_val_kwargs['vertical_flip'] = False\n",
    "\n",
    "# Minimum number of objects in an image\n",
    "min_objects = 3 if model_type == 'retinamask' else 0\n",
    "\n",
    "# Random seed\n",
    "seed=808\n",
    "\n",
    "if model_type == 'watershed':\n",
    "    datagen = image_generators.SemanticDataGenerator(**generator_kwargs)\n",
    "    datagen_val = image_generators.SemanticDataGenerator(**generator_val_kwargs)\n",
    "    \n",
    "    train_data = datagen.flow(\n",
    "        train_dict,\n",
    "        seed=seed,\n",
    "        transforms=['centroid', 'watershed-cont', 'fgbg'],\n",
    "        transforms_kwargs={'watershed-cont': {'erosion_width': 1}, 'centroid': {'alpha':'auto', 'beta':0.5}},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    val_data = datagen_val.flow(\n",
    "        val_dict,\n",
    "        seed=seed,\n",
    "        transforms=['centroid', 'watershed-cont', 'fgbg'],\n",
    "        transforms_kwargs={'watershed-cont': {'erosion_width': 1}, 'centroid': {'alpha':'auto', 'beta':0.5}},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "elif model_type == 'pixelwise':\n",
    "    datagen = image_generators.SemanticDataGenerator(**generator_kwargs)\n",
    "    datagen_val = image_generators.SemanticDataGenerator(**generator_val_kwargs)\n",
    "    \n",
    "    train_data = datagen.flow(\n",
    "        train_dict,\n",
    "        seed=seed,\n",
    "        transforms=['pixelwise'],\n",
    "        transforms_kwargs={},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    val_data = datagen_val.flow(\n",
    "        val_dict,\n",
    "        seed=seed,\n",
    "        transforms=['pixelwise'],\n",
    "        transforms_kwargs={},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "elif model_type == 'retinamask':\n",
    "    datagen = image_generators.RetinaNetGenerator(**generator_kwargs)\n",
    "    datagen_val = image_generators.RetinaNetGenerator(**generator_val_kwargs)\n",
    "    \n",
    "    train_data = datagen.flow(\n",
    "        train_dict=train_dict,\n",
    "        seed=seed,\n",
    "        transforms=[],\n",
    "        transforms_kwargs={},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size,\n",
    "        anchor_params=anchor_params,\n",
    "        pyramid_levels=pyramid_levels,\n",
    "        include_masks=True)\n",
    "    \n",
    "    val_data = datagen_val.flow(\n",
    "        train_dict=val_dict,\n",
    "        seed=seed,\n",
    "        transforms=[],\n",
    "        transforms_kwargs={},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size,\n",
    "        anchor_params=anchor_params,\n",
    "        pyramid_levels=pyramid_levels,\n",
    "        include_masks=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define model\n",
    "\n",
    "Here we define a PanopticNet to perform the image segmentation. This model will predict the inner distance and outer distance transform (as done in ), as well as the foreground-background transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deepcell.model_zoo.panopticnet import PanopticNet\n",
    "from deepcell.model_zoo.retinamask import RetinaMask\n",
    "from deepcell import losses\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.losses import MSE\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = Adam(lr=1e-4, clipnorm=0.001)\n",
    "    \n",
    "if model_type == 'watershed':\n",
    "    model = PanopticNet(backbone,\n",
    "                       input_shape=train_data.x.shape[1:],\n",
    "                       norm_method='whole_image',\n",
    "                       num_semantic_heads=3,\n",
    "                       num_semantic_classes=[1,1,2],\n",
    "                       location=False,\n",
    "                       include_top=True,\n",
    "                       interpolation='bilinear',\n",
    "                       lite=True)\n",
    "\n",
    "    # Define loss\n",
    "    loss_layers = ['semantic_0', 'semantic_1', 'semantic_2']\n",
    "    loss = {}\n",
    "\n",
    "    for layer_name in loss_layers:\n",
    "        n_classes = model.get_layer(layer_name).output_shape[-1]\n",
    "        if n_classes > 1:\n",
    "            def loss_function(y_true, y_pred):\n",
    "                return 0.01 * losses.weighted_categorical_crossentropy(\n",
    "                    y_true, y_pred, n_classes=n_classes)\n",
    "            loss[layer_name] = loss_function\n",
    "        elif n_classes == 1:\n",
    "            loss[layer_name] = MSE\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "elif model_type == 'pixelwise':\n",
    "    model = PanopticNet(backbone,\n",
    "                       input_shape=train_data.x.shape[1:],\n",
    "                       norm_method='whole_image',\n",
    "                       num_semantic_heads=1,\n",
    "                       num_semantic_classes=[3],\n",
    "                       location=False,\n",
    "                       include_top=True)\n",
    "\n",
    "    # Define loss\n",
    "    loss = {}\n",
    "    \n",
    "    def loss_function(y_true, y_pred):\n",
    "        return losses.weighted_categorical_crossentropy(\n",
    "                    y_true, y_pred, n_classes=3)\n",
    "    \n",
    "    loss['semantic_0'] = loss_function\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "elif model_type == 'retinamask':\n",
    "    model = RetinaMask(backbone,\n",
    "                      num_classes=1,\n",
    "                      input_shape=train_data.x.shape[1:],\n",
    "                      norm_method='whole_image',\n",
    "                      use_imagenet=False,\n",
    "                      pyramid_levels=pyramid_levels,\n",
    "                      anchor_params=anchor_params)\n",
    "    \n",
    "    # Define loss\n",
    "    retinanet_losses = losses.RetinaNetLosses()\n",
    "    loss = {\n",
    "        'regression': retinanet_losses.regress_loss,\n",
    "        'classification': retinanet_losses.classification_loss,\n",
    "        'masks': retinanet_losses.mask_loss\n",
    "    }\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Define model training parameters\n",
    "\n",
    "Here, we define all of the parameters needed for training. The model trainer objects will record these metadata after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nuclear_{}_{}_{}_{}'.format(str(train_permutation_seed), dataset_fraction, backbone, model_type)\n",
    "model_path = os.path.join('/data', 'models', date, model_name)\n",
    "dataset_metadata={'name': dataset_name,\n",
    "                  'other': 'Pooled nuclear data from HEK293, HeLa-S3, NIH-3T3, and RAW264.7 cells.'}\n",
    "training_kwargs = {}\n",
    "training_kwargs['batch_size'] = batch_size\n",
    "training_kwargs['lr'] = 1e-5 if model_type=='retinamask' else 1e-4\n",
    "training_kwargs['lr_decay'] = 0.95\n",
    "training_kwargs['training_seed'] = 0\n",
    "training_kwargs['n_epochs'] = n_epochs\n",
    "training_kwargs['training_steps_per_epoch'] = 82800//16 if model_type == 'retinamask' else 82800 // training_kwargs['batch_size']\n",
    "training_kwargs['validation_steps_per_epoch'] = val_data.y.shape[0] // training_kwargs['batch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Create the model trainer and train the model\n",
    "\n",
    "Here, we create the model trainer, train the model, and export the model - along with the metadata and benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "from skimage.morphology import remove_small_objects\n",
    "\n",
    "from deepcell.utils.export_utils import export_model\n",
    "from deepcell.utils.train_utils import rate_scheduler, get_callbacks\n",
    "from deepcell.metrics import Metrics\n",
    "\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "\n",
    "class ModelTrainer(object):\n",
    "    def __init__(self, \n",
    "                model,\n",
    "                model_name,\n",
    "                model_path,\n",
    "                train_generator,\n",
    "                validation_generator,\n",
    "                benchmarking_data=None,\n",
    "                log_dir=None,\n",
    "                tfserving_path=None,\n",
    "                training_callbacks='default',\n",
    "                max_batch_size=256,\n",
    "                export_precisions = ['fp16'],\n",
    "                postprocessing_fn=None,\n",
    "                postprocessing_kwargs={},\n",
    "                predict_batch_size=4,\n",
    "                model_version=0,\n",
    "                min_size=100,\n",
    "                dataset_metadata={},\n",
    "                training_kwargs={}):\n",
    "    \n",
    "        \"\"\"\n",
    "        Model trainer class for segmentation models. This class eases model development by\n",
    "        linking relevant metadata (dataset, training parameters, benchmarking) to the model\n",
    "        training process.\n",
    "        \"\"\"\n",
    "\n",
    "        # Add model information\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.model_path = model_path\n",
    "        self.model_version = model_version\n",
    "\n",
    "        # Add dataset information\n",
    "        self.train_generator = train_generator\n",
    "        self.validation_generator = validation_generator\n",
    "        self.benchmarking_data = benchmarking_data\n",
    "        self.dataset_metadata = dataset_metadata\n",
    "        self.postprocessing_fn = postprocessing_fn\n",
    "        self.postprocessing_kwargs = postprocessing_kwargs\n",
    "        self.predict_batch_size = predict_batch_size\n",
    "\n",
    "        # Add benchmarking information\n",
    "        self.min_size = min_size\n",
    "\n",
    "        # Add export information\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.export_precisions = export_precisions\n",
    "\n",
    "        # Add directories for logging and model export\n",
    "        if log_dir is None:\n",
    "            self.log_dir = os.path.join(model_path, 'logging')\n",
    "        else:\n",
    "            self.log_dir = log_dir\n",
    "        \n",
    "        if tfserving_path is None:\n",
    "            self.tfserving_path = os.path.join(model_path, 'serving')\n",
    "        else:\n",
    "            self.tfserving_path = tfserving_path\n",
    "\n",
    "        # Add training kwargs\n",
    "        self.batch_size = training_kwargs.pop('batch_size', 1)\n",
    "        self.training_steps_per_epoch = training_kwargs.pop('training_steps_per_epoch', \n",
    "                                                self.train_generator.y.shape[0] // self.batch_size)\n",
    "        self.validation_steps_per_epoch = training_kwargs.pop('validation_steps_per_epoch', \n",
    "                                                self.validation_generator.y.shape[0] // self.batch_size)\n",
    "        self.n_epochs = training_kwargs.pop('n_epochs', 8)\n",
    "        self.lr = training_kwargs.pop('lr', 1e-5)\n",
    "        self.lr_decay = training_kwargs.pop('lr_decay', 0.95)\n",
    "        self.lr_sched = training_kwargs.pop('lr_sched', rate_scheduler(lr=self.lr, decay=self.lr_decay))\n",
    "\n",
    "        # Add callbacks\n",
    "        if training_callbacks == 'default':\n",
    "            model_name = os.path.join(model_path, model_name + '.h5')\n",
    "            self.training_callbacks = get_callbacks(model_name, lr_sched=self.lr_sched,\n",
    "                                        tensorboard_log_dir=self.log_dir,\n",
    "                                        save_weights_only=False,\n",
    "                                        monitor='val_loss', verbose=1)\n",
    "        else:\n",
    "            self.training_callbacks = training_callbacks\n",
    "\n",
    "        self.trained = False\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _create_hash(self):\n",
    "        if not self.trained:\n",
    "            raise ValueError('Can only create a hash for a trained model')\n",
    "        else:\n",
    "            weights = []\n",
    "            for layer in self.model.layers:\n",
    "                weights += layer.get_weights()\n",
    "            summed_weights_list = [np.sum(w) for w in weights]\n",
    "            summed_weights = sum(summed_weights_list)\n",
    "            model_hash = hashlib.md5(str(summed_weights).encode())\n",
    "            self.model_hash = model_hash.hexdigest()\n",
    "\n",
    "    def _fit(self):\n",
    "        loss_history = self.model.fit_generator(\n",
    "        self.train_generator,\n",
    "        steps_per_epoch=self.training_steps_per_epoch,\n",
    "        epochs=self.n_epochs,\n",
    "        validation_data=self.validation_generator,\n",
    "        validation_steps=self.validation_steps_per_epoch,\n",
    "        callbacks=self.training_callbacks,\n",
    "        verbose=2)\n",
    "\n",
    "        self.trained = True\n",
    "        self.loss_history = loss_history\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _benchmark(self):\n",
    "        if not self.trained:\n",
    "            raise ValueError('Model training is not complete')\n",
    "        else:\n",
    "            if self.benchmarking_data is None:\n",
    "                x = self.validation_generator.x.copy()\n",
    "                y_true = self.validation_generator.y.copy()\n",
    "            else:\n",
    "                x = self.benchmarking_data['X']\n",
    "                y_true = self.benchmarking_data['y']\n",
    "\n",
    "            outputs = self.model.predict(x, batch_size=self.predict_batch_size)\n",
    "            y_pred = self.postprocessing_fn(outputs, **self.postprocessing_kwargs)\n",
    "\n",
    "            if len(y_pred.shape) == 3:\n",
    "                y_pred = np.expand_dims(y_pred, axis=-1)    #TODO: This is a hack because the postprocessing fn returns\n",
    "                                                            #masks with no channel dimensions. This should be fixed.\n",
    "            \n",
    "            benchmarks = Metrics(self.model_name, seg=False)\n",
    "            benchmarks.calc_object_stats(y_true, y_pred)\n",
    "\n",
    "            # Save benchmarks in dict\n",
    "            self.benchmarks = {}\n",
    "            for key in benchmarks.stats.keys():\n",
    "                self.benchmarks[key] = int(benchmarks.stats[key].sum())\n",
    "\n",
    "            for i in range(y_pred.shape[0]):\n",
    "                y_pred[i] = remove_small_objects(y_pred[i].astype(int), min_size=self.min_size)\n",
    "                y_true[i] = remove_small_objects(y_true[i].astype(int), min_size=self.min_size)\n",
    "\n",
    "            benchmarks = Metrics(self.model_name + ' - Removed objects less than {} pixels'.format(self.min_size), \n",
    "                                    seg=False)\n",
    "            benchmarks.calc_object_stats(y_true, y_pred)\n",
    "\n",
    "            # Save benchmarks in dict\n",
    "            self.benchmarks_remove_small_objects = {}\n",
    "            for key in benchmarks.stats.keys():\n",
    "                self.benchmarks_remove_small_objects[key] = int(benchmarks.stats[key].sum())\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _create_training_metadata(self):\n",
    "        training_metadata = {}\n",
    "        training_metadata['batch_size'] = self.batch_size\n",
    "        training_metadata['lr'] = self.lr\n",
    "        training_metadata['lr_decay'] = self.lr_decay\n",
    "        training_metadata['n_epochs'] = self.n_epochs\n",
    "        training_metadata['training_steps_per_epoch'] = self.training_steps_per_epoch\n",
    "        training_metadata['validation_steps_per_epoch'] = self.validation_steps_per_epoch\n",
    "\n",
    "        self.training_metadata = training_metadata\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _export_tf_serving(self):\n",
    "        export_model(self.model, self.tfserving_path, model_version=self.model_version)\n",
    "\n",
    "        # Convert model to TensorRT with float16\n",
    "        if 'fp16' in self.export_precisions:\n",
    "            export_model_dir = os.path.join(self.tfserving_path, str(self.model_version))\n",
    "            export_model_dir_fp16 = os.path.join(self.tfserving_path + '_fp16', str(self.model_version))\n",
    "\n",
    "            converter = trt.TrtGraphConverter(input_saved_model_dir=export_model_dir,\n",
    "                                            max_batch_size=self.max_batch_size,\n",
    "                                            precision_mode='fp16')\n",
    "            converter.convert()\n",
    "            converter.save(export_model_dir_fp16)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def create_model(self, export_serving=False, export_lite=False):\n",
    "\n",
    "        # Train model\n",
    "        self._fit()\n",
    "\n",
    "        # Load best performing weights\n",
    "        model_name = os.path.join(self.model_path, self.model_name + '.h5')\n",
    "        self.model.load_weights(model_name)\n",
    "\n",
    "        # Create model hash\n",
    "        self._create_hash()\n",
    "\n",
    "        # Create benchmarks\n",
    "        self._benchmark()\n",
    "\n",
    "        # Create model metadata\n",
    "        self._create_training_metadata()\n",
    "\n",
    "        # Save model\n",
    "        model_name = os.path.join(self.model_path, self.model_name + '_' + self.model_hash + '.h5')\n",
    "        self.model.save(model_name)\n",
    "\n",
    "        # Save loss history\n",
    "        loss_name = os.path.join(self.model_path, self.model_name + '_loss_' + self.model_hash + '.npz')\n",
    "        np.savez(loss_name, loss_history=self.loss_history.history)\n",
    "\n",
    "        # Save metadata (training and dataset) and benchmarks\n",
    "        metadata = {}\n",
    "        metadata['model_hash'] = self.model_hash\n",
    "        metadata['training_metadata'] = self.training_metadata\n",
    "        metadata['dataset_metadata'] = self.dataset_metadata\n",
    "        metadata['benchmarks'] = self.benchmarks\n",
    "        metadata['benchmarks_remove_small_objects'] = self.benchmarks_remove_small_objects\n",
    "\n",
    "        # TODO: Saving the benchmarking object in this way saves each individual benchmark.\n",
    "        # This should be refactored to save the sums.\n",
    "\n",
    "        metadata_name = os.path.join(self.model_path, self.model_name + '_' + self.model_hash + '.json')\n",
    "        \n",
    "        with open(metadata_name, 'w') as json_file:\n",
    "            json.dump(metadata, json_file)\n",
    "\n",
    "        # Export tf serving model\n",
    "        if export_serving:\n",
    "            self._export_tf_serving()\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from deepcell_toolbox import retinamask_postprocess\n",
    "from deepcell_toolbox.deep_watershed import deep_watershed as watershed_postprocess \n",
    "from functools import partial\n",
    "from scipy import ndimage\n",
    "\n",
    "if model_type == 'watershed':\n",
    "    postprocessing_fn = watershed_postprocess\n",
    "elif model_type == 'pixelwise':\n",
    "    def pixelwise(prediction, threshold=.5):\n",
    "        \"\"\"Post-processing for pixelwise transform predictions.\n",
    "        Uses the interior predictions to uniquely label every instance.\n",
    "        Args:\n",
    "            prediction: pixelwise transform prediction\n",
    "            threshold: confidence threshold for interior predictions\n",
    "        Returns:\n",
    "            post-processed data with each cell uniquely annotated\n",
    "        \"\"\"\n",
    "        labeled = []\n",
    "        for b in range(prediction.shape[0]):\n",
    "            pred = prediction[b]\n",
    "            interior = pred[..., 1] > threshold\n",
    "            data = np.expand_dims(interior, axis=-1)\n",
    "            label_image = ndimage.label(data)[0]\n",
    "            labeled.append(label_image)\n",
    "        labeled = np.stack(labeled, axis=0)\n",
    "        return labeled\n",
    "    postprocessing_fn = pixelwise\n",
    "elif model_type == 'retinamask':\n",
    "    postprocessing_fn = partial(retinamask_postprocess, image_shape=(128,128))\n",
    "    \n",
    "model_trainer = ModelTrainer(model,\n",
    "                            model_name,\n",
    "                            model_path,\n",
    "                            train_data,\n",
    "                            val_data,\n",
    "                            benchmarking_data = test_dict,\n",
    "                            postprocessing_fn=postprocessing_fn,\n",
    "                            predict_batch_size=32,\n",
    "                            dataset_metadata=dataset_metadata,\n",
    "                            training_kwargs=training_kwargs)\n",
    "\n",
    "model_trainer.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
