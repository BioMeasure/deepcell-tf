{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import getopt\n",
    "import errno\n",
    "\n",
    "sys.path.insert(0,\"/home/sunnycui/deepcell-tf\")\n",
    "\n",
    "MODEL_DIR = os.path.join(sys.path[0], 'scripts/recurr_gru/models/')\n",
    "LOG_DIR = os.path.join(sys.path[0], 'scripts/recurr_gru/logs/')\n",
    "DATA_DIR = os.path.join(sys.path[0], 'scripts/recurr_gru/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunnycui/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sunnycui/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunnycui/deepcell-tf/deepcell/utils/__init__.py:48: UserWarning: To use `compute_overlap`, the C extensions must be built using `python setup.py build_ext --inplace`\n",
      "  warnings.warn('To use `compute_overlap`, the C extensions must be built '\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import callbacks\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "\n",
    "import deepcell\n",
    "from deepcell import losses\n",
    "from scripts.recurr_gru import image_gen\n",
    "from deepcell import image_generators, stack_generator\n",
    "from deepcell import model_zoo\n",
    "\n",
    "from deepcell.utils import train_utils\n",
    "from deepcell.utils.data_utils import get_data\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "\n",
    "from deepcell.layers import TensorProduct, ReflectionPadding3D, DilatedMaxPool3D\n",
    "from tensorflow.python.keras.layers import MaxPool3D, Conv3DTranspose, UpSampling3D\n",
    "from scripts.recurr_gru.conv_gru_layer import ConvGRU2D\n",
    "from tensorflow.python.keras.layers import BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.python.keras.layers import Conv3D, ZeroPadding3D, ConvLSTM2D, Cropping3D\n",
    "from tensorflow.python.keras.layers import Input, Add, Concatenate, Flatten\n",
    "from tensorflow.python.keras.engine.input_layer import InputLayer\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "from deepcell.layers import ImageNormalization3D\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Activation, Softmax\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dapi_mini.npz\n",
      " -\n",
      "X.shape: (108, 61, 128, 128, 1)\n",
      "y.shape: (108, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "filename = 'dapi_mini.npz'\n",
    "DATA_FILE = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "print(\"Loading data from \" + filename)\n",
    "train_dict, test_dict = get_data(DATA_FILE, mode='conv', test_size=0.1, seed=0)\n",
    "\n",
    "print(' -\\nX.shape: {}\\ny.shape: {}'.format(train_dict['X'].shape, train_dict['y'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fgbg_model_name = 'nuclei_model_fgbg'\n",
    "watershed_model_name = 'nuclei_model_watershed'\n",
    "\n",
    "\n",
    "n_epoch = 20  # Number of training epochs\n",
    "test_size = .10  # % of data saved as test\n",
    "norm_method = None  # data normalization\n",
    "receptive_field = 61  # should be adjusted for the scale of the data\n",
    "\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=0.01, decay=0.99)\n",
    "\n",
    "# FC training settings\n",
    "n_skips = 0  # number of skip-connections (only for FC training)\n",
    "batch_size = 1  # FC training uses 1 image per batch\n",
    "\n",
    "# Transformation settings\n",
    "transform = 'watershed'\n",
    "dilation_radius = 1  # change dilation radius for edge dilation\n",
    "\n",
    "# 3D Settings\n",
    "frames_per_batch = 61\n",
    "# norm_method = None #'whole_image'\n",
    "\n",
    "distance_bins = 4  # number of distance classes\n",
    "erosion_width = 0  # erode edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Lambda;\n",
    "    \n",
    "    \n",
    "def feature_net_3D(receptive_field=61,\n",
    "                    n_frames=3,\n",
    "                    input_shape=(5, 256, 256, 1),\n",
    "                    n_features=3,\n",
    "                    n_channels=1,\n",
    "                    reg=1e-5,\n",
    "                    n_conv_filters=64,\n",
    "                    n_dense_filters=200,\n",
    "                    VGG_mode=False,\n",
    "                    init='he_normal',\n",
    "                    norm_method='whole_image',\n",
    "                    gru=False,\n",
    "                    location=False,\n",
    "                    dilated=False,\n",
    "                    padding=False,\n",
    "                    padding_mode='reflect',\n",
    "                    multires=False,\n",
    "                    include_top=False):\n",
    "    # Create layers list (x) to store all of the layers.\n",
    "    # We need to use the functional API to enable the multiresolution mode\n",
    "    x = []\n",
    "\n",
    "    win = (receptive_field - 1) // 2\n",
    "    win_z = (n_frames - 1) // 2\n",
    "\n",
    "    if dilated:\n",
    "        padding = True\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "        time_axis = 2\n",
    "        row_axis = 3\n",
    "        col_axis = 4\n",
    "        if not dilated:\n",
    "            input_shape = (n_channels, n_frames, receptive_field, receptive_field)\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "        time_axis = 1\n",
    "        row_axis = 2\n",
    "        col_axis = 3\n",
    "        if not dilated:\n",
    "            input_shape = (n_frames, receptive_field, receptive_field, n_channels)\n",
    "\n",
    "    x.append(Input(shape=input_shape))\n",
    "    x.append(ImageNormalization3D(norm_method=norm_method, filter_size=receptive_field)(x[-1]))\n",
    "    # x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "\n",
    "    if padding:\n",
    "        if padding_mode == 'reflect':\n",
    "            x.append(ReflectionPadding3D(padding=(win_z, win, win))(x[-1]))\n",
    "        elif padding_mode == 'zero':\n",
    "            x.append(ZeroPadding3D(padding=(win_z, win, win))([-1]))\n",
    "\n",
    "    if location:\n",
    "        x.append(Location3D(in_shape=tuple(x[-1].shape.as_list()[1:]))(x[-1]))\n",
    "        x.append(Concatenate(axis=channel_axis)([x[-2], x[-1]]))\n",
    "\n",
    "    if multires:\n",
    "        layers_to_concat = []\n",
    "\n",
    "    rf_counter = receptive_field\n",
    "    block_counter = 0\n",
    "    d = 1\n",
    "\n",
    "    append_gru = False\n",
    "    while rf_counter > 4:\n",
    "        filter_size = 3 if rf_counter % 2 == 0 else 4\n",
    "        x.append(Conv3D(n_conv_filters, (1, filter_size, filter_size), \n",
    "                        dilation_rate=(1, d, d), kernel_initializer=init,\n",
    "                        padding='valid', kernel_regularizer=l2(reg))(x[-1]))\n",
    "\n",
    "        \n",
    "        x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "        x.append(Activation('relu')(x[-1]))\n",
    "\n",
    "        block_counter += 1\n",
    "        rf_counter -= filter_size - 1\n",
    "\n",
    "        if block_counter % 2 == 0:\n",
    "            if dilated:\n",
    "                x.append(DilatedMaxPool3D(dilation_rate=(1, d, d), pool_size=(1, 2, 2))(x[-1]))\n",
    "                d *= 2\n",
    "            else:\n",
    "                x.append(MaxPool3D(pool_size=(1, 2, 2))(x[-1]))\n",
    "\n",
    "            if VGG_mode:\n",
    "                n_conv_filters *= 2\n",
    "\n",
    "            rf_counter = rf_counter // 2\n",
    "\n",
    "            if multires:\n",
    "                layers_to_concat.append(len(x) - 1)\n",
    "\n",
    "    if multires:\n",
    "        c = []\n",
    "        for l in layers_to_concat:\n",
    "            output_shape = x[l].get_shape().as_list()\n",
    "            target_shape = x[-1].get_shape().as_list()\n",
    "            time_crop = (0, 0)\n",
    "\n",
    "            row_crop = int(output_shape[row_axis] - target_shape[row_axis])\n",
    "\n",
    "            if row_crop % 2 == 0:\n",
    "                row_crop = (row_crop // 2, row_crop // 2)\n",
    "            else:\n",
    "                row_crop = (row_crop // 2, row_crop // 2 + 1)\n",
    "\n",
    "            col_crop = int(output_shape[col_axis] - target_shape[col_axis])\n",
    "\n",
    "            if col_crop % 2 == 0:\n",
    "                col_crop = (col_crop // 2, col_crop // 2)\n",
    "            else:\n",
    "                col_crop = (col_crop // 2, col_crop // 2 + 1)\n",
    "\n",
    "            cropping = (time_crop, row_crop, col_crop)\n",
    "\n",
    "            c.append(Cropping3D(cropping=cropping)(x[l]))\n",
    "        x.append(Concatenate(axis=channel_axis)(c))\n",
    "        \n",
    "    \n",
    "    x.append(Conv3D(n_dense_filters, (1, rf_counter, rf_counter), dilation_rate=(1, d, d), kernel_initializer=init, padding='valid', kernel_regularizer=l2(reg))(x[-1]))\n",
    "    x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "    x.append(Activation('relu')(x[-1]))\n",
    "\n",
    "    x.append(Conv3D(n_dense_filters, (n_frames, 1, 1), dilation_rate=(1, d, d), kernel_initializer=init, padding='valid', kernel_regularizer=l2(reg))(x[-1]))\n",
    "    x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "    x.append(Activation('relu')(x[-1]))\n",
    "    \n",
    "    x.append(TensorProduct(n_dense_filters, kernel_initializer=init, kernel_regularizer=l2(reg))(x[-1]))\n",
    "    x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "    x.append(Activation('relu')(x[-1]))\n",
    "\n",
    "    x.append(TensorProduct(1, kernel_initializer=init, kernel_regularizer=l2(reg))(x[-1]))\n",
    "    x.append(MaxPool3D(pool_size=(61, 1, 1))(x[-1]))\n",
    "    x.append((Lambda(lambda x: x[:,0,...]))(x[-1]))\n",
    "    \n",
    "    \n",
    "    if not dilated:\n",
    "        x.append(Flatten()(x[-1]))\n",
    "\n",
    "    if include_top:\n",
    "        x.append(Softmax(axis=channel_axis)(x[-1]))\n",
    "\n",
    "    model = Model(inputs=x[0], outputs=x[-1])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Lambda;\n",
    "\n",
    "def image_norm(inputs):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "    axes = [3, 4] if channel_axis == 1 else [2, 3]\n",
    "    output = inputs - K.mean(inputs, axis=axes, keepdims=True)\n",
    "    output = output / K.std(inputs, axis=axes, keepdims=True)\n",
    "    return output\n",
    "\n",
    "def feature_net_skip_3D(receptive_field=61,\n",
    "                        input_shape=(5, 256, 256, 1),\n",
    "                        fgbg_model=None,\n",
    "                        last_only=True,\n",
    "                        n_skips=1,\n",
    "                        norm_method='whole_image',\n",
    "                        padding_mode='reflect',\n",
    "                        **kwargs):\n",
    "    # print(K.image_data_format())\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    img = Lambda(image_norm)(inputs)\n",
    "    #img = BatchNormalization(axis=channel_axis)(inputs)\n",
    "\n",
    "    models = []\n",
    "    model_outputs = []\n",
    "\n",
    "    if fgbg_model is not None:\n",
    "        for layer in fgbg_model.layers:\n",
    "            layer.trainable = False\n",
    "        models.append(fgbg_model)\n",
    "        fgbg_output = fgbg_model(inputs)\n",
    "        if isinstance(fgbg_output, list):\n",
    "            fgbg_output = fgbg_output[-1]\n",
    "        model_outputs.append(fgbg_output)\n",
    "\n",
    "    for _ in range(n_skips + 1):\n",
    "        if model_outputs:\n",
    "            model_input = Concatenate(axis=channel_axis)([img, model_outputs[-1]])\n",
    "        else:\n",
    "            model_input = img\n",
    "        new_input_shape = model_input.get_shape().as_list()[1:]\n",
    "        models.append(feature_net_3D(receptive_field=receptive_field, \n",
    "                                     input_shape=new_input_shape, norm_method=None, dilated=True, \n",
    "                                     padding=True, padding_mode=padding_mode, **kwargs))\n",
    "        model_outputs.append(models[-1](model_input))\n",
    "\n",
    "    if last_only:\n",
    "        model = Model(inputs=inputs, outputs=model_outputs[-1])\n",
    "    else:\n",
    "        if fgbg_model is None:\n",
    "            model = Model(inputs=inputs, outputs=model_outputs)\n",
    "        else:\n",
    "            model = Model(inputs=inputs, outputs=model_outputs[1:])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model_conv(model,\n",
    "                     #dataset,\n",
    "                     train_dict,\n",
    "                     test_dict,\n",
    "                     expt='',\n",
    "                     test_size=.1,\n",
    "                     n_epoch=10,\n",
    "                     batch_size=1,\n",
    "                     num_gpus=None,\n",
    "                     frames_per_batch=5,\n",
    "                     transform=None,\n",
    "                     optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "                     log_dir='/data/tensorboard_logs',\n",
    "                     model_dir='/data/models',\n",
    "                     model_name=None,\n",
    "                     focal=False,\n",
    "                     gamma=0.5,\n",
    "                     lr_sched=rate_scheduler(lr=0.01, decay=0.95),\n",
    "                     rotation_range=0,\n",
    "                     flip=True,\n",
    "                     shear=0,\n",
    "                     zoom_range=0,\n",
    "                     seed=None,\n",
    "                     **kwargs):\n",
    "    is_channels_first = K.image_data_format() == 'channels_first'\n",
    "\n",
    "    if model_name is None:\n",
    "        todays_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "        data_name = os.path.splitext(os.path.basename(dataset))[0]\n",
    "        model_name = '{}_{}_{}'.format(todays_date, data_name, expt)\n",
    "    model_path = os.path.join(model_dir, '{}.h5'.format(model_name))\n",
    "    loss_path = os.path.join(model_dir, '{}.npz'.format(model_name))\n",
    "    \n",
    "    # train_dict, test_dict = get_data(dataset, mode='conv', test_size=test_size)\n",
    "\n",
    "    n_classes = model.layers[-1].output_shape[1 if is_channels_first else -1]\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    print('X_train shape:', train_dict['X'].shape)\n",
    "    print('y_train shape:', train_dict['y'].shape)\n",
    "    print('X_test shape:', test_dict['X'].shape)\n",
    "    print('y_test shape:', test_dict['y'].shape)\n",
    "    print('Output Shape:', model.layers[-1].output_shape)\n",
    "    print('Number of Classes:', n_classes)\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        if isinstance(transform, str) and transform.lower() == 'disc':\n",
    "            return losses.discriminative_instance_loss(y_true, y_pred)\n",
    "        if focal:\n",
    "            return losses.weighted_focal_loss(\n",
    "                y_true, y_pred, gamma=gamma, n_classes=n_classes)\n",
    "        return losses.weighted_categorical_crossentropy(\n",
    "            y_true, y_pred, n_classes=n_classes, from_logits=False)\n",
    "\n",
    "    if num_gpus is None:\n",
    "        num_gpus = train_utils.count_gpus()\n",
    "\n",
    "    if num_gpus >= 2:\n",
    "        batch_size = batch_size * num_gpus\n",
    "        model = train_utils.MultiGpuModel(model, num_gpus)\n",
    "\n",
    "    print('Training on {} GPUs'.format(num_gpus))\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    if isinstance(model.output_shape, list):\n",
    "        skip = len(model.output_shape) - 1\n",
    "    else:\n",
    "        skip = None\n",
    "\n",
    "    if train_dict['X'].ndim == 4:\n",
    "        DataGenerator = image_generators.ImageFullyConvDataGenerator\n",
    "    elif train_dict['X'].ndim == 5:\n",
    "        DataGenerator = stack_generator.StackDataGenerator\n",
    "    else:\n",
    "        raise ValueError('Expected `X` to have ndim 4 or 5. Got',\n",
    "                         train_dict['X'].ndim)\n",
    "\n",
    "    if num_gpus >= 2:\n",
    "        # Each GPU must have at least one validation example\n",
    "        if test_dict['y'].shape[0] < num_gpus:\n",
    "            raise ValueError('Not enough validation data for {} GPUs. '\n",
    "                             'Received {} validation sample.'.format(\n",
    "                                 test_dict['y'].shape[0], num_gpus))\n",
    "\n",
    "        # When using multiple GPUs and skip_connections,\n",
    "        # the training data must be evenly distributed across all GPUs\n",
    "        num_train = train_dict['y'].shape[0]\n",
    "        nb_samples = num_train - num_train % batch_size\n",
    "        if nb_samples:\n",
    "            train_dict['y'] = train_dict['y'][:nb_samples]\n",
    "            train_dict['X'] = train_dict['X'][:nb_samples]\n",
    "\n",
    "    # this will do preprocessing and realtime data augmentation\n",
    "    datagen = DataGenerator(\n",
    "        rotation_range=rotation_range,\n",
    "        shear_range=shear,\n",
    "        zoom_range=zoom_range,\n",
    "        horizontal_flip=flip,\n",
    "        vertical_flip=flip)\n",
    "\n",
    "    datagen_val = DataGenerator(\n",
    "        rotation_range=0,\n",
    "        shear_range=0,\n",
    "        zoom_range=0,\n",
    "        horizontal_flip=0,\n",
    "        vertical_flip=0)\n",
    "\n",
    "    if train_dict['X'].ndim == 5:\n",
    "        train_data = datagen_val.flow(\n",
    "            train_dict,\n",
    "            skip=skip,\n",
    "            batch_size=batch_size,\n",
    "            transform=transform,\n",
    "            transform_kwargs=kwargs,\n",
    "            frames_per_batch=frames_per_batch)\n",
    "\n",
    "        val_data = datagen_val.flow(\n",
    "            test_dict,\n",
    "            skip=skip,\n",
    "            batch_size=batch_size,\n",
    "            transform=transform,\n",
    "            transform_kwargs=kwargs,\n",
    "            frames_per_batch=frames_per_batch)\n",
    "    else:\n",
    "        train_data = datagen.flow(\n",
    "            train_dict,\n",
    "            skip=skip,\n",
    "            batch_size=batch_size,\n",
    "            transform=transform,\n",
    "            transform_kwargs=kwargs)\n",
    "\n",
    "        val_data = datagen_val.flow(\n",
    "            test_dict,\n",
    "            skip=skip,\n",
    "            batch_size=batch_size,\n",
    "            transform=transform,\n",
    "            transform_kwargs=kwargs)\n",
    "\n",
    "    # fit the model on the batches generated by datagen.flow()\n",
    "    loss_history = model.fit_generator(\n",
    "        train_data,\n",
    "        steps_per_epoch=train_data.y.shape[0] // batch_size,\n",
    "        epochs=n_epoch,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=val_data.y.shape[0] // batch_size,\n",
    "        callbacks=[\n",
    "            callbacks.LearningRateScheduler(lr_sched),\n",
    "            callbacks.ModelCheckpoint(\n",
    "                model_path, monitor='val_loss', verbose=1,\n",
    "                save_best_only=True, save_weights_only=num_gpus >= 2),\n",
    "            callbacks.TensorBoard(log_dir=os.path.join(log_dir, model_name))\n",
    "        ])\n",
    "\n",
    "    model.save_weights(model_path)\n",
    "    np.savez(loss_path, loss_history=loss_history.history)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 61, 128, 128, 1)   0         \n",
      "_________________________________________________________________\n",
      "image_normalization3d (Image (None, 61, 128, 128, 1)   226981    \n",
      "_________________________________________________________________\n",
      "reflection_padding3d (Reflec (None, 121, 188, 188, 1)  0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 121, 185, 185, 32) 544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 121, 185, 185, 32) 128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 121, 185, 185, 32) 0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 121, 183, 183, 32) 9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 121, 183, 183, 32) 128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 121, 183, 183, 32) 0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d (DilatedM (None, 121, 182, 182, 32) 0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 121, 178, 178, 32) 9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 121, 178, 178, 32) 128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 121, 178, 178, 32) 0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 121, 174, 174, 32) 9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 121, 174, 174, 32) 128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 121, 174, 174, 32) 0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_1 (Dilate (None, 121, 172, 172, 32) 0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 121, 164, 164, 32) 9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 121, 164, 164, 32) 128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 121, 164, 164, 32) 0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 121, 156, 156, 32) 9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 121, 156, 156, 32) 128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 121, 156, 156, 32) 0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_2 (Dilate (None, 121, 152, 152, 32) 0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 121, 128, 128, 128 65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 121, 128, 128, 128 512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 121, 128, 128, 128 0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 61, 128, 128, 128) 999552    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 61, 128, 128, 128) 512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 61, 128, 128, 128) 0         \n",
      "_________________________________________________________________\n",
      "tensor_product (TensorProduc (None, 61, 128, 128, 128) 16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 61, 128, 128, 128) 512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 61, 128, 128, 128) 0         \n",
      "_________________________________________________________________\n",
      "tensor_product_1 (TensorProd (None, 61, 128, 128, 1)   129       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 1, 128, 128, 1)    0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 128, 128, 1)       0         \n",
      "=================================================================\n",
      "Total params: 1,357,926\n",
      "Trainable params: 1,129,793\n",
      "Non-trainable params: 228,133\n",
      "_________________________________________________________________\n",
      "X_train shape: (108, 61, 128, 128, 1)\n",
      "y_train shape: (108, 128, 128, 1)\n",
      "X_test shape: (12, 61, 128, 128, 1)\n",
      "y_test shape: (12, 128, 128, 1)\n",
      "Output Shape: (None, 128, 128, 1)\n",
      "Number of Classes: 1\n",
      "Training on 2 GPUs\n",
      "WARNING:tensorflow:From /home/sunnycui/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128,128,121,16,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv3d_6/Conv3D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node GroupCrossDeviceControlEdges_0/training_1/group_deps/NoOp_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-be61658576ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mrotation_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mflip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     shear=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-720c69c4e407>\u001b[0m in \u001b[0;36mtrain_model_conv\u001b[0;34m(model, train_dict, test_dict, expt, test_size, n_epoch, batch_size, num_gpus, frames_per_batch, transform, optimizer, log_dir, model_dir, model_name, focal, gamma, lr_sched, rotation_range, flip, shear, zoom_range, seed, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 save_best_only=True, save_weights_only=num_gpus >= 2),\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         ])\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,128,121,16,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv3d_6/Conv3D}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node GroupCrossDeviceControlEdges_0/training_1/group_deps/NoOp_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "dapi_model = feature_net_3D(\n",
    "        receptive_field=receptive_field,\n",
    "        n_features=2,  # segmentation mask (is_cell, is_not_cell)\n",
    "        n_frames=frames_per_batch,\n",
    "        dilated=True, \n",
    "        padding=True,\n",
    "        # n_skips=n_skips,\n",
    "        n_conv_filters=32,\n",
    "        n_dense_filters=128,\n",
    "        input_shape=tuple([frames_per_batch] + list(train_dict['X'].shape[2:])),\n",
    "        #multires=False,\n",
    "        #last_only=False,\n",
    "        norm_method=norm_method)\n",
    "\n",
    "dapi_model = train_model_conv(\n",
    "    model=dapi_model,\n",
    "    train_dict=train_dict,\n",
    "    test_dict=test_dict,\n",
    "    model_name=fgbg_model_name,\n",
    "    test_size=test_size,\n",
    "    optimizer=optimizer,\n",
    "    n_epoch=n_epoch,\n",
    "    batch_size=batch_size,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    transform='fgbg',\n",
    "    model_dir=MODEL_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fgbg_weights_file = os.path.join(MODEL_DIR, '{}.h5'.format(fgbg_model_name))\n",
    "# fgbg_model.save_weights(fgbg_weights_file)\n",
    "\n",
    "# watershed_weights_file = os.path.join(MODEL_DIR, '{}.h5'.format(watershed_model_name))\n",
    "# watershed_model.save_weights(watershed_weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# run_fgbg_model = feature_net_skip_3D(\n",
    "#     receptive_field=receptive_field,\n",
    "#     n_features=2,\n",
    "#     n_frames=frames_per_batch,\n",
    "#     n_skips=n_skips,\n",
    "#     n_conv_filters=32,\n",
    "#     n_dense_filters=128,\n",
    "#     input_shape=tuple(test_dict['X'].shape[1:]),\n",
    "#     multires=False,\n",
    "#     last_only=False,\n",
    "#     norm_method=norm_method)\n",
    "# run_fgbg_model.load_weights(fgbg_weights_file)\n",
    "\n",
    "# run_watershed_model = feature_net_skip_3D(\n",
    "#     receptive_field=receptive_field,\n",
    "#     n_skips=n_skips,\n",
    "#     n_features=distance_bins,\n",
    "#     n_frames=frames_per_batch,\n",
    "#     gru=False,\n",
    "#     gru_kernel_size=3,\n",
    "#     n_conv_filters=32,\n",
    "#     n_dense_filters=128,\n",
    "#     multires=False,\n",
    "#     last_only=False,\n",
    "#     input_shape=tuple(test_dict['X'].shape[1:]),\n",
    "#     norm_method=norm_method)\n",
    "\n",
    "# run_watershed_model.load_weights(watershed_weights_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_images = run_watershed_model.predict(test_dict['X'])[-1]\n",
    "# test_images_fgbg = run_fgbg_model.predict(test_dict['X'])[-1]\n",
    "\n",
    "# # test_images = watershed_model.predict(test_dict['X'])[-1]\n",
    "# # test_images_fgbg = fgbg_model.predict(test_dict['X'])[-1]\n",
    "\n",
    "# print('watershed transform shape:', test_images.shape)\n",
    "# print('segmentation mask shape:', test_images_fgbg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
