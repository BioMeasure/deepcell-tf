{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import getopt\n",
    "import errno\n",
    "\n",
    "sys.path.insert(0,\"/home/sunnycui/deepcell-tf\")\n",
    "\n",
    "MODEL_DIR = os.path.join(sys.path[0], 'scripts/recurr_gru/models/')\n",
    "LOG_DIR = os.path.join(sys.path[0], 'scripts/recurr_gru/logs/')\n",
    "DATA_DIR = os.path.join(sys.path[0], 'scripts/recurr_gru/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sunnycui/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import callbacks\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "\n",
    "import deepcell\n",
    "from deepcell import losses\n",
    "from scripts.recurr_gru import image_gen\n",
    "from deepcell import image_generators\n",
    "from deepcell import model_zoo\n",
    "\n",
    "from deepcell.utils import train_utils\n",
    "from deepcell.utils.data_utils import get_data\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "from deepcell.training import train_model_conv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import callbacks\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "\n",
    "import deepcell\n",
    "from deepcell import losses\n",
    "from scripts.recurr_gru import image_gen\n",
    "from deepcell import image_generators\n",
    "from deepcell import model_zoo\n",
    "from deepcell.layers import TensorProduct, ReflectionPadding3D, DilatedMaxPool3D\n",
    "\n",
    "from deepcell.utils import train_utils\n",
    "from deepcell.utils.data_utils import get_data\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "from deepcell.training import train_model_conv\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.layers import MaxPool3D, Conv3DTranspose, UpSampling3D\n",
    "from scripts.recurr_gru.conv_gru_layer import ConvGRU2D\n",
    "from tensorflow.python.keras.layers import BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.python.keras.layers import Conv3D, ZeroPadding3D, ConvLSTM2D, Cropping3D\n",
    "from tensorflow.python.keras.layers import Input, Add, Concatenate, Flatten\n",
    "from tensorflow.python.keras.engine.input_layer import InputLayer\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "from deepcell.layers import ImageNormalization3D\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Activation, Softmax\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from combined_is_nuclei.npz\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sunnycui/deepcell-tf/scripts/recurr_gru/data/combined_is_nuclei.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5dc09c5a82b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data from \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' -\\nX.shape: {}\\ny.shape: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepcell-tf/deepcell/utils/data_utils.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(file_name, mode, test_size, seed)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sunnycui/deepcell-tf/scripts/recurr_gru/data/combined_is_nuclei.npz'"
     ]
    }
   ],
   "source": [
    "filename = 'combined_is_nuclei.npz'\n",
    "DATA_FILE = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "print(\"Loading data from \" + filename)\n",
    "train_dict, test_dict = get_data(DATA_FILE, mode='conv', test_size=0.1, seed=0)\n",
    "\n",
    "print(' -\\nX.shape: {}\\ny.shape: {}'.format(train_dict['X'].shape, train_dict['y'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgbg_model_name = 'nuclei_model_fgbg'\n",
    "watershed_model_name = 'nuclei_model_watershed'\n",
    "\n",
    "\n",
    "n_epoch = 20  # Number of training epochs\n",
    "test_size = .10  # % of data saved as test\n",
    "norm_method = None  # data normalization\n",
    "receptive_field = 61  # should be adjusted for the scale of the data\n",
    "\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=0.01, decay=0.99)\n",
    "\n",
    "# FC training settings\n",
    "n_skips = 0  # number of skip-connections (only for FC training)\n",
    "batch_size = 1  # FC training uses 1 image per batch\n",
    "\n",
    "# Transformation settings\n",
    "transform = 'watershed'\n",
    "dilation_radius = 1  # change dilation radius for edge dilation\n",
    "\n",
    "# 3D Settings\n",
    "frames_per_batch = 3\n",
    "# norm_method = None #'whole_image'\n",
    "\n",
    "distance_bins = 4  # number of distance classes\n",
    "erosion_width = 0  # erode edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_net_3D(receptive_field=61,\n",
    "                    n_frames=3,\n",
    "                    input_shape=(5, 256, 256, 1),\n",
    "                    n_features=3,\n",
    "                    n_channels=1,\n",
    "                    reg=1e-5,\n",
    "                    n_conv_filters=64,\n",
    "                    n_dense_filters=200,\n",
    "                    gru_kernel_size =3,\n",
    "                    VGG_mode=False,\n",
    "                    init='he_normal',\n",
    "                    norm_method='whole_image',\n",
    "                    gru=False,\n",
    "                    location=False,\n",
    "                    dilated=False,\n",
    "                    padding=False,\n",
    "                    padding_mode='reflect',\n",
    "                    multires=False,\n",
    "                    include_top=True):\n",
    "    # Create layers list (x) to store all of the layers.\n",
    "    # We need to use the functional API to enable the multiresolution mode\n",
    "    x = []\n",
    "\n",
    "    win = (receptive_field - 1) // 2\n",
    "    win_z = (n_frames - 1) // 2\n",
    "\n",
    "    if dilated:\n",
    "        padding = True\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "        time_axis = 2\n",
    "        row_axis = 3\n",
    "        col_axis = 4\n",
    "        if not dilated:\n",
    "            input_shape = (n_channels, n_frames, receptive_field, receptive_field)\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "        time_axis = 1\n",
    "        row_axis = 2\n",
    "        col_axis = 3\n",
    "        if not dilated:\n",
    "            input_shape = (n_frames, receptive_field, receptive_field, n_channels)\n",
    "\n",
    "    x.append(Input(shape=input_shape))\n",
    "    # x.append(ImageNormalization3D(norm_method=norm_method, filter_size=receptive_field)(x[-1]))\n",
    "    # x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "\n",
    "    if padding:\n",
    "        if padding_mode == 'reflect':\n",
    "            x.append(ReflectionPadding3D(padding=(win_z, win, win))(x[-1]))\n",
    "        elif padding_mode == 'zero':\n",
    "            x.append(ZeroPadding3D(padding=(win_z, win, win))([-1]))\n",
    "\n",
    "    if location:\n",
    "        x.append(Location3D(in_shape=tuple(x[-1].shape.as_list()[1:]))(x[-1]))\n",
    "        x.append(Concatenate(axis=channel_axis)([x[-2], x[-1]]))\n",
    "\n",
    "    if multires:\n",
    "        layers_to_concat = []\n",
    "\n",
    "    rf_counter = receptive_field\n",
    "    block_counter = 0\n",
    "    d = 1\n",
    "\n",
    "    append_gru = False\n",
    "    while rf_counter > 4:\n",
    "        filter_size = 3 if rf_counter % 2 == 0 else 4\n",
    "        if append_gru == True:\n",
    "            x.append(ConvGRU2D(filters=n_conv_filters, kernel_size=(filter_size, filter_size), dilation_rate=(d, d),\n",
    "                            padding='valid', kernel_initializer=init,\n",
    "                            kernel_regularizer=l2(reg), return_sequences=True)(x[-1]))\n",
    "        else:\n",
    "            x.append(Conv3D(n_conv_filters, (1, filter_size, filter_size), \n",
    "                            dilation_rate=(1, d, d), kernel_initializer=init,\n",
    "                            padding='valid', kernel_regularizer=l2(reg))(x[-1]))\n",
    "\n",
    "        \n",
    "        x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "        x.append(Activation('relu')(x[-1]))\n",
    "\n",
    "        block_counter += 1\n",
    "        rf_counter -= filter_size - 1\n",
    "\n",
    "        if block_counter % 2 == 0:\n",
    "            if dilated:\n",
    "                x.append(DilatedMaxPool3D(dilation_rate=(1, d, d), pool_size=(1, 2, 2))(x[-1]))\n",
    "                d *= 2\n",
    "            else:\n",
    "                x.append(MaxPool3D(pool_size=(1, 2, 2))(x[-1]))\n",
    "\n",
    "            if VGG_mode:\n",
    "                n_conv_filters *= 2\n",
    "\n",
    "            rf_counter = rf_counter // 2\n",
    "\n",
    "            if multires:\n",
    "                layers_to_concat.append(len(x) - 1)\n",
    "\n",
    "    if multires:\n",
    "        c = []\n",
    "        for l in layers_to_concat:\n",
    "            output_shape = x[l].get_shape().as_list()\n",
    "            target_shape = x[-1].get_shape().as_list()\n",
    "            time_crop = (0, 0)\n",
    "\n",
    "            row_crop = int(output_shape[row_axis] - target_shape[row_axis])\n",
    "\n",
    "            if row_crop % 2 == 0:\n",
    "                row_crop = (row_crop // 2, row_crop // 2)\n",
    "            else:\n",
    "                row_crop = (row_crop // 2, row_crop // 2 + 1)\n",
    "\n",
    "            col_crop = int(output_shape[col_axis] - target_shape[col_axis])\n",
    "\n",
    "            if col_crop % 2 == 0:\n",
    "                col_crop = (col_crop // 2, col_crop // 2)\n",
    "            else:\n",
    "                col_crop = (col_crop // 2, col_crop // 2 + 1)\n",
    "\n",
    "            cropping = (time_crop, row_crop, col_crop)\n",
    "\n",
    "            c.append(Cropping3D(cropping=cropping)(x[l]))\n",
    "        x.append(Concatenate(axis=channel_axis)(c))\n",
    "        \n",
    "    \n",
    "    x.append(Conv3D(n_dense_filters, (1, rf_counter, rf_counter), dilation_rate=(1, d, d), kernel_initializer=init, padding='valid', kernel_regularizer=l2(reg))(x[-1]))\n",
    "    x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "    x.append(Activation('relu')(x[-1]))\n",
    "\n",
    "    x.append(Conv3D(n_dense_filters, (n_frames, 1, 1), dilation_rate=(1, d, d), kernel_initializer=init, padding='valid', kernel_regularizer=l2(reg))(x[-1]))\n",
    "    x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "    x.append(Activation('relu')(x[-1]))\n",
    "    \n",
    "    if gru == True:\n",
    "        x.append(ConvGRU2D(filters=n_conv_filters, kernel_size=(gru_kernel_size, gru_kernel_size),\n",
    "                            padding='same', kernel_initializer=init, activation='relu',\n",
    "                            kernel_regularizer=l2(reg), return_sequences=True)(x[-1]))\n",
    "        x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "        # x.append(Activation('relu')(x[-1]))\n",
    "        x.append(ConvGRU2D(filters=n_conv_filters, kernel_size=(gru_kernel_size +2, gru_kernel_size +2),\n",
    "                            padding='same', kernel_initializer=init, activation='relu',\n",
    "                            kernel_regularizer=l2(reg), return_sequences=True)(x[-1]))\n",
    "        x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "        # x.append(Activation('relu')(x[-1]))\n",
    "    \n",
    "    \n",
    "    x.append(TensorProduct(n_dense_filters, kernel_initializer=init, kernel_regularizer=l2(reg))(x[-1]))\n",
    "    x.append(BatchNormalization(axis=channel_axis)(x[-1]))\n",
    "    x.append(Activation('relu')(x[-1]))\n",
    "\n",
    "    x.append(TensorProduct(n_features, kernel_initializer=init, kernel_regularizer=l2(reg))(x[-1]))\n",
    "\n",
    "    if not dilated:\n",
    "        x.append(Flatten()(x[-1]))\n",
    "\n",
    "    if include_top:\n",
    "        x.append(Softmax(axis=channel_axis)(x[-1]))\n",
    "\n",
    "    model = Model(inputs=x[0], outputs=x[-1])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Lambda;\n",
    "\n",
    "def image_norm(inputs):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "    axes = [3, 4] if channel_axis == 1 else [2, 3]\n",
    "    output = inputs - K.mean(inputs, axis=axes, keepdims=True)\n",
    "    output = output / K.std(inputs, axis=axes, keepdims=True)\n",
    "    return output\n",
    "\n",
    "def feature_net_skip_3D(receptive_field=61,\n",
    "                        input_shape=(5, 256, 256, 1),\n",
    "                        fgbg_model=None,\n",
    "                        gru=False,\n",
    "                        gru_kernel_size=3,\n",
    "                        last_only=True,\n",
    "                        n_skips=1,\n",
    "                        norm_method='whole_image',\n",
    "                        padding_mode='reflect',\n",
    "                        **kwargs):\n",
    "    # print(K.image_data_format())\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    img = Lambda(image_norm)(inputs)\n",
    "    #img = BatchNormalization(axis=channel_axis)(inputs)\n",
    "\n",
    "    models = []\n",
    "    model_outputs = []\n",
    "\n",
    "    if fgbg_model is not None:\n",
    "        for layer in fgbg_model.layers:\n",
    "            layer.trainable = False\n",
    "        models.append(fgbg_model)\n",
    "        fgbg_output = fgbg_model(inputs)\n",
    "        if isinstance(fgbg_output, list):\n",
    "            fgbg_output = fgbg_output[-1]\n",
    "        model_outputs.append(fgbg_output)\n",
    "\n",
    "    for _ in range(n_skips + 1):\n",
    "        if model_outputs:\n",
    "            model_input = Concatenate(axis=channel_axis)([img, model_outputs[-1]])\n",
    "        else:\n",
    "            model_input = img\n",
    "        new_input_shape = model_input.get_shape().as_list()[1:]\n",
    "        models.append(feature_net_3D(receptive_field=receptive_field, \n",
    "                                     input_shape=new_input_shape, norm_method=None, dilated=True, \n",
    "                                     padding=True, padding_mode=padding_mode, gru=gru, \n",
    "                                     gru_kernel_size=gru_kernel_size, **kwargs))\n",
    "        model_outputs.append(models[-1](model_input))\n",
    "\n",
    "    if last_only:\n",
    "        model = Model(inputs=inputs, outputs=model_outputs[-1])\n",
    "    else:\n",
    "        if fgbg_model is None:\n",
    "            model = Model(inputs=inputs, outputs=model_outputs)\n",
    "        else:\n",
    "            model = Model(inputs=inputs, outputs=model_outputs[1:])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_74 (InputLayer)        (None, 3, 160, 160, 1)    0         \n",
      "_________________________________________________________________\n",
      "reflection_padding3d_36 (Ref (None, 5, 220, 220, 1)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_292 (Conv3D)          (None, 5, 217, 217, 32)   544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_328 (Bat (None, 5, 217, 217, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_328 (Activation)  (None, 5, 217, 217, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_293 (Conv3D)          (None, 5, 215, 215, 32)   9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_329 (Bat (None, 5, 215, 215, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_329 (Activation)  (None, 5, 215, 215, 32)   0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_104 (Dila (None, 5, 214, 214, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_294 (Conv3D)          (None, 5, 210, 210, 32)   9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_330 (Bat (None, 5, 210, 210, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_330 (Activation)  (None, 5, 210, 210, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_295 (Conv3D)          (None, 5, 206, 206, 32)   9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_331 (Bat (None, 5, 206, 206, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_331 (Activation)  (None, 5, 206, 206, 32)   0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_105 (Dila (None, 5, 204, 204, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_296 (Conv3D)          (None, 5, 196, 196, 32)   9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_332 (Bat (None, 5, 196, 196, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_332 (Activation)  (None, 5, 196, 196, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_297 (Conv3D)          (None, 5, 188, 188, 32)   9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_333 (Bat (None, 5, 188, 188, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_333 (Activation)  (None, 5, 188, 188, 32)   0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_106 (Dila (None, 5, 184, 184, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_298 (Conv3D)          (None, 5, 160, 160, 128)  65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_334 (Bat (None, 5, 160, 160, 128)  512       \n",
      "_________________________________________________________________\n",
      "activation_334 (Activation)  (None, 5, 160, 160, 128)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_299 (Conv3D)          (None, 3, 160, 160, 128)  49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_335 (Bat (None, 3, 160, 160, 128)  512       \n",
      "_________________________________________________________________\n",
      "activation_335 (Activation)  (None, 3, 160, 160, 128)  0         \n",
      "_________________________________________________________________\n",
      "tensor_product_72 (TensorPro (None, 3, 160, 160, 128)  16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_336 (Bat (None, 3, 160, 160, 128)  512       \n",
      "_________________________________________________________________\n",
      "activation_336 (Activation)  (None, 3, 160, 160, 128)  0         \n",
      "_________________________________________________________________\n",
      "tensor_product_73 (TensorPro (None, 3, 160, 160, 2)    258       \n",
      "_________________________________________________________________\n",
      "softmax_36 (Softmax)         (None, 3, 160, 160, 2)    0         \n",
      "=================================================================\n",
      "Total params: 180,802\n",
      "Trainable params: 179,650\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_73 (InputLayer)        (None, 3, 160, 160, 1)    0         \n",
      "_________________________________________________________________\n",
      "lambda_36 (Lambda)           (None, 3, 160, 160, 1)    0         \n",
      "_________________________________________________________________\n",
      "model_72 (Model)             (None, 3, 160, 160, 2)    180802    \n",
      "=================================================================\n",
      "Total params: 180,802\n",
      "Trainable params: 179,650\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_76 (InputLayer)        (None, 3, 160, 160, 1)    0         \n",
      "_________________________________________________________________\n",
      "reflection_padding3d_37 (Ref (None, 5, 220, 220, 1)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_300 (Conv3D)          (None, 5, 217, 217, 32)   544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_337 (Bat (None, 5, 217, 217, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_337 (Activation)  (None, 5, 217, 217, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_301 (Conv3D)          (None, 5, 215, 215, 32)   9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_338 (Bat (None, 5, 215, 215, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_338 (Activation)  (None, 5, 215, 215, 32)   0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_107 (Dila (None, 5, 214, 214, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_302 (Conv3D)          (None, 5, 210, 210, 32)   9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_339 (Bat (None, 5, 210, 210, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_339 (Activation)  (None, 5, 210, 210, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_303 (Conv3D)          (None, 5, 206, 206, 32)   9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_340 (Bat (None, 5, 206, 206, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_340 (Activation)  (None, 5, 206, 206, 32)   0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_108 (Dila (None, 5, 204, 204, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_304 (Conv3D)          (None, 5, 196, 196, 32)   9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_341 (Bat (None, 5, 196, 196, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_341 (Activation)  (None, 5, 196, 196, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_305 (Conv3D)          (None, 5, 188, 188, 32)   9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_342 (Bat (None, 5, 188, 188, 32)   128       \n",
      "_________________________________________________________________\n",
      "activation_342 (Activation)  (None, 5, 188, 188, 32)   0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_109 (Dila (None, 5, 184, 184, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_306 (Conv3D)          (None, 5, 160, 160, 128)  65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_343 (Bat (None, 5, 160, 160, 128)  512       \n",
      "_________________________________________________________________\n",
      "activation_343 (Activation)  (None, 5, 160, 160, 128)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_307 (Conv3D)          (None, 3, 160, 160, 128)  49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_344 (Bat (None, 3, 160, 160, 128)  512       \n",
      "_________________________________________________________________\n",
      "activation_344 (Activation)  (None, 3, 160, 160, 128)  0         \n",
      "_________________________________________________________________\n",
      "tensor_product_74 (TensorPro (None, 3, 160, 160, 128)  16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_345 (Bat (None, 3, 160, 160, 128)  512       \n",
      "_________________________________________________________________\n",
      "activation_345 (Activation)  (None, 3, 160, 160, 128)  0         \n",
      "_________________________________________________________________\n",
      "tensor_product_75 (TensorPro (None, 3, 160, 160, 4)    516       \n",
      "_________________________________________________________________\n",
      "softmax_37 (Softmax)         (None, 3, 160, 160, 4)    0         \n",
      "=================================================================\n",
      "Total params: 181,060\n",
      "Trainable params: 179,908\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_75 (InputLayer)        (None, 3, 160, 160, 1)    0         \n",
      "_________________________________________________________________\n",
      "lambda_37 (Lambda)           (None, 3, 160, 160, 1)    0         \n",
      "_________________________________________________________________\n",
      "model_74 (Model)             (None, 3, 160, 160, 4)    181060    \n",
      "=================================================================\n",
      "Total params: 181,060\n",
      "Trainable params: 179,908\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fgbg_model = feature_net_skip_3D(\n",
    "    n_features=2,  # segmentation mask (is_cell, is_not_cell)\n",
    "    receptive_field=receptive_field,\n",
    "    n_frames=frames_per_batch,\n",
    "    gru=False,\n",
    "    gru_kernel_size=3,\n",
    "    n_skips=n_skips,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    input_shape=tuple([frames_per_batch] + list(train_dict['X'].shape[2:])),\n",
    "    last_only=False)\n",
    "\n",
    "watershed_model = feature_net_skip_3D(\n",
    "    n_features=distance_bins,  # segmentation mask (is_cell, is_not_cell)\n",
    "    receptive_field=receptive_field,\n",
    "    n_frames=frames_per_batch,\n",
    "    n_skips=n_skips,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    input_shape=tuple([frames_per_batch] + list(train_dict['X'].shape[2:])),\n",
    "    last_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8, 24, 160, 160, 1)\n",
      "y_train shape: (8, 24, 160, 160, 1)\n",
      "X_test shape: (1, 24, 160, 160, 1)\n",
      "y_test shape: (1, 24, 160, 160, 1)\n",
      "Output Shape: (None, 3, 160, 160, 2)\n",
      "Number of Classes: 2\n",
      "Training on 1 GPUs\n",
      "Epoch 1/20\n",
      "7/8 [=========================>....] - ETA: 2s - loss: 1.0675 - acc: 0.7545\n",
      "Epoch 00001: val_loss improved from inf to 7.16207, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 26s 3s/step - loss: 1.0280 - acc: 0.7487 - val_loss: 7.1621 - val_acc: 0.1297\n",
      "Epoch 2/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.6125 - acc: 0.8267\n",
      "Epoch 00002: val_loss improved from 7.16207 to 4.92512, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 0.5652 - acc: 0.8405 - val_loss: 4.9251 - val_acc: 0.1005\n",
      "Epoch 3/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.3783 - acc: 0.9259\n",
      "Epoch 00003: val_loss improved from 4.92512 to 4.21322, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.4083 - acc: 0.9187 - val_loss: 4.2132 - val_acc: 0.4482\n",
      "Epoch 4/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.3232 - acc: 0.9377\n",
      "Epoch 00004: val_loss improved from 4.21322 to 2.55915, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.3454 - acc: 0.9286 - val_loss: 2.5592 - val_acc: 0.5471\n",
      "Epoch 5/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.3384 - acc: 0.8982\n",
      "Epoch 00005: val_loss improved from 2.55915 to 1.56006, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.3363 - acc: 0.8947 - val_loss: 1.5601 - val_acc: 0.6495\n",
      "Epoch 6/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.2329 - acc: 0.9191\n",
      "Epoch 00006: val_loss improved from 1.56006 to 1.05216, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.3011 - acc: 0.9041 - val_loss: 1.0522 - val_acc: 0.6957\n",
      "Epoch 7/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1776 - acc: 0.9433\n",
      "Epoch 00007: val_loss improved from 1.05216 to 0.65432, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.1933 - acc: 0.9389 - val_loss: 0.6543 - val_acc: 0.7912\n",
      "Epoch 8/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1596 - acc: 0.9445\n",
      "Epoch 00008: val_loss improved from 0.65432 to 0.41181, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.1773 - acc: 0.9402 - val_loss: 0.4118 - val_acc: 0.8303\n",
      "Epoch 9/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1692 - acc: 0.9411\n",
      "Epoch 00009: val_loss improved from 0.41181 to 0.30687, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.1596 - acc: 0.9447 - val_loss: 0.3069 - val_acc: 0.8486\n",
      "Epoch 10/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1884 - acc: 0.9394\n",
      "Epoch 00010: val_loss improved from 0.30687 to 0.23749, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.1775 - acc: 0.9418 - val_loss: 0.2375 - val_acc: 0.8947\n",
      "Epoch 11/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1454 - acc: 0.9511\n",
      "Epoch 00011: val_loss improved from 0.23749 to 0.23560, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.1520 - acc: 0.9489 - val_loss: 0.2356 - val_acc: 0.8981\n",
      "Epoch 12/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.2352 - acc: 0.9314\n",
      "Epoch 00012: val_loss improved from 0.23560 to 0.21448, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.3325 - acc: 0.9137 - val_loss: 0.2145 - val_acc: 0.9443\n",
      "Epoch 13/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1913 - acc: 0.9281\n",
      "Epoch 00013: val_loss did not improve from 0.21448\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.1792 - acc: 0.9344 - val_loss: 0.2470 - val_acc: 0.9357\n",
      "Epoch 14/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1729 - acc: 0.9390\n",
      "Epoch 00014: val_loss did not improve from 0.21448\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.2654 - acc: 0.9210 - val_loss: 0.2384 - val_acc: 0.9390\n",
      "Epoch 15/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.2668 - acc: 0.9159\n",
      "Epoch 00015: val_loss did not improve from 0.21448\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.2535 - acc: 0.9199 - val_loss: 0.2239 - val_acc: 0.9581\n",
      "Epoch 16/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1757 - acc: 0.9407\n",
      "Epoch 00016: val_loss did not improve from 0.21448\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.1679 - acc: 0.9410 - val_loss: 0.2310 - val_acc: 0.9471\n",
      "Epoch 17/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.2585 - acc: 0.9035\n",
      "Epoch 00017: val_loss improved from 0.21448 to 0.17909, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_fgbg.h5\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.2386 - acc: 0.9097 - val_loss: 0.1791 - val_acc: 0.9605\n",
      "Epoch 18/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1365 - acc: 0.9481\n",
      "Epoch 00018: val_loss did not improve from 0.17909\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.1332 - acc: 0.9492 - val_loss: 0.2025 - val_acc: 0.9667\n",
      "Epoch 19/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1327 - acc: 0.9553\n",
      "Epoch 00019: val_loss did not improve from 0.17909\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.1275 - acc: 0.9560 - val_loss: 0.2221 - val_acc: 0.9580\n",
      "Epoch 20/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1670 - acc: 0.9425\n",
      "Epoch 00020: val_loss did not improve from 0.17909\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.1565 - acc: 0.9467 - val_loss: 0.2181 - val_acc: 0.9584\n",
      "X_train shape: (8, 24, 160, 160, 1)\n",
      "y_train shape: (8, 24, 160, 160, 1)\n",
      "X_test shape: (1, 24, 160, 160, 1)\n",
      "y_test shape: (1, 24, 160, 160, 1)\n",
      "Output Shape: (None, 3, 160, 160, 4)\n",
      "Number of Classes: 4\n",
      "Training on 1 GPUs\n",
      "Epoch 1/20\n",
      "7/8 [=========================>....] - ETA: 2s - loss: 1.6713 - acc: 0.1866\n",
      "Epoch 00001: val_loss improved from inf to 7.88333, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_watershed.h5\n",
      "8/8 [==============================] - 24s 3s/step - loss: 1.6711 - acc: 0.2458 - val_loss: 7.8833 - val_acc: 0.0030\n",
      "Epoch 2/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.1633 - acc: 0.7515\n",
      "Epoch 00002: val_loss did not improve from 7.88333\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 1.1648 - acc: 0.7552 - val_loss: 9.9323 - val_acc: 0.0433\n",
      "Epoch 3/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.2553 - acc: 0.8328\n",
      "Epoch 00003: val_loss did not improve from 7.88333\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 1.2297 - acc: 0.8312 - val_loss: 9.2085 - val_acc: 0.4774\n",
      "Epoch 4/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.9030 - acc: 0.8463\n",
      "Epoch 00004: val_loss improved from 7.88333 to 7.75601, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_watershed.h5\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.8886 - acc: 0.8580 - val_loss: 7.7560 - val_acc: 0.6283\n",
      "Epoch 5/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.8009 - acc: 0.8590\n",
      "Epoch 00005: val_loss improved from 7.75601 to 3.91242, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_watershed.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 89ms/step - loss: 0.7986 - acc: 0.8617 - val_loss: 3.9124 - val_acc: 0.6250\n",
      "Epoch 6/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.9640 - acc: 0.8865\n",
      "Epoch 00006: val_loss did not improve from 3.91242\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.9683 - acc: 0.8977 - val_loss: 7.4156 - val_acc: 0.7175\n",
      "Epoch 7/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0831 - acc: 0.8809\n",
      "Epoch 00007: val_loss improved from 3.91242 to 1.79661, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_watershed.h5\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.0300 - acc: 0.8835 - val_loss: 1.7966 - val_acc: 0.5803\n",
      "Epoch 8/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.8966 - acc: 0.8831\n",
      "Epoch 00008: val_loss did not improve from 1.79661\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.8952 - acc: 0.8797 - val_loss: 2.3594 - val_acc: 0.4651\n",
      "Epoch 9/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0345 - acc: 0.8787\n",
      "Epoch 00009: val_loss did not improve from 1.79661\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 1.0842 - acc: 0.8819 - val_loss: 5.0288 - val_acc: 0.7348\n",
      "Epoch 10/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.8563 - acc: 0.8894\n",
      "Epoch 00010: val_loss improved from 1.79661 to 1.32697, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_watershed.h5\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.8404 - acc: 0.8885 - val_loss: 1.3270 - val_acc: 0.7507\n",
      "Epoch 11/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.7956 - acc: 0.8941\n",
      "Epoch 00011: val_loss did not improve from 1.32697\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.7833 - acc: 0.8845 - val_loss: 2.2430 - val_acc: 0.7162\n",
      "Epoch 12/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.8091 - acc: 0.8939\n",
      "Epoch 00012: val_loss improved from 1.32697 to 0.98527, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_watershed.h5\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.8111 - acc: 0.8923 - val_loss: 0.9853 - val_acc: 0.7285\n",
      "Epoch 13/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.9459 - acc: 0.8821\n",
      "Epoch 00013: val_loss improved from 0.98527 to 0.92651, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_watershed.h5\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.9134 - acc: 0.8825 - val_loss: 0.9265 - val_acc: 0.8397\n",
      "Epoch 14/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.7513 - acc: 0.8959\n",
      "Epoch 00014: val_loss improved from 0.92651 to 0.91726, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_watershed.h5\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.7654 - acc: 0.8949 - val_loss: 0.9173 - val_acc: 0.9255\n",
      "Epoch 15/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.8386 - acc: 0.9099\n",
      "Epoch 00015: val_loss did not improve from 0.91726\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.8450 - acc: 0.9138 - val_loss: 1.4213 - val_acc: 0.8810\n",
      "Epoch 16/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.7594 - acc: 0.8953\n",
      "Epoch 00016: val_loss improved from 0.91726 to 0.68890, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/nuclei_model_watershed.h5\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.7490 - acc: 0.8968 - val_loss: 0.6889 - val_acc: 0.8811\n",
      "Epoch 17/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.8054 - acc: 0.8951\n",
      "Epoch 00017: val_loss did not improve from 0.68890\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.7809 - acc: 0.8868 - val_loss: 0.7525 - val_acc: 0.8920\n",
      "Epoch 18/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.8175 - acc: 0.9115\n",
      "Epoch 00018: val_loss did not improve from 0.68890\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.8044 - acc: 0.9096 - val_loss: 0.7217 - val_acc: 0.8585\n",
      "Epoch 19/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.7516 - acc: 0.9110\n",
      "Epoch 00019: val_loss did not improve from 0.68890\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.7263 - acc: 0.9029 - val_loss: 0.7070 - val_acc: 0.9283\n",
      "Epoch 20/20\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.7216 - acc: 0.9049\n",
      "Epoch 00020: val_loss did not improve from 0.68890\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.7311 - acc: 0.9136 - val_loss: 0.7782 - val_acc: 0.9185\n"
     ]
    }
   ],
   "source": [
    "from deepcell.training import train_model_conv\n",
    "\n",
    "fgbg_model = train_model_conv(\n",
    "    model=fgbg_model,\n",
    "    train_dict=train_dict,\n",
    "    test_dict=test_dict,\n",
    "    model_name=fgbg_model_name,\n",
    "    test_size=test_size,\n",
    "    optimizer=optimizer,\n",
    "    n_epoch=n_epoch,\n",
    "    batch_size=batch_size,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    transform='fgbg',\n",
    "    model_dir=MODEL_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False)\n",
    "\n",
    "watershed_model = train_model_conv(\n",
    "    model=watershed_model,\n",
    "    train_dict=train_dict,\n",
    "    test_dict=test_dict,\n",
    "    model_name=watershed_model_name,\n",
    "    test_size=test_size,\n",
    "    optimizer=optimizer,\n",
    "    n_epoch=n_epoch,\n",
    "    batch_size=batch_size,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    transform='watershed',\n",
    "    distance_bins=distance_bins,\n",
    "    erosion_width=erosion_width,\n",
    "    model_dir=MODEL_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fgbg_weights_file = os.path.join(MODEL_DIR, '{}.h5'.format(fgbg_model_name))\n",
    "fgbg_model.save_weights(fgbg_weights_file)\n",
    "\n",
    "watershed_weights_file = os.path.join(MODEL_DIR, '{}.h5'.format(watershed_model_name))\n",
    "watershed_model.save_weights(watershed_weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_78 (InputLayer)        (None, 24, 160, 160, 1)   0         \n",
      "_________________________________________________________________\n",
      "reflection_padding3d_38 (Ref (None, 26, 220, 220, 1)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_308 (Conv3D)          (None, 26, 217, 217, 32)  544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_346 (Bat (None, 26, 217, 217, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_346 (Activation)  (None, 26, 217, 217, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_309 (Conv3D)          (None, 26, 215, 215, 32)  9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_347 (Bat (None, 26, 215, 215, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_347 (Activation)  (None, 26, 215, 215, 32)  0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_110 (Dila (None, 26, 214, 214, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_310 (Conv3D)          (None, 26, 210, 210, 32)  9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_348 (Bat (None, 26, 210, 210, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_348 (Activation)  (None, 26, 210, 210, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_311 (Conv3D)          (None, 26, 206, 206, 32)  9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_349 (Bat (None, 26, 206, 206, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_349 (Activation)  (None, 26, 206, 206, 32)  0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_111 (Dila (None, 26, 204, 204, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_312 (Conv3D)          (None, 26, 196, 196, 32)  9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_350 (Bat (None, 26, 196, 196, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_350 (Activation)  (None, 26, 196, 196, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_313 (Conv3D)          (None, 26, 188, 188, 32)  9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_351 (Bat (None, 26, 188, 188, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_351 (Activation)  (None, 26, 188, 188, 32)  0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_112 (Dila (None, 26, 184, 184, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_314 (Conv3D)          (None, 26, 160, 160, 128) 65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_352 (Bat (None, 26, 160, 160, 128) 512       \n",
      "_________________________________________________________________\n",
      "activation_352 (Activation)  (None, 26, 160, 160, 128) 0         \n",
      "_________________________________________________________________\n",
      "conv3d_315 (Conv3D)          (None, 24, 160, 160, 128) 49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_353 (Bat (None, 24, 160, 160, 128) 512       \n",
      "_________________________________________________________________\n",
      "activation_353 (Activation)  (None, 24, 160, 160, 128) 0         \n",
      "_________________________________________________________________\n",
      "tensor_product_76 (TensorPro (None, 24, 160, 160, 128) 16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_354 (Bat (None, 24, 160, 160, 128) 512       \n",
      "_________________________________________________________________\n",
      "activation_354 (Activation)  (None, 24, 160, 160, 128) 0         \n",
      "_________________________________________________________________\n",
      "tensor_product_77 (TensorPro (None, 24, 160, 160, 2)   258       \n",
      "_________________________________________________________________\n",
      "softmax_38 (Softmax)         (None, 24, 160, 160, 2)   0         \n",
      "=================================================================\n",
      "Total params: 180,802\n",
      "Trainable params: 179,650\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_77 (InputLayer)        (None, 24, 160, 160, 1)   0         \n",
      "_________________________________________________________________\n",
      "lambda_38 (Lambda)           (None, 24, 160, 160, 1)   0         \n",
      "_________________________________________________________________\n",
      "model_76 (Model)             (None, 24, 160, 160, 2)   180802    \n",
      "=================================================================\n",
      "Total params: 180,802\n",
      "Trainable params: 179,650\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_80 (InputLayer)        (None, 24, 160, 160, 1)   0         \n",
      "_________________________________________________________________\n",
      "reflection_padding3d_39 (Ref (None, 26, 220, 220, 1)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_316 (Conv3D)          (None, 26, 217, 217, 32)  544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_355 (Bat (None, 26, 217, 217, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_355 (Activation)  (None, 26, 217, 217, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_317 (Conv3D)          (None, 26, 215, 215, 32)  9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_356 (Bat (None, 26, 215, 215, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_356 (Activation)  (None, 26, 215, 215, 32)  0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_113 (Dila (None, 26, 214, 214, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_318 (Conv3D)          (None, 26, 210, 210, 32)  9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_357 (Bat (None, 26, 210, 210, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_357 (Activation)  (None, 26, 210, 210, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_319 (Conv3D)          (None, 26, 206, 206, 32)  9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_358 (Bat (None, 26, 206, 206, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_358 (Activation)  (None, 26, 206, 206, 32)  0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_114 (Dila (None, 26, 204, 204, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_320 (Conv3D)          (None, 26, 196, 196, 32)  9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_359 (Bat (None, 26, 196, 196, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_359 (Activation)  (None, 26, 196, 196, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_321 (Conv3D)          (None, 26, 188, 188, 32)  9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_360 (Bat (None, 26, 188, 188, 32)  128       \n",
      "_________________________________________________________________\n",
      "activation_360 (Activation)  (None, 26, 188, 188, 32)  0         \n",
      "_________________________________________________________________\n",
      "dilated_max_pool3d_115 (Dila (None, 26, 184, 184, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_322 (Conv3D)          (None, 26, 160, 160, 128) 65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_361 (Bat (None, 26, 160, 160, 128) 512       \n",
      "_________________________________________________________________\n",
      "activation_361 (Activation)  (None, 26, 160, 160, 128) 0         \n",
      "_________________________________________________________________\n",
      "conv3d_323 (Conv3D)          (None, 24, 160, 160, 128) 49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_362 (Bat (None, 24, 160, 160, 128) 512       \n",
      "_________________________________________________________________\n",
      "activation_362 (Activation)  (None, 24, 160, 160, 128) 0         \n",
      "_________________________________________________________________\n",
      "tensor_product_78 (TensorPro (None, 24, 160, 160, 128) 16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_363 (Bat (None, 24, 160, 160, 128) 512       \n",
      "_________________________________________________________________\n",
      "activation_363 (Activation)  (None, 24, 160, 160, 128) 0         \n",
      "_________________________________________________________________\n",
      "tensor_product_79 (TensorPro (None, 24, 160, 160, 4)   516       \n",
      "_________________________________________________________________\n",
      "softmax_39 (Softmax)         (None, 24, 160, 160, 4)   0         \n",
      "=================================================================\n",
      "Total params: 181,060\n",
      "Trainable params: 179,908\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_79 (InputLayer)        (None, 24, 160, 160, 1)   0         \n",
      "_________________________________________________________________\n",
      "lambda_39 (Lambda)           (None, 24, 160, 160, 1)   0         \n",
      "_________________________________________________________________\n",
      "model_78 (Model)             (None, 24, 160, 160, 4)   181060    \n",
      "=================================================================\n",
      "Total params: 181,060\n",
      "Trainable params: 179,908\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_fgbg_model = feature_net_skip_3D(\n",
    "    receptive_field=receptive_field,\n",
    "    n_features=2,\n",
    "    n_frames=frames_per_batch,\n",
    "    n_skips=n_skips,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    input_shape=tuple(test_dict['X'].shape[1:]),\n",
    "    multires=False,\n",
    "    last_only=False,\n",
    "    norm_method=norm_method)\n",
    "run_fgbg_model.load_weights(fgbg_weights_file)\n",
    "\n",
    "run_watershed_model = feature_net_skip_3D(\n",
    "    receptive_field=receptive_field,\n",
    "    n_skips=n_skips,\n",
    "    n_features=distance_bins,\n",
    "    n_frames=frames_per_batch,\n",
    "    gru=False,\n",
    "    gru_kernel_size=3,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    multires=False,\n",
    "    last_only=False,\n",
    "    input_shape=tuple(test_dict['X'].shape[1:]),\n",
    "    norm_method=norm_method)\n",
    "\n",
    "run_watershed_model.load_weights(watershed_weights_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watershed transform shape: (24, 160, 160, 4)\n",
      "segmentation mask shape: (24, 160, 160, 2)\n"
     ]
    }
   ],
   "source": [
    "test_images = run_watershed_model.predict(test_dict['X'])[-1]\n",
    "test_images_fgbg = run_fgbg_model.predict(test_dict['X'])[-1]\n",
    "\n",
    "# test_images = watershed_model.predict(test_dict['X'])[-1]\n",
    "# test_images_fgbg = fgbg_model.predict(test_dict['X'])[-1]\n",
    "\n",
    "print('watershed transform shape:', test_images.shape)\n",
    "print('segmentation mask shape:', test_images_fgbg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watershed argmax shape: (24, 160, 160, 1)\n"
     ]
    }
   ],
   "source": [
    "argmax_images = []\n",
    "for i in range(test_images.shape[0]):\n",
    "    max_image = np.argmax(test_images[i], axis=-1)\n",
    "    argmax_images.append(max_image)\n",
    "argmax_images = np.array(argmax_images)\n",
    "argmax_images = np.expand_dims(argmax_images, axis=-1)\n",
    "\n",
    "print('watershed argmax shape:', argmax_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply watershed method with the distance transform as seed\n",
    "from skimage.measure import label\n",
    "from skimage.morphology import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "# threshold the foreground/background\n",
    "# and remove back ground from watershed transform\n",
    "threshold = 0.5\n",
    "\n",
    "fg_thresh = test_images_fgbg[..., 1] > threshold\n",
    "fg_thresh = np.expand_dims(fg_thresh, axis=-1)\n",
    "\n",
    "argmax_images_post_fgbg = argmax_images * fg_thresh\n",
    "\n",
    "\n",
    "watershed_images = []\n",
    "for i in range(argmax_images_post_fgbg.shape[0]):\n",
    "    image = fg_thresh[i, ..., 0]\n",
    "    distance = argmax_images_post_fgbg[i, ..., 0]\n",
    "\n",
    "    local_maxi = peak_local_max(\n",
    "        test_images[i, ..., -1],\n",
    "        min_distance=10,\n",
    "        threshold_abs=0.05,\n",
    "        indices=False,\n",
    "        labels=image,\n",
    "        exclude_border=False)\n",
    "\n",
    "    markers = label(local_maxi)\n",
    "    segments = watershed(-distance, markers, mask=image)\n",
    "    watershed_images.append(segments)\n",
    "\n",
    "watershed_images = np.array(watershed_images)\n",
    "watershed_images = np.expand_dims(watershed_images, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame: 22\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frame = np.random.randint(low=0, high=watershed_images.shape[0])\n",
    "\n",
    "print('Frame:', frame)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(15, 15), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(test_dict['X'][0, frame, ..., 0])\n",
    "ax[0].set_title('Source Image')\n",
    "\n",
    "ax[1].imshow(test_images[frame, ..., 1])\n",
    "ax[1].set_title('FGBG Prediction')\n",
    "\n",
    "ax[2].imshow(fg_thresh[frame, ..., 0], cmap='jet')\n",
    "ax[2].set_title('FGBG {}% Threshold'.format(int(threshold * 100)))\n",
    "\n",
    "ax[3].imshow(argmax_images[frame, ..., 0], cmap='jet')\n",
    "ax[3].set_title('Distance Transform')\n",
    "\n",
    "ax[4].imshow(argmax_images_post_fgbg[frame, ..., 0], cmap='jet')\n",
    "ax[4].set_title('Distance Transform without background')\n",
    "\n",
    "ax[5].imshow(watershed_images[frame, ..., 0], cmap='jet')\n",
    "ax[5].set_title('Watershed Segmentation')\n",
    "\n",
    "# ax[5].imshow(test_dict['y'][0, frame, ..., 0], cmap='jet')\n",
    "# ax[5].set_title('Ground truth')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('watershed_nuclei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 160, 160, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_dict['y'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for Lumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X0 = DAPI   \n",
    "\n",
    "X1 = GFP  \n",
    "\n",
    "y0 = lumens  \n",
    "\n",
    "y1 = organoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -\n",
      "X.shape: (222, 2, 160, 160, 1)\n",
      "y.shape: (222, 2, 160, 160, 1)\n",
      " -\n",
      "X_train.shape: (199, 160, 160, 1)\n",
      "y_train.shape: (199, 160, 160, 1)\n",
      " -\n",
      "X_test.shape: (23, 160, 160, 1)\n",
      "y_test.shape: (23, 160, 160, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "filename = 'combined_is_all.npz'\n",
    "DATA_FILE = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "combined_npz = np.load(DATA_FILE)\n",
    "X_full, y_full = combined_npz['X'], combined_npz['y']\n",
    "print(' -\\nX.shape: {}\\ny.shape: {}'.format(X_full.shape, y_full.shape))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.1, random_state=0)\n",
    "\n",
    "\n",
    "train_dict = {\n",
    "    'X': np.swapaxes(X_train, 0, 1)[1],\n",
    "    'y': np.swapaxes(y_train, 0, 1)[1],\n",
    "}\n",
    "\n",
    "\n",
    "test_dict = {\n",
    "    'X': np.swapaxes(X_test, 0, 1)[1],\n",
    "    'y': np.swapaxes(y_test, 0, 1)[1],\n",
    "}\n",
    "\n",
    "print(' -\\nX_train.shape: {}\\ny_train.shape: {}'.format(train_dict['X'].shape, train_dict['y'].shape))\n",
    "print(' -\\nX_test.shape: {}\\ny_test.shape: {}'.format(test_dict['X'].shape, test_dict['y'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "fgbg_model_name = 'organoid_fgbg_model'\n",
    "conv_model_name = 'organoid_watershed_model'\n",
    "\n",
    "n_epoch = 15  # Number of training epochs\n",
    "test_size = .10  # % of data saved as test\n",
    "norm_method = 'std'  # data normalization\n",
    "receptive_field = 81  # should be adjusted for the scale of the data\n",
    "\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=0.01, decay=0.99)\n",
    "\n",
    "# FC training settings\n",
    "n_skips = 0  # number of skip-connections (only for FC training)\n",
    "batch_size = 1  # FC training uses 1 image per batch\n",
    "\n",
    "# Transformation settings\n",
    "transform = 'watershed'\n",
    "distance_bins = 4\n",
    "erosion_width = 0  # erode edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (199, 160, 160, 1)\n",
      "y_train shape: (199, 160, 160, 1)\n",
      "X_test shape: (23, 160, 160, 1)\n",
      "y_test shape: (23, 160, 160, 1)\n",
      "Output Shape: (None, 160, 160, 2)\n",
      "Number of Classes: 2\n",
      "Training on 1 GPUs\n",
      "Epoch 1/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8109\n",
      "Epoch 00001: val_loss improved from inf to 0.22732, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/organoid_fgbg_model.h5\n",
      "199/199 [==============================] - 22s 110ms/step - loss: 0.3239 - acc: 0.8115 - val_loss: 0.2273 - val_acc: 0.8823\n",
      "Epoch 2/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.8520\n",
      "Epoch 00002: val_loss improved from 0.22732 to 0.21487, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/organoid_fgbg_model.h5\n",
      "199/199 [==============================] - 6s 28ms/step - loss: 0.2632 - acc: 0.8528 - val_loss: 0.2149 - val_acc: 0.8840\n",
      "Epoch 3/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.8650\n",
      "Epoch 00003: val_loss did not improve from 0.21487\n",
      "199/199 [==============================] - 6s 28ms/step - loss: 0.2368 - acc: 0.8647 - val_loss: 0.2425 - val_acc: 0.8962\n",
      "Epoch 4/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.8765\n",
      "Epoch 00004: val_loss improved from 0.21487 to 0.16209, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/organoid_fgbg_model.h5\n",
      "199/199 [==============================] - 6s 29ms/step - loss: 0.2290 - acc: 0.8772 - val_loss: 0.1621 - val_acc: 0.9116\n",
      "Epoch 5/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.8876\n",
      "Epoch 00005: val_loss improved from 0.16209 to 0.15964, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/organoid_fgbg_model.h5\n",
      "199/199 [==============================] - 6s 29ms/step - loss: 0.2156 - acc: 0.8876 - val_loss: 0.1596 - val_acc: 0.9125\n",
      "Epoch 6/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9084\n",
      "Epoch 00006: val_loss did not improve from 0.15964\n",
      "199/199 [==============================] - 6s 28ms/step - loss: 0.1939 - acc: 0.9086 - val_loss: 0.1667 - val_acc: 0.9185\n",
      "Epoch 7/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9280\n",
      "Epoch 00007: val_loss did not improve from 0.15964\n",
      "199/199 [==============================] - 6s 28ms/step - loss: 0.1766 - acc: 0.9281 - val_loss: 0.1709 - val_acc: 0.9183\n",
      "Epoch 8/15\n",
      "198/199 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9151\n",
      "Epoch 00008: val_loss did not improve from 0.15964\n",
      "199/199 [==============================] - 6s 28ms/step - loss: 0.1935 - acc: 0.9146 - val_loss: 0.1759 - val_acc: 0.9311\n",
      "Epoch 9/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9370\n",
      "Epoch 00009: val_loss improved from 0.15964 to 0.13548, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/organoid_fgbg_model.h5\n",
      "199/199 [==============================] - 6s 28ms/step - loss: 0.1606 - acc: 0.9372 - val_loss: 0.1355 - val_acc: 0.9268\n",
      "Epoch 10/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9488\n",
      "Epoch 00010: val_loss did not improve from 0.13548\n",
      "199/199 [==============================] - 5s 28ms/step - loss: 0.1372 - acc: 0.9491 - val_loss: 0.1432 - val_acc: 0.9270\n",
      "Epoch 11/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9289\n",
      "Epoch 00011: val_loss did not improve from 0.13548\n",
      "199/199 [==============================] - 6s 28ms/step - loss: 0.1811 - acc: 0.9290 - val_loss: 0.4039 - val_acc: 0.8676\n",
      "Epoch 12/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9250\n",
      "Epoch 00012: val_loss did not improve from 0.13548\n",
      "199/199 [==============================] - 6s 28ms/step - loss: 0.1757 - acc: 0.9252 - val_loss: 0.2049 - val_acc: 0.8855\n",
      "Epoch 13/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9348\n",
      "Epoch 00013: val_loss improved from 0.13548 to 0.11756, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/organoid_fgbg_model.h5\n",
      "199/199 [==============================] - 6s 29ms/step - loss: 0.1614 - acc: 0.9349 - val_loss: 0.1176 - val_acc: 0.9419\n",
      "Epoch 14/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9510\n",
      "Epoch 00014: val_loss improved from 0.11756 to 0.11278, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/organoid_fgbg_model.h5\n",
      "199/199 [==============================] - 6s 28ms/step - loss: 0.1319 - acc: 0.9514 - val_loss: 0.1128 - val_acc: 0.9515\n",
      "Epoch 15/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9542\n",
      "Epoch 00015: val_loss did not improve from 0.11278\n",
      "199/199 [==============================] - 6s 28ms/step - loss: 0.1266 - acc: 0.9544 - val_loss: 0.1904 - val_acc: 0.9075\n"
     ]
    }
   ],
   "source": [
    "from deepcell import model_zoo\n",
    "from deepcell.training import train_model_conv\n",
    "\n",
    "\n",
    "fgbg_model = model_zoo.bn_feature_net_skip_2D(\n",
    "    n_features=2,  # segmentation mask (is_cell, is_not_cell)\n",
    "    receptive_field=receptive_field,\n",
    "    n_skips=n_skips,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    input_shape=tuple(train_dict['X'].shape[1:]),\n",
    "    last_only=False)\n",
    "\n",
    "fgbg_model = train_model_conv(\n",
    "    model=fgbg_model,\n",
    "    train_dict=train_dict,\n",
    "    test_dict=test_dict,\n",
    "    model_name=fgbg_model_name,\n",
    "    test_size=test_size,\n",
    "    optimizer=optimizer,\n",
    "    n_epoch=n_epoch,\n",
    "    batch_size=batch_size,\n",
    "    transform='fgbg',\n",
    "    model_dir=MODEL_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False,\n",
    "    zoom_range=(0.8, 1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (199, 160, 160, 1)\n",
      "y_train shape: (199, 160, 160, 1)\n",
      "X_test shape: (23, 160, 160, 1)\n",
      "y_test shape: (23, 160, 160, 1)\n",
      "Output Shape: (None, 160, 160, 4)\n",
      "Number of Classes: 4\n",
      "Training on 1 GPUs\n",
      "Epoch 1/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 1.1685 - acc: 0.5948\n",
      "Epoch 00001: val_loss improved from inf to 1.37706, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/organoid_watershed_model.h5\n",
      "199/199 [==============================] - 25s 124ms/step - loss: 1.1650 - acc: 0.5964 - val_loss: 1.3771 - val_acc: 0.6565\n",
      "Epoch 2/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.9821 - acc: 0.6564\n",
      "Epoch 00002: val_loss did not improve from 1.37706\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.9795 - acc: 0.6577 - val_loss: 1.4026 - val_acc: 0.7247\n",
      "Epoch 3/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.9702 - acc: 0.6758\n",
      "Epoch 00003: val_loss did not improve from 1.37706\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.9690 - acc: 0.6769 - val_loss: 1.4056 - val_acc: 0.7172\n",
      "Epoch 4/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.9170 - acc: 0.6959\n",
      "Epoch 00004: val_loss did not improve from 1.37706\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.9185 - acc: 0.6969 - val_loss: 1.5039 - val_acc: 0.6655\n",
      "Epoch 5/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.8761 - acc: 0.7165\n",
      "Epoch 00005: val_loss did not improve from 1.37706\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.8795 - acc: 0.7108 - val_loss: 1.4199 - val_acc: 0.7138\n",
      "Epoch 6/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.8315 - acc: 0.7157\n",
      "Epoch 00006: val_loss did not improve from 1.37706\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.8317 - acc: 0.7169 - val_loss: 1.5982 - val_acc: 0.6828\n",
      "Epoch 7/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.8285 - acc: 0.7155\n",
      "Epoch 00007: val_loss did not improve from 1.37706\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.8295 - acc: 0.7159 - val_loss: 1.4998 - val_acc: 0.6804\n",
      "Epoch 8/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.8043 - acc: 0.7300\n",
      "Epoch 00008: val_loss did not improve from 1.37706\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.8019 - acc: 0.7319 - val_loss: 1.6145 - val_acc: 0.7116\n",
      "Epoch 9/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.8084 - acc: 0.7417\n",
      "Epoch 00009: val_loss did not improve from 1.37706\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.8061 - acc: 0.7389 - val_loss: 1.5735 - val_acc: 0.7099\n",
      "Epoch 10/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.7818 - acc: 0.7416\n",
      "Epoch 00010: val_loss improved from 1.37706 to 1.24755, saving model to /home/sunnycui/deepcell-tf/scripts/recurr_gru/models/organoid_watershed_model.h5\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.7827 - acc: 0.7403 - val_loss: 1.2475 - val_acc: 0.7389\n",
      "Epoch 11/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.7579 - acc: 0.7495\n",
      "Epoch 00011: val_loss did not improve from 1.24755\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.7588 - acc: 0.7485 - val_loss: 1.3461 - val_acc: 0.7228\n",
      "Epoch 12/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.7341 - acc: 0.7604\n",
      "Epoch 00012: val_loss did not improve from 1.24755\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.7361 - acc: 0.7615 - val_loss: 2.2291 - val_acc: 0.6482\n",
      "Epoch 13/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.7285 - acc: 0.7596\n",
      "Epoch 00013: val_loss did not improve from 1.24755\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.7258 - acc: 0.7614 - val_loss: 1.9219 - val_acc: 0.6759\n",
      "Epoch 14/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.7296 - acc: 0.7558\n",
      "Epoch 00014: val_loss did not improve from 1.24755\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.7319 - acc: 0.7573 - val_loss: 1.5747 - val_acc: 0.6850\n",
      "Epoch 15/15\n",
      "197/199 [============================>.] - ETA: 0s - loss: 0.7372 - acc: 0.7583\n",
      "Epoch 00015: val_loss did not improve from 1.24755\n",
      "199/199 [==============================] - 8s 40ms/step - loss: 0.7371 - acc: 0.7600 - val_loss: 1.5916 - val_acc: 0.7132\n"
     ]
    }
   ],
   "source": [
    "from deepcell import model_zoo\n",
    "from deepcell.training import train_model_conv\n",
    "\n",
    "\n",
    "watershed_model = model_zoo.bn_feature_net_skip_2D(\n",
    "    fgbg_model=fgbg_model,\n",
    "    receptive_field=receptive_field,\n",
    "    n_skips=n_skips,\n",
    "    n_features=distance_bins,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    last_only=False,\n",
    "    input_shape=tuple(train_dict['X'].shape[1:]))\n",
    "\n",
    "watershed_model = train_model_conv(\n",
    "    model=watershed_model,\n",
    "    train_dict=train_dict,\n",
    "    test_dict=test_dict,\n",
    "    model_name=conv_model_name,\n",
    "    test_size=test_size,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=batch_size,\n",
    "    n_epoch=n_epoch,\n",
    "    transform=transform,\n",
    "    distance_bins=distance_bins,\n",
    "    erosion_width=erosion_width,\n",
    "    model_dir=MODEL_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False,\n",
    "    zoom_range=(0.8, 1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watershed transform shape: (23, 160, 160, 4)\n",
      "segmentation mask shape: (23, 160, 160, 2)\n",
      "watershed argmax shape: (23, 160, 160, 1)\n"
     ]
    }
   ],
   "source": [
    "# make predictions on testing data\n",
    "test_images = watershed_model.predict(test_dict['X'])\n",
    "test_images_fgbg = fgbg_model.predict(test_dict['X'])\n",
    "\n",
    "print('watershed transform shape:', test_images.shape)\n",
    "print('segmentation mask shape:', test_images_fgbg.shape)\n",
    "\n",
    "# Collapse predictions into semantic segmentation mask\n",
    "\n",
    "argmax_images = []\n",
    "for i in range(test_images.shape[0]):\n",
    "    max_image = np.argmax(test_images[i], axis=-1)\n",
    "    argmax_images.append(max_image)\n",
    "argmax_images = np.array(argmax_images)\n",
    "argmax_images = np.expand_dims(argmax_images, axis=-1)\n",
    "\n",
    "print('watershed argmax shape:', argmax_images.shape)\n",
    "\n",
    "# threshold the foreground/background\n",
    "# and remove background from watershed transform\n",
    "threshold = 0.5\n",
    "fg_thresh = test_images_fgbg[..., 1] > threshold\n",
    "\n",
    "fg_thresh = np.expand_dims(fg_thresh.astype('int16'), axis=-1)\n",
    "argmax_images_post_fgbg = argmax_images * fg_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply watershed method with the distance transform as seed\n",
    "from skimage.measure import label\n",
    "from skimage.morphology import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "watershed_images = []\n",
    "for i in range(argmax_images_post_fgbg.shape[0]):\n",
    "    image = fg_thresh[i, ..., 0]\n",
    "    distance = argmax_images_post_fgbg[i, ..., 0]\n",
    "\n",
    "    local_maxi = peak_local_max(test_images[i, ..., -1], min_distance=15, \n",
    "                                exclude_border=False, indices=False, labels=image)\n",
    "\n",
    "    markers = label(local_maxi)\n",
    "    segments = watershed(-distance, markers, mask=image)\n",
    "    watershed_images.append(segments)\n",
    "\n",
    "watershed_images = np.array(watershed_images)\n",
    "watershed_images = np.expand_dims(watershed_images, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image number: 2\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = np.random.randint(low=0, high=test_dict['X'].shape[0])\n",
    "print('Image number:', index)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(15, 15), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(test_dict['X'][index, ..., 0])\n",
    "ax[0].set_title('Source Image')\n",
    "\n",
    "ax[1].imshow(test_images_fgbg[index, ..., 1])\n",
    "ax[1].set_title('Organoid Prediction')\n",
    "\n",
    "ax[2].imshow(fg_thresh[index, ..., 0], cmap='jet')\n",
    "ax[2].set_title('FGBG Threshold {}%'.format(threshold * 100))\n",
    "\n",
    "ax[3].imshow(test_dict['y'][index, ..., 0], cmap='jet')\n",
    "ax[3].set_title('Ground truth')\n",
    "\n",
    "ax[4].imshow(argmax_images_post_fgbg[index, ..., 0], cmap='jet')\n",
    "ax[4].set_title('Distance Transform w/o Background')\n",
    "\n",
    "ax[5].imshow(watershed_images[index, ..., 0], cmap='jet')\n",
    "ax[5].set_title('Watershed Segmentation')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('watershed_organoid_2d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'combined_is_nuclei.npz'\n",
    "DATA_FILE = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "print(\"Loading data from \" + filename)\n",
    "train_dict, test_dict = get_data(DATA_FILE, mode='conv', test_size=0.1, seed=0)\n",
    "\n",
    "train_dict['X'] = train_dict['X'].reshape((8*24, 160, 160, 1))\n",
    "train_dict['y'] = train_dict['y'].reshape((8*24, 160, 160, 1))\n",
    "\n",
    "test_dict['X'] = test_dict['X'].reshape((24, 160, 160, 1))\n",
    "test_dict['y'] = test_dict['y'].reshape((24, 160, 160, 1))\n",
    "\n",
    "print(' -\\nX.shape: {}\\ny.shape: {}'.format(test_dict['X'].shape, test_dict['y'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "fgbg_model_name = 'conv_fgbg_model'\n",
    "conv_model_name = 'conv_watershed_model'\n",
    "\n",
    "n_epoch = 5  # Number of training epochs\n",
    "test_size = .10  # % of data saved as test\n",
    "norm_method = 'std'  # data normalization\n",
    "receptive_field = 61  # should be adjusted for the scale of the data\n",
    "\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "lr_sched = rate_scheduler(lr=0.01, decay=0.99)\n",
    "\n",
    "# FC training settings\n",
    "n_skips = 0  # number of skip-connections (only for FC training)\n",
    "batch_size = 1  # FC training uses 1 image per batch\n",
    "\n",
    "# Transformation settings\n",
    "transform = 'watershed'\n",
    "distance_bins = 4\n",
    "erosion_width = 0  # erode edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepcell import model_zoo\n",
    "from deepcell.training import train_model_conv\n",
    "\n",
    "\n",
    "fgbg_model = model_zoo.bn_feature_net_skip_2D(\n",
    "    n_features=2,  # segmentation mask (is_cell, is_not_cell)\n",
    "    receptive_field=receptive_field,\n",
    "    n_skips=n_skips,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    input_shape=tuple(train_dict['X'].shape[1:]),\n",
    "    last_only=False)\n",
    "\n",
    "fgbg_model = train_model_conv(\n",
    "    model=fgbg_model,\n",
    "    train_dict=train_dict,\n",
    "    test_dict=test_dict,\n",
    "    model_name=fgbg_model_name,\n",
    "    test_size=test_size,\n",
    "    optimizer=optimizer,\n",
    "    n_epoch=n_epoch,\n",
    "    batch_size=batch_size,\n",
    "    transform='fgbg',\n",
    "    model_dir=MODEL_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False,\n",
    "    zoom_range=(0.8, 1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepcell import model_zoo\n",
    "from deepcell.training import train_model_conv\n",
    "\n",
    "\n",
    "watershed_model = model_zoo.bn_feature_net_skip_2D(\n",
    "    fgbg_model=fgbg_model,\n",
    "    receptive_field=receptive_field,\n",
    "    n_skips=n_skips,\n",
    "    n_features=distance_bins,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    last_only=False,\n",
    "    input_shape=tuple(train_dict['X'].shape[1:]))\n",
    "\n",
    "watershed_model = train_model_conv(\n",
    "    model=watershed_model,\n",
    "    train_dict=train_dict,\n",
    "    test_dict=test_dict,\n",
    "    model_name=conv_model_name,\n",
    "    test_size=test_size,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=batch_size,\n",
    "    n_epoch=n_epoch,\n",
    "    transform=transform,\n",
    "    distance_bins=distance_bins,\n",
    "    erosion_width=erosion_width,\n",
    "    model_dir=MODEL_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False,\n",
    "    zoom_range=(0.8, 1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watershed argmax shape: (24, 160, 160, 1)\n"
     ]
    }
   ],
   "source": [
    "# make predictions on testing data\n",
    "test_images = watershed_model.predict(test_dict['X'])\n",
    "test_images_fgbg = fgbg_model.predict(test_dict['X'])\n",
    "\n",
    "print('watershed transform shape:', test_images.shape)\n",
    "print('segmentation mask shape:', test_images_fgbg.shape)\n",
    "\n",
    "# Collapse predictions into semantic segmentation mask\n",
    "\n",
    "argmax_images = []\n",
    "for i in range(test_images.shape[0]):\n",
    "    max_image = np.argmax(test_images[i], axis=-1)\n",
    "    argmax_images.append(max_image)\n",
    "argmax_images = np.array(argmax_images)\n",
    "argmax_images = np.expand_dims(argmax_images, axis=-1)\n",
    "\n",
    "print('watershed argmax shape:', argmax_images.shape)\n",
    "\n",
    "# threshold the foreground/background\n",
    "# and remove background from watershed transform\n",
    "threshold = 0.5\n",
    "fg_thresh = test_images_fgbg[..., 1] > threshold\n",
    "\n",
    "fg_thresh = np.expand_dims(fg_thresh.astype('int16'), axis=-1)\n",
    "argmax_images_post_fgbg = argmax_images * fg_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply watershed method with the distance transform as seed\n",
    "from skimage.measure import label\n",
    "from skimage.morphology import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "watershed_images = []\n",
    "for i in range(argmax_images_post_fgbg.shape[0]):\n",
    "    image = fg_thresh[i, ..., 0]\n",
    "    distance = argmax_images_post_fgbg[i, ..., 0]\n",
    "\n",
    "    local_maxi = peak_local_max(test_images[i, ..., -1], min_distance=15, \n",
    "                                exclude_border=False, indices=False, labels=image)\n",
    "\n",
    "    markers = label(local_maxi)\n",
    "    segments = watershed(-distance, markers, mask=image)\n",
    "    watershed_images.append(segments)\n",
    "\n",
    "watershed_images = np.array(watershed_images)\n",
    "watershed_images = np.expand_dims(watershed_images, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image number: 14\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = np.random.randint(low=0, high=test_dict['X'].shape[0])\n",
    "print('Image number:', index)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(15, 15), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(test_dict['X'][index, ..., 0])\n",
    "ax[0].set_title('Source Image')\n",
    "\n",
    "ax[1].imshow(test_images_fgbg[index, ..., 1])\n",
    "ax[1].set_title('Segmentation Prediction')\n",
    "\n",
    "ax[2].imshow(fg_thresh[index, ..., 0], cmap='jet')\n",
    "ax[2].set_title('FGBG Threshold {}%'.format(threshold * 100))\n",
    "\n",
    "ax[3].imshow(argmax_images[index, ..., 0], cmap='jet')\n",
    "ax[3].set_title('Distance Transform')\n",
    "\n",
    "ax[4].imshow(argmax_images_post_fgbg[index, ..., 0], cmap='jet')\n",
    "ax[4].set_title('Distance Transform w/o Background')\n",
    "\n",
    "ax[5].imshow(watershed_images[index, ..., 0], cmap='jet')\n",
    "ax[5].set_title('Watershed Segmentation')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('watershed_nuclei_2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
